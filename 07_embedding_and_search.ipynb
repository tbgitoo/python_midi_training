{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca81c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and loading the trained model\n",
      "\n",
      "Loading TFLite model from: models/music_vae_encoder_tf2.tflite...\n",
      "TFLite model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print('Importing libraries and loading the trained model')\n",
    "\n",
    "import note_seq\n",
    "from note_seq import sequences_lib\n",
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import NoExtractedExamplesError\n",
    "from magenta.models.music_vae.trained_model import MultipleExtractedExamplesError \n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd # For keaping the embeddings in a pandas dataframe\n",
    "import pyreadr # for conversion to rda files for statistical analysis in R\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss  # You'll need to install this: pip install faiss-cpu\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "# for setting up setup_faiss_db.py\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "\n",
    "# Path to your newly created TFLite model\n",
    "TFLITE_MODEL_PATH = 'models/music_vae_encoder_tf2.tflite'\n",
    "\n",
    "\n",
    "print(f\"\\nLoading TFLite model from: {TFLITE_MODEL_PATH}...\")\n",
    "# The TFLite interpreter is independent of TF sessions and graphs.\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output details for later use\n",
    "tflite_input_details = interpreter.get_input_details()\n",
    "tflite_output_details = interpreter.get_output_details()\n",
    "print(\"TFLite model loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc13c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration and function definitions  --\n",
    "DB_PATH = os.path.join('data_sets', 'midi_embeddings.db')\n",
    "FAISS_INDEX_PATH = os.path.join('data_sets', 'midi_embeddings.index')\n",
    "MELODY_DIR = os.path.join('data_sets', 'lmd_melodies') # Directory of extracted melodies\n",
    "EMBEDDING_DIM = 512 # The dimension of your MusicVAE embeddings\n",
    "\n",
    "def filter_pitch_range(ns, min_pitch=36, max_pitch=84):\n",
    "    \"\"\"Removes notes outside the specified MIDI pitch range.\"\"\"\n",
    "    valid_notes = [n for n in ns.notes if min_pitch <= n.pitch <= max_pitch]   \n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(valid_notes)   \n",
    "    return ns\n",
    "\n",
    "# --- Helper Functions (from previous iterations, still useful for cleaning) ---\n",
    "def make_monophonic(ns,steps_per_quarter=4):\n",
    "    \"\"\"Reduces a NoteSequence to be monophonic by picking the highest note at each step.\"\"\"\n",
    "    if not ns.notes:\n",
    "        return ns\n",
    "    quantized_ns = sequences_lib.quantize_note_sequence(ns, steps_per_quarter)    \n",
    "    notes_by_step = {}\n",
    "    for note in quantized_ns.notes:\n",
    "        # Use quantized_start_step for already quantized sequences\n",
    "        if note.quantized_start_step not in notes_by_step:\n",
    "            notes_by_step[note.quantized_start_step] = []\n",
    "        notes_by_step[note.quantized_start_step].append(note)\n",
    "    monophonic_notes = []\n",
    "    for step in sorted(notes_by_step.keys()):\n",
    "        notes_at_step = notes_by_step[step]\n",
    "        # If multiple notes at a step, pick the highest pitch\n",
    "        highest_note = max(notes_at_step, key=lambda n: n.pitch)\n",
    "        monophonic_notes.append(highest_note)\n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(monophonic_notes)\n",
    "    return ns\n",
    "\n",
    "def snap_chunk_notes_to_grid(unquantized_chunk, steps_per_quarter):\n",
    "    \"\"\"\n",
    "    Creates a new, unquantized NoteSequence with notes snapped to a grid.\n",
    "    This is the key function. It takes a time-based chunk, finds the ideal\n",
    "    quantized steps for its notes, and then creates a *new* unquantized\n",
    "    sequence where the note start/end times correspond perfectly to those steps.\n",
    "    Args:\n",
    "      unquantized_chunk: The unquantized NoteSequence chunk.\n",
    "      steps_per_quarter: The quantization resolution.\n",
    "    Returns:\n",
    "      A new, unquantized NoteSequence with grid-aligned note timings.\n",
    "    \"\"\"\n",
    "    # 1. Quantize the chunk to determine the ideal grid steps for each note.\n",
    "    try:\n",
    "        quantized_temp_chunk = note_seq.quantize_note_sequence(\n",
    "            unquantized_chunk, steps_per_quarter)\n",
    "    except note_seq.BadTimeSignatureError:\n",
    "        return None # Cannot process this chunk\n",
    "    qpm = unquantized_chunk.tempos[0].qpm if unquantized_chunk.tempos else 120.0\n",
    "    seconds_per_quarter = 60.0 / qpm\n",
    "    # 2. Create a new, empty, unquantized sequence to be the output.\n",
    "    grid_aligned_ns = music_pb2.NoteSequence()\n",
    "    grid_aligned_ns.tempos.add().qpm = qpm\n",
    "    grid_aligned_ns.ticks_per_quarter = unquantized_chunk.ticks_per_quarter\n",
    "    # 3. For each note in the quantized version, create a new note in our\n",
    "    #    output sequence with timings calculated from the quantized steps.\n",
    "    for q_note in quantized_temp_chunk.notes:\n",
    "        new_note = grid_aligned_ns.notes.add()\n",
    "        new_note.pitch = q_note.pitch\n",
    "        new_note.velocity = q_note.velocity\n",
    "        new_note.instrument = q_note.instrument\n",
    "        new_note.program = q_note.program\n",
    "        # Convert quantized steps back into precise seconds\n",
    "        start_quarters = q_note.quantized_start_step / steps_per_quarter\n",
    "        end_quarters = q_note.quantized_end_step / steps_per_quarter\n",
    "        new_note.start_time = start_quarters * seconds_per_quarter\n",
    "        new_note.end_time = end_quarters * seconds_per_quarter\n",
    "    # Set the total time of the new sequence.\n",
    "    total_quarters = quantized_temp_chunk.total_quantized_steps / steps_per_quarter\n",
    "    grid_aligned_ns.total_time = total_quarters * seconds_per_quarter\n",
    "    return grid_aligned_ns\n",
    "\n",
    "def set_program_for_all_notes(note_sequence, program_number=0):\n",
    "    \"\"\"\n",
    "    Resets the instrument program for every note in a NoteSequence.\n",
    "    Args:\n",
    "      note_sequence: The note_seq.NoteSequence object to modify.\n",
    "      program_number: The integer program number to set for all notes.\n",
    "                      Defaults to 0 (Acoustic Grand Piano).\n",
    "    Returns:\n",
    "      The modified NoteSequence.\n",
    "    \"\"\"\n",
    "    for note in note_sequence.notes:\n",
    "        note.program = program_number\n",
    "    return note_sequence\n",
    "\n",
    "def estimate_tempo_from_notes(\n",
    "    note_sequence: music_pb2.NoteSequence,\n",
    "    min_bpm: float = 60.0,\n",
    "    max_bpm: float = 240.0,\n",
    "    prior_bpm: float = 120.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimates the tempo of an unquantized NoteSequence by analyzing note onsets.\n",
    "\n",
    "    Args:\n",
    "        note_sequence: An unquantized NoteSequence object.\n",
    "        min_bpm: The minimum plausible tempo to consider.\n",
    "        max_bpm: The maximum plausible tempo to consider.\n",
    "        prior_bpm: The tempo to prefer (e.g., 120 BPM). The algorithm will favor\n",
    "                   candidates closer to this value.\n",
    "\n",
    "    Returns:\n",
    "        The estimated tempo in beats per minute (BPM). Returns prior_bpm if\n",
    "        not enough notes are present to make a guess.\n",
    "    \"\"\"\n",
    "    # 1. Extract unique, sorted note onset times\n",
    "    onsets = sorted(list(set(note.start_time for note in note_sequence.notes)))\n",
    "\n",
    "    if len(onsets) < 3:  # Need a reasonable number of notes for a good guess\n",
    "        print(\"Warning: Too few notes to reliably estimate tempo. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 2. Calculate Inter-Onset Intervals (IOIs)\n",
    "    iois = np.diff(onsets)\n",
    "    if len(iois) == 0:\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 3. Build a histogram of IOIs to find the most common intervals\n",
    "    # We use a small bin size to capture fine timing details\n",
    "    hist, bin_edges = np.histogram(iois, bins=np.arange(0, 5, 0.01), density=False)\n",
    "    \n",
    "    # Find peaks in the histogram. These are our primary rhythmic intervals.\n",
    "    # A simple way is to get the top N bins.\n",
    "    peak_indices = np.argsort(hist)[-10:] # Get indices of 10 strongest peaks\n",
    "    \n",
    "    tempo_candidates = defaultdict(float)\n",
    "\n",
    "    # 4. Generate and score tempo candidates from histogram peaks\n",
    "    for i in peak_indices:\n",
    "        if hist[i] < 2: # Ignore insignificant peaks\n",
    "            continue\n",
    "            \n",
    "        # The time (in seconds) corresponding to this peak\n",
    "        interval = bin_edges[i]\n",
    "        \n",
    "        # This interval could be a quarter note, eighth note, etc.\n",
    "        # Generate hypotheses based on this interval.\n",
    "        for multiple in [0.25, 0.33, 0.5, 1, 2, 3, 4]:\n",
    "            potential_beat_duration = interval * multiple\n",
    "            if potential_beat_duration == 0:\n",
    "                continue\n",
    "            \n",
    "            tempo = 60.0 / potential_beat_duration\n",
    "            \n",
    "            if min_bpm <= tempo <= max_bpm:\n",
    "                # 5. Score the candidate\n",
    "                # Score part 1: Rhythmic Strength (how strong was the peak?)\n",
    "                strength_score = hist[i]\n",
    "                \n",
    "                # Score part 2: Proximity to prior_bpm (Gaussian score)\n",
    "                # This gives a high score if tempo is near prior_bpm\n",
    "                proximity_score = np.exp(-0.5 * ((tempo - prior_bpm) / 20.0)**2)\n",
    "                \n",
    "                # Combine scores and add to any existing score for this tempo\n",
    "                combined_score = strength_score * proximity_score\n",
    "                tempo_candidates[tempo] += combined_score\n",
    "\n",
    "    if not tempo_candidates:\n",
    "        print(\"Warning: Could not find any valid tempo candidates. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 6. Return the tempo with the highest score\n",
    "    best_tempo = max(tempo_candidates, key=tempo_candidates.get)\n",
    "    return best_tempo\n",
    "\n",
    "def convert_note_sequence_to_input_tensor(note_sequence: music_pb2.NoteSequence) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts a NoteSequence into the input tensor format expected by the TFLite model.\n",
    "    Args:\n",
    "        note_sequence: A NoteSequence object to convert.\n",
    "    Returns:\n",
    "        A numpy array of shape (1, sequence_length, feature_dim) suitable for model input.\n",
    "    \"\"\"\n",
    "    data_converter = mel_2bar_config.data_converter\n",
    "    extracted_tensors = data_converter.to_tensors(note_sequence)\n",
    "\n",
    "    inputs = []\n",
    "    \n",
    "\n",
    "    if not extracted_tensors.inputs:\n",
    "        raise NoExtractedExamplesError(\n",
    "            'No examples extracted from NoteSequence: %s' % note_sequence)\n",
    "    if len(extracted_tensors.inputs) > 1:\n",
    "        raise MultipleExtractedExamplesError(\n",
    "            'Multiple (%d) examples extracted from NoteSequence: %s' %\n",
    "            (len(extracted_tensors.inputs), note_sequence))\n",
    "    inputs.append(extracted_tensors.inputs[0])\n",
    "    #controls.append(extracted_tensors.controls[0])\n",
    "    #lengths.append(extracted_tensors.lengths[0])\n",
    "    # Stack and reshape to match model input\n",
    "    input_tensor = np.stack(inputs).astype(np.float32)\n",
    "    return input_tensor\n",
    "    \n",
    "\n",
    "def get_embeddings_for_song(track_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Finds all MIDI files for a given track_id, generates embeddings for each,\n",
    "    and returns them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        track_id (str): The ID of the track, e.g., \"TRAAAGR128F425B14B\".\n",
    "        root_path (str): The root path of the repository.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are MIDI filenames and values are lists of\n",
    "        numpy array embeddings generated from that MIDI file.\n",
    "        Returns an empty dictionary if the folder is not found or contains no MIDI files.\n",
    "    \"\"\"\n",
    "    # 1. Construct the folder path from the track_id\n",
    "    # e.g., 'data_sets/lmd_melodies/A/A/A/TRAAAGR128F425B14B'\n",
    "    if len(track_id) < 5:\n",
    "        print(f\"Error: track_id '{track_id}' is too short to build a path.\")\n",
    "        return {}\n",
    "        \n",
    "    song_folder_path = os.path.join(\n",
    "        'data_sets',\n",
    "        'lmd_melodies',\n",
    "        track_id[2],\n",
    "        track_id[3],\n",
    "        track_id[4],\n",
    "        track_id\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(song_folder_path):\n",
    "        print(f\"Warning: Directory not found at {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Find all MIDI files in the directory\n",
    "    midi_filepaths = glob.glob(os.path.join(song_folder_path, '*.mid'))\n",
    "    midi_filepaths.extend(glob.glob(os.path.join(song_folder_path, '*.midi')))\n",
    "\n",
    "    if not midi_filepaths:\n",
    "        print(f\"Warning: No MIDI files found in {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Process each MIDI file to generate embeddings\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    \n",
    "\n",
    "    for midi_path in midi_filepaths:\n",
    "        filename = os.path.basename(midi_path)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and quantize the MIDI file\n",
    "            midi_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "            print(f\"Processing {filename}...\")\n",
    "            cleaned_quantized_list = []\n",
    "            qpm = estimate_tempo_from_notes(midi_ns)\n",
    "            seconds_per_quarter = 60.0 / qpm\n",
    "            steps_per_quarter=mel_2bar_config.data_converter._steps_per_quarter \n",
    "            seconds_per_step = seconds_per_quarter / steps_per_quarter\n",
    "            num_steps_per_chunk = mel_2bar_config.hparams.max_seq_len\n",
    "            hop_size_in_seconds = num_steps_per_chunk * seconds_per_step # 32 / 4 = 8.0 seconds\n",
    "            cleaned_ms = make_monophonic(midi_ns)\n",
    "            \n",
    "            cleaned_ms = snap_chunk_notes_to_grid(cleaned_ms, steps_per_quarter)\n",
    "            cleaned_ms = set_program_for_all_notes(cleaned_ms, program_number=0)\n",
    "            \n",
    "            \n",
    "\n",
    "            if cleaned_ms.notes:\n",
    "                slices = sequences_lib.split_note_sequence(\n",
    "                    note_sequence=cleaned_ms,\n",
    "                    hop_size_seconds=hop_size_in_seconds  \n",
    "                )\n",
    "                for chunk in slices:\n",
    "                    cleaned_quantized_list.append(chunk)\n",
    "            embeddings=[]\n",
    "            \n",
    "            for chunk in cleaned_quantized_list:\n",
    "                try:\n",
    "                    input_tensor = convert_note_sequence_to_input_tensor(chunk)\n",
    "                    interpreter.set_tensor(tflite_input_details[0]['index'], input_tensor)\n",
    "                    interpreter.invoke()\n",
    "                    tflite_embedding = interpreter.get_tensor(tflite_output_details[0]['index'])\n",
    "                    embedding = np.array(tflite_embedding[0])\n",
    "                    embeddings.append(embedding)\n",
    "                except NoExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, insufficient note data\")\n",
    "                except MultipleExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, multiple examples extracted\")    \n",
    "                continue\n",
    "            if(len(embeddings)>0):\n",
    "                all_embeddings[filename] = embeddings\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Could not process file {filename}. Error: {e}\")\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "\n",
    "# --- NEW: Main Orchestrator Function ---\n",
    "def process_tracks_from_db(\n",
    "    start_line_to_process: int=0,\n",
    "    num_tracks_to_process: int = 1,\n",
    "    db_path: str='data_sets/track_metadata.db'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Reads track_ids from an SQLite database, generates embeddings for each,\n",
    "    and returns a nested dictionary of all embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_tracks_to_process (int, optional): The number of tracks to process.\n",
    "                                               If None, processes all tracks. Defaults to 1.\n",
    "        db_path (str): Path to the track_metadata.db SQLite file.\n",
    "       \n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are track_ids and values are the dictionaries\n",
    "        returned by get_embeddings_for_song.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database not found at {db_path}\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Connecting to database: {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 1. Fetch the track_ids from the database\n",
    "    query = \"SELECT track_id FROM songs\" # Assuming the table is named 'songs'\n",
    "    if num_tracks_to_process is not None and num_tracks_to_process > 0:\n",
    "        num_tracks_to_query=num_tracks_to_process\n",
    "        query += f\" LIMIT {start_line_to_process}, {num_tracks_to_query}\"\n",
    "    \n",
    "    print(\"Fetching track_ids...\")\n",
    "    cursor.execute(query)\n",
    "    # Fetch all rows and flatten the list of tuples [('id1',), ('id2',)] -> ['id1', 'id2']\n",
    "    track_ids = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    \n",
    "    if not track_ids:\n",
    "        print(\"No track_ids found in the database.\")\n",
    "        return {}\n",
    "\n",
    "    total_tracks = len(track_ids)\n",
    "    print(f\"Found {total_tracks} track_ids to process.\")\n",
    "\n",
    "    # 2. Iterate through track_ids and get embeddings for each\n",
    "    all_track_embeddings = {}\n",
    "    for i, track_id in enumerate(track_ids):\n",
    "        print(f\"\\n--- Processing track {i + 1}/{total_tracks}: {track_id} ---\")\n",
    "        \n",
    "        song_embeddings = get_embeddings_for_song(track_id)\n",
    "        \n",
    "        if song_embeddings:\n",
    "            all_track_embeddings[track_id] = song_embeddings\n",
    "            print(f\"-> Success: Found and processed {len(song_embeddings)} MIDI file(s) for this track.\")\n",
    "            \n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    return all_track_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b44388cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index with dimension 512.\n",
      "Manager initialized with 0 vectors, 0 unique hashes, and 0 tracks in metadata.\n",
      "FaissRDAManager initialized. Tracking 0 embeddings in 'data_sets\\faiss\\embeddings.rda'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DB_FOLDER = os.path.join(\"data_sets\",\"faiss\")\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "METADATA_FILE =\"faiss_metadata.json\"\n",
    "FINGERPRINT_FILE =  \"embedding_fingerprints.json\"\n",
    "RDA_FILE = \"embeddings.rda\"\n",
    "\n",
    "\n",
    "# -------------------\n",
    "\n",
    "class FaissManager:\n",
    "    \"\"\"\n",
    "    Manages a FAISS index with a deduplication layer, designed to accept\n",
    "    simple 1D lists of numbers as embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dimension: int = 512, \n",
    "                 index_file: str = 'faiss.index', \n",
    "                 hashes_file: str = 'embeddings_hashes.json', \n",
    "                 metadata_file: str = 'metadata.json',\n",
    "                 data_root: str = 'data_sets/faiss'):\n",
    "        \"\"\"\n",
    "        Initializes the manager.\n",
    "\n",
    "        Args:\n",
    "            dimension: The dimension of the embedding vectors (e.g., 512).\n",
    "            index_file: Path to save/load the FAISS index.\n",
    "            hashes_file: Path to the JSON file storing unique hashes.\n",
    "            data_root: Root directory for storing FAISS data files. Defaults to 'data_sets/faiss'.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.data_root = data_root\n",
    "        self.index_file = os.path.join(data_root,index_file)\n",
    "        self.hashes_file = os.path.join(data_root,hashes_file)\n",
    "        self.metadata_file = os.path.join(data_root,metadata_file)\n",
    "        \n",
    "        # Load or initialize the FAISS index\n",
    "\n",
    "        # Load or initialize the FAISS index\n",
    "        self.index = self._load_faiss_index()\n",
    "            \n",
    "        # Load existing hashes for deduplication\n",
    "        self.existing_hashes = self._load_json_to_set(self.hashes_file)\n",
    "        \n",
    "        # Load the metadata mapping track_ids to FAISS indices\n",
    "        # Use defaultdict to simplify adding new track_ids\n",
    "        self.metadata = defaultdict(list, self._load_json_to_dict(self.metadata_file))\n",
    "        \n",
    "        self.generate_faiss_index_to_track_id_lookup()\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"Manager initialized with {self.index.ntotal} vectors, \"\n",
    "            f\"{len(self.existing_hashes)} unique hashes, and \"\n",
    "            f\"{len(self.metadata)} tracks in metadata.\"\n",
    "        )\n",
    "\n",
    "    def _load_faiss_index(self) -> faiss.Index:\n",
    "        \"\"\"Loads or creates a FAISS index.\"\"\"\n",
    "        if os.path.exists(self.index_file):\n",
    "            print(f\"Loading existing FAISS index from {self.index_file}\")\n",
    "            index = faiss.read_index(self.index_file)\n",
    "            if index.d != self.dimension:\n",
    "                raise ValueError(f\"Index dimension mismatch: loaded index has {index.d}, manager expects {self.dimension}.\")\n",
    "            return index\n",
    "        else:\n",
    "            print(f\"Creating new FAISS index with dimension {self.dimension}.\")\n",
    "            return faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "    def _load_json_to_set(self, file_path: str) -> set:\n",
    "        \"\"\"Loads a JSON array from a file into a set.\"\"\"\n",
    "        if not os.path.exists(file_path): return set()\n",
    "        try:\n",
    "            with open(file_path, 'r') as f: return set(json.load(f))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            print(f\"Warning: {file_path} is corrupted. Starting fresh.\")\n",
    "            return set()\n",
    "\n",
    "    def _load_json_to_dict(self, file_path: str) -> dict:\n",
    "        \"\"\"Loads a JSON object from a file into a dict.\"\"\"\n",
    "        if not os.path.exists(file_path): return {}\n",
    "        try:\n",
    "            with open(file_path, 'r') as f: return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: {file_path} is corrupted. Starting fresh.\")\n",
    "            return {}\n",
    "\n",
    "    def _generate_hash(self, track_id: str, embedding: list[float]) -> str:\n",
    "        \"\"\"Generates a SHA-256 hash for a track_id and embedding.\"\"\"\n",
    "\n",
    "         # --- FIX ---\n",
    "    # Convert the embedding to a standard Python list if it's a NumPy array.\n",
    "    # The hasattr check is a safe way to detect NumPy-like objects.\n",
    "        if hasattr(embedding, 'tolist'):\n",
    "            embedding_list = embedding.tolist()\n",
    "        else:\n",
    "            embedding_list = embedding\n",
    "        # --- END FIX ---\n",
    "\n",
    "        data_to_hash = {\"track_id\": track_id, \"embedding\": embedding_list}\n",
    "        canonical_string = json.dumps(data_to_hash, sort_keys=True, separators=(',', ':'))\n",
    "        return hashlib.sha256(canonical_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "    import numpy as np\n",
    "# Assume other necessary parts of the class like __init__ are defined elsewhere.\n",
    "\n",
    "    def add_embedding(self, track_id: str, embedding: list[float]) -> int | None:\n",
    "        \"\"\"\n",
    "        Adds an embedding, updating the FAISS index and metadata if not a duplicate.\n",
    "\n",
    "        Args:\n",
    "            track_id: The identifier for the track.\n",
    "            embedding: A 1D list of floats representing the embedding.\n",
    "\n",
    "        Returns:\n",
    "            The new Faiss index (int) if the embedding was added, None otherwise.\n",
    "        \"\"\"\n",
    "        if len(embedding) != self.dimension:\n",
    "            print(f\"Error: Embedding length {len(embedding)} != index dimension {self.dimension}.\")\n",
    "            return None\n",
    "\n",
    "        new_hash = self._generate_hash(track_id, embedding)\n",
    "        if new_hash in self.existing_hashes:\n",
    "            # This is now a silent failure as the batch method will handle reporting.\n",
    "            # print(f\"Duplicate found for (track_id, embedding) pair: '{track_id}'. Skipping.\")\n",
    "            return None\n",
    "        \n",
    "        # Get the numerical index for the new vector *before* adding it.\n",
    "        new_faiss_id = self.index.ntotal\n",
    "    \n",
    "        # Add to FAISS index\n",
    "        vector_batch = np.array([embedding], dtype='float32')\n",
    "        self.index.add(vector_batch)\n",
    "    \n",
    "        # Update hashes\n",
    "        self.existing_hashes.add(new_hash)\n",
    "    \n",
    "        # Update metadata\n",
    "        # Assuming self.metadata is a defaultdict(list) or similar\n",
    "        self.metadata[track_id].append(new_faiss_id)\n",
    "    \n",
    "        print(f\"Added new embedding for track_id '{track_id}' at FAISS index {new_faiss_id}.\")\n",
    "        self.generate_faiss_index_to_track_id_lookup()\n",
    "    \n",
    "        # Return the new index ID on success\n",
    "        return new_faiss_id\n",
    "\n",
    "    \n",
    "    \n",
    "    # --- UPDATED FUNCTION ---\n",
    "    def add_embeddings(self, track_ids: list[str], embeddings: list[list[float]]) -> tuple[list[str], list[list[float]], list[int]]:\n",
    "        \"\"\"\n",
    "        Adds a batch of embeddings, ensuring no duplicates based on track_id.\n",
    "\n",
    "        Args:\n",
    "            track_ids: A list of track identifiers.\n",
    "            embeddings: A list of 1D embedding lists (e.g., of 512 numbers each).\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing three lists:\n",
    "            - The list of track_ids for new embeddings that were successfully added.\n",
    "            - The corresponding list of the new embeddings themselves.\n",
    "            - The list of corresponding Faiss indices for the new embeddings.\n",
    "        \"\"\"\n",
    "        if len(track_ids) != len(embeddings):\n",
    "            raise ValueError(\"Input error: The number of track_ids must match the number of embeddings.\")\n",
    "\n",
    "        if not track_ids:\n",
    "            print(\"Warning: Called add_embeddings with empty lists.\")\n",
    "            return [], [], []\n",
    "\n",
    "        added_track_ids = []\n",
    "        added_embeddings = []\n",
    "        added_faiss_indices = []\n",
    "\n",
    "        for track_id, embedding in zip(track_ids, embeddings):\n",
    "            if len(embedding) != self.dimension:\n",
    "                print(f\"Warning: Skipping embedding for track '{track_id}'. Invalid dimension {len(embedding)}.\")\n",
    "                continue\n",
    "        \n",
    "            # This helper method is now expected to return the Faiss index (int) on success, or None on failure.\n",
    "            new_index_id = self.add_embedding(track_id, embedding)\n",
    "        \n",
    "            if new_index_id is not None:\n",
    "                added_track_ids.append(track_id)\n",
    "                added_embeddings.append(embedding)\n",
    "                added_faiss_indices.append(new_index_id)\n",
    "\n",
    "        # Return the lists of added data, now including the Faiss indices\n",
    "        return added_track_ids, added_embeddings, added_faiss_indices\n",
    "\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Saves the FAISS index, deduplication hashes, and metadata to disk.\"\"\"\n",
    "        print(\"\\n--- Saving all data ---\")\n",
    "\n",
    "\n",
    "        # --- FIX: Ensure destination directories exist before writing ---\n",
    "    # Create a set of unique directory paths to avoid redundant checks\n",
    "        dir_paths = {\n",
    "            os.path.dirname(self.index_file),\n",
    "            os.path.dirname(self.hashes_file),\n",
    "            os.path.dirname(self.metadata_file)\n",
    "        }\n",
    "\n",
    "        for path in dir_paths:\n",
    "        # An empty path means the file is in the current directory, no need to create.\n",
    "            if path:\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                print(f\"Ensured directory exists: {path}\")\n",
    "    # --- END FIX ---\n",
    "        \n",
    "        # 1. Save FAISS index\n",
    "        faiss.write_index(self.index, self.index_file)\n",
    "        print(f\"FAISS index with {self.index.ntotal} vectors saved to {self.index_file}.\")\n",
    "        \n",
    "        # 2. Save hashes\n",
    "        with open(self.hashes_file, 'w') as f:\n",
    "            json.dump(list(self.existing_hashes), f)\n",
    "        print(f\"{len(self.existing_hashes)} hashes saved to {self.hashes_file}.\")\n",
    "        \n",
    "        # 3. Save metadata\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        print(f\"Metadata for {len(self.metadata)} tracks saved to {self.metadata_file}.\")\n",
    "        \n",
    "        print(\"Save complete.\")\n",
    "    \n",
    "    # Add this code inside your FaissManager's __init__ method, after self.metadata is loaded.\n",
    "\n",
    "# Create a reverse mapping from FAISS index ID -> track_id for fast lookups.\n",
    "    def generate_faiss_index_to_track_id_lookup(self):\n",
    "        self.reverse_metadata = {}\n",
    "        for track_id, faiss_ids in self.metadata.items():\n",
    "            for faiss_id in faiss_ids:\n",
    "                self.reverse_metadata[faiss_id] = track_id\n",
    "\n",
    "    \n",
    "    def search_best_match(self, query_embedding: list[float]) -> tuple[str | None, float | None]:\n",
    "        \"\"\"\n",
    "        Searches the FAISS index for the single best match to a query embedding.\n",
    "\n",
    "        Args:\n",
    "            query_embedding: A 1D list of floats representing the observed embedding.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - The track_id of the best match (or None if not found).\n",
    "            - The L2 distance (similarity score) to the best match (or None if not found).\n",
    "        \"\"\"\n",
    "        if self.index.ntotal == 0:\n",
    "            print(\"Warning: Search attempted on an empty index.\")\n",
    "            return None, None\n",
    "\n",
    "        if len(query_embedding) != self.dimension:\n",
    "            print(f\"Error: Query embedding has dimension {len(query_embedding)}, but index requires {self.dimension}.\")\n",
    "            return None, None\n",
    "\n",
    "        # FAISS requires a 2D numpy array for queries.\n",
    "        query_vector = np.array([query_embedding], dtype='float32')\n",
    "\n",
    "        # Perform the search for the 1 nearest neighbor (k=1).\n",
    "        # D: distances, I: indices\n",
    "        distances, indices = self.index.search(query_vector, k=1)\n",
    "\n",
    "        # The result for the first (and only) query vector is at index 0.\n",
    "        best_faiss_id = indices[0][0]\n",
    "        best_distance = distances[0][0]\n",
    "\n",
    "        # If the index is empty, FAISS can return -1.\n",
    "        if best_faiss_id == -1:\n",
    "            return None, None\n",
    "        \n",
    "    # Use the reverse metadata map to find the track_id.\n",
    "        track_id = self.reverse_metadata.get(best_faiss_id)\n",
    "\n",
    "        if track_id is None:\n",
    "            # This would indicate an inconsistency between the index and metadata.\n",
    "            print(f\"Warning: FAISS ID {best_faiss_id} found, but not in metadata.\")\n",
    "            return None, None\n",
    "\n",
    "        return track_id, float(best_distance)\n",
    "    \n",
    "#===================================\n",
    "\n",
    "class FaissRDAManager(FaissManager):\n",
    "    \"\"\"\n",
    "    Extends FaissManager to also manage an .rda file with a specific\n",
    "    data structure: track_id (factor), faiss_index (numeric), emb... (numeric).\n",
    "    \"\"\"\n",
    "    def __init__(self, rda_path: str, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the FaissRDAManager.\n",
    "\n",
    "        Args:\n",
    "            rda_path (str): The file path for the .rda file.\n",
    "            *args, **kwargs: Arguments for the parent FaissManager (e.g., index_path, dimension).\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.rda_path = os.path.join(self.data_root, rda_path)\n",
    "        \n",
    "        # Define column names\n",
    "        self.embedding_columns = [f\"emb{i:03d}\" for i in range(1, self.dimension + 1)]\n",
    "        self.all_columns = ['track_id', 'faiss_index'] + self.embedding_columns\n",
    "        \n",
    "        # NEW: Define the pandas dtypes to ensure correct R types.\n",
    "        # 'category' -> R factor\n",
    "        # 'int32'/'float32' -> R numeric\n",
    "        self.column_dtypes = {\n",
    "            'track_id': 'category',\n",
    "            'faiss_index': 'int32'\n",
    "        }\n",
    "        self.column_dtypes.update({col: 'float32' for col in self.embedding_columns})\n",
    "        \n",
    "        self.embeddings_df = self._load_rda()\n",
    "        \n",
    "        print(f\"FaissRDAManager initialized. Tracking {len(self.embeddings_df)} embeddings in '{self.rda_path}'.\")\n",
    "\n",
    "    def _load_rda(self) -> pd.DataFrame:\n",
    "        \"\"\"Loads embeddings from the .rda file or creates an empty DataFrame with correct types.\"\"\"\n",
    "        df = pd.DataFrame(columns=self.all_columns) # Start with a base empty frame\n",
    "        if os.path.exists(self.rda_path):\n",
    "            try:\n",
    "                print(f\"Loading existing embeddings from '{self.rda_path}'...\")\n",
    "                r_objects = pyreadr.read_r(self.rda_path)\n",
    "                loaded_df = r_objects.get(\"embeddings_data\")\n",
    "                if loaded_df is not None:\n",
    "                    df = loaded_df\n",
    "                    df.columns = self.all_columns # Ensure column names are correct\n",
    "                else:\n",
    "                    print(\"Warning: RDA file exists but contains no 'embeddings_data' object.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read RDA file at '{self.rda_path}'. Error: {e}.\")\n",
    "        \n",
    "        # NEW: Enforce the correct dtypes on the loaded or newly created DataFrame.\n",
    "        # This is the crucial step for ensuring type consistency.\n",
    "        return df.astype(self.column_dtypes)\n",
    "\n",
    "    def _update_rda_file(self):\n",
    "        \"\"\"Writes the current DataFrame of embeddings to the .rda file.\"\"\"\n",
    "        if self.embeddings_df.empty:\n",
    "            return\n",
    "\n",
    "        print(f\"Updating RDA file at '{self.rda_path}'...\")\n",
    "        # 3. Verify the dtypes before writing (optional but good for debugging)\n",
    "        print(\"--- dtypes of the final DataFrame before writing ---\")\n",
    "        print(self.embeddings_df[self.embeddings_df.keys()[4 ]].dtypes)\n",
    "        print(self.embeddings_df[self.embeddings_df.keys()[4 ]].values[0])\n",
    "        # --- FIX for float32 not being recognized in pyrdr: Convert all float32 columns to float64 for pyreadr compatibility ---\n",
    "        for col in self.embeddings_df.select_dtypes(include=['float32']).columns:\n",
    "            self.embeddings_df[col] = self.embeddings_df[col].astype('float64')\n",
    "\n",
    "        self.embeddings_df.columns = self.embeddings_df.columns.astype(str)\n",
    "        pyreadr.write_rdata(self.rda_path, self.embeddings_df, df_name=\"embeddings_data\")\n",
    "        print(\"RDA file successfully updated.\")\n",
    "\n",
    "    def add_embeddings(self, track_ids: list[str], embeddings: list[list[float]]) -> tuple[list[str], list[list[float]], list[int]]:\n",
    "        \"\"\"\n",
    "        Adds new embeddings to the FAISS index and updates the .rda file\n",
    "        with the structured DataFrame.\n",
    "        \"\"\"\n",
    "        newly_added_ids, newly_added_embeddings, newly_added_faiss_indices = super().add_embeddings(track_ids, embeddings)\n",
    "\n",
    "        if newly_added_embeddings:\n",
    "            print(f\"Detected {len(newly_added_embeddings)} new embeddings to add to RDA file.\")\n",
    "            \n",
    "            new_data = {\n",
    "                'track_id': newly_added_ids,\n",
    "                'faiss_index': newly_added_faiss_indices,\n",
    "                **dict(zip(self.embedding_columns, np.array(newly_added_embeddings, dtype=np.float32).T))\n",
    "            }\n",
    "            new_df = pd.DataFrame(new_data)\n",
    "            \n",
    "            # NEW: Enforce the schema on the new data before concatenating.\n",
    "\n",
    "            new_df = new_df.astype(self.column_dtypes)\n",
    "            \n",
    "            # Append to the main DataFrame\n",
    "            self.embeddings_df = pd.concat([self.embeddings_df, new_df], ignore_index=True).astype(self.column_dtypes)\n",
    "\n",
    "            # Iterate through the columns you want to convert\n",
    "            for col, dtype in self.column_dtypes.items():\n",
    "                if 'float' in str(dtype):\n",
    "                # Use pd.to_numeric with errors='coerce'.\n",
    "                # This will turn any non-numeric strings (like '') into NaN.\n",
    "                    self.embeddings_df[col] = pd.to_numeric(self.embeddings_df[col], errors='coerce')\n",
    "            \n",
    "            self._update_rda_file()\n",
    "        else:\n",
    "            print(\"No new non-duplicate embeddings were added. RDA file is already up-to-date.\")\n",
    "        \n",
    "        return newly_added_ids, newly_added_embeddings, newly_added_faiss_indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Instantiate it for further use\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===========================================\n",
    "# --\n",
    "manager = FaissRDAManager( rda_path=RDA_FILE,\n",
    "                           dimension=EMBEDDING_DIM,\n",
    "                           index_file=INDEX_FILE,\n",
    "                           hashes_file=FINGERPRINT_FILE,\n",
    "                           metadata_file=METADATA_FILE,\n",
    "                           data_root=DB_FOLDER)\n",
    "\n",
    "\n",
    "\n",
    "    # Add multiple embeddings for the same track\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293abeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database: data_sets/track_metadata.db\n",
      "Fetching track_ids...\n",
      "Found 50 track_ids to process.\n",
      "\n",
      "--- Processing track 1/50: TRAAAAK128F9318786 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAK128F9318786\n",
      "\n",
      "--- Processing track 2/50: TRAAAAV128F421A322 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAV128F421A322\n",
      "\n",
      "--- Processing track 3/50: TRAAAAW128F429D538 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAW128F429D538\n",
      "\n",
      "--- Processing track 4/50: TRAAAAY128F42A73F0 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAY128F42A73F0\n",
      "\n",
      "--- Processing track 5/50: TRAAABD128F429CF47 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAABD128F429CF47\n",
      "\n",
      "--- Processing track 6/50: TRAAACN128F9355673 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAACN128F9355673\n",
      "\n",
      "--- Processing track 7/50: TRAAACV128F423E09E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAACV128F423E09E\n",
      "\n",
      "--- Processing track 8/50: TRAAADJ128F4287B47 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADJ128F4287B47\n",
      "\n",
      "--- Processing track 9/50: TRAAADT12903CCC339 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADT12903CCC339\n",
      "\n",
      "--- Processing track 10/50: TRAAADZ128F9348C2E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADZ128F9348C2E\n",
      "\n",
      "--- Processing track 11/50: TRAAAEA128F935A30D ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEA128F935A30D\n",
      "\n",
      "--- Processing track 12/50: TRAAAED128E0783FAB ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAED128E0783FAB\n",
      "\n",
      "--- Processing track 13/50: TRAAAEF128F4273421 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEF128F4273421\n",
      "\n",
      "--- Processing track 14/50: TRAAAEM128F93347B9 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEM128F93347B9\n",
      "\n",
      "--- Processing track 15/50: TRAAAEW128F42930C0 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEW128F42930C0\n",
      "\n",
      "--- Processing track 16/50: TRAAAFD128F92F423A ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFD128F92F423A\n",
      "\n",
      "--- Processing track 17/50: TRAAAFI12903CE4F0E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFI12903CE4F0E\n",
      "\n",
      "--- Processing track 18/50: TRAAAFP128F931B4E3 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFP128F931B4E3\n",
      "\n",
      "--- Processing track 19/50: TRAAAFW128F42A4CFD ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFW128F42A4CFD\n",
      "\n",
      "--- Processing track 20/50: TRAAAGF12903CEC202 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAGF12903CEC202\n",
      "\n",
      "--- Processing track 21/50: TRAAAGR128F425B14B ---\n",
      "Processing 1d9d16a9da90c090809c153754823c2b.mid...\n",
      "Processing 1d9d16a9da90c090809c153754823c2b.mid...\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Processing 5dd29e99ed7bd3cc0c5177a6e9de22ea.mid...\n",
      "Processing 5dd29e99ed7bd3cc0c5177a6e9de22ea.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Processing b97c529ab9ef783a849b896816001748.mid...\n",
      "Processing b97c529ab9ef783a849b896816001748.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Processing dac3cdd0db6341d8dc14641e44ed0d44.mid...\n",
      "Processing dac3cdd0db6341d8dc14641e44ed0d44.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "-> Success: Found and processed 4 MIDI file(s) for this track.\n",
      "\n",
      "--- Processing track 22/50: TRAAAGW12903CC1049 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAGW12903CC1049\n",
      "\n",
      "--- Processing track 23/50: TRAAAHD128F42635A5 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHD128F42635A5\n",
      "\n",
      "--- Processing track 24/50: TRAAAHE12903C9669C ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHE12903C9669C\n",
      "\n",
      "--- Processing track 25/50: TRAAAHJ128F931194C ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHJ128F931194C\n",
      "\n",
      "--- Processing track 26/50: TRAAAHO128F423BBE3 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHO128F423BBE3\n",
      "\n",
      "--- Processing track 27/50: TRAAAHZ128E0799171 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHZ128E0799171\n",
      "\n",
      "--- Processing track 28/50: TRAAAIC128F14A5138 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAIC128F14A5138\n",
      "\n",
      "--- Processing track 29/50: TRAAAIR128F1480971 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAIR128F1480971\n",
      "\n",
      "--- Processing track 30/50: TRAAAJG128F9308A25 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAJG128F9308A25\n",
      "\n",
      "--- Processing track 31/50: TRAAAJN128F428E437 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAJN128F428E437\n",
      "\n",
      "--- Processing track 32/50: TRAAAJO12903CAAC69 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAJO12903CAAC69\n",
      "\n",
      "--- Processing track 33/50: TRAAAKO128F426441E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAKO128F426441E\n",
      "\n",
      "--- Processing track 34/50: TRAAAMG128F9318C5E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAMG128F9318C5E\n",
      "\n",
      "--- Processing track 35/50: TRAAAMO128F1481E7F ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAMO128F1481E7F\n",
      "\n",
      "--- Processing track 36/50: TRAAAMQ128F1460CD3 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAMQ128F1460CD3\n",
      "\n",
      "--- Processing track 37/50: TRAAAND12903CD1F1B ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAND12903CD1F1B\n",
      "\n",
      "--- Processing track 38/50: TRAAANK128F428B515 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAANK128F428B515\n",
      "\n",
      "--- Processing track 39/50: TRAAAOF128F429C156 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAOF128F429C156\n",
      "\n",
      "--- Processing track 40/50: TRAAAPK128E0786D96 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAPK128E0786D96\n",
      "\n",
      "--- Processing track 41/50: TRAAAQN128F9353BA0 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAQN128F9353BA0\n",
      "\n",
      "--- Processing track 42/50: TRAAAQO12903CD8E1C ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAQO12903CD8E1C\n",
      "\n",
      "--- Processing track 43/50: TRAAARJ128F9320760 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAARJ128F9320760\n",
      "\n",
      "--- Processing track 44/50: TRAAAUC128F428716F ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAUC128F428716F\n",
      "\n",
      "--- Processing track 45/50: TRAAAUR128F428B1FA ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAUR128F428B1FA\n",
      "\n",
      "--- Processing track 46/50: TRAAAVG12903CFA543 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAVG12903CFA543\n",
      "\n",
      "--- Processing track 47/50: TRAAAVH128F42554C6 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAVH128F42554C6\n",
      "\n",
      "--- Processing track 48/50: TRAAAVL128F93028BC ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAVL128F93028BC\n",
      "\n",
      "--- Processing track 49/50: TRAAAVO128F93133D4 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAVO128F93133D4\n",
      "\n",
      "--- Processing track 50/50: TRAAAYL128F4271A5B ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAYL128F4271A5B\n",
      "\n",
      "--- Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings for a set of tracks from the database\n",
    "all_embeddings = process_tracks_from_db(\n",
    "    start_line_to_process=0,\n",
    "    num_tracks_to_process=50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e02efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total embeddings to add to FAISS index: 150\n",
      "\n",
      "Length of each embedding vector: 512\n"
     ]
    }
   ],
   "source": [
    "total_embeddings = sum(len(array) for track_dict in all_embeddings.values() for array in track_dict.values())\n",
    "\n",
    "track_ids = []\n",
    "\n",
    "# Iterate through each track ID and its corresponding inner dictionary\n",
    "for track_id, inner_dict in all_embeddings.items():\n",
    "    # Calculate the total number of embeddings for the current track_id\n",
    "    # This uses the sum() and generator expression method from before\n",
    "    count = sum(len(array) for array in inner_dict.values())  \n",
    "    # Extend the main list by adding the track_id 'count' times\n",
    "    # The expression [track_id] * count creates a new list like ['document_1', 'document_1', ...]\n",
    "    track_ids.extend([track_id] * count)\n",
    "\n",
    "# Flatten all numbers into a single list\n",
    "embedding_matrix = []\n",
    "\n",
    "# Loop through the top-level dictionary\n",
    "for inner_dict in all_embeddings.values():\n",
    "    # Loop through the second-level dictionary\n",
    "    for list_of_tuples in inner_dict.values():\n",
    "        # Loop through the list of tuples\n",
    "        for tpl in list_of_tuples:\n",
    "            # 1. Access the first element of the tuple (the wrapper)\n",
    "            embedding_vector = tpl               \n",
    "            # 3. Add the 512-element vector to our matrix\n",
    "            embedding_matrix.append(embedding_vector)\n",
    "\n",
    "print(f\"\\nTotal embeddings to add to FAISS index: {len(embedding_matrix)}\")\n",
    "print(f\"\\nLength of each embedding vector: {len(embedding_matrix[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a0f1015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 0.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 1.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 2.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 3.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 4.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 5.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 6.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 7.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 8.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 9.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 10.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 11.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 12.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 13.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 14.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 15.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 16.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 17.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 18.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 19.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 20.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 21.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 22.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 23.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 24.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 25.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 26.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 27.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 28.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 29.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 30.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 31.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 32.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 33.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 34.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 35.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 36.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 37.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 38.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 39.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 40.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 41.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 42.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 43.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 44.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 45.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 46.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 47.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 48.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 49.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 50.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 51.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 52.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 53.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 54.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 55.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 56.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 57.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 58.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 59.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 60.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 61.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 62.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 63.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 64.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 65.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 66.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 67.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 68.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 69.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 70.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 71.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 72.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 73.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 74.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 75.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 76.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 77.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 78.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 79.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 80.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 81.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 82.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 83.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 84.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 85.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 86.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 87.\n",
      "Detected 88 new embeddings to add to RDA file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_4212\\2495409364.py:381: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.embeddings_df = pd.concat([self.embeddings_df, new_df], ignore_index=True).astype(self.column_dtypes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating RDA file at 'data_sets\\faiss\\embeddings.rda'...\n",
      "--- dtypes of the final DataFrame before writing ---\n",
      "float32\n",
      "-0.12608287\n",
      "RDA file successfully updated.\n",
      "\n",
      "--- Saving all data ---\n",
      "Ensured directory exists: data_sets\\faiss\n",
      "FAISS index with 88 vectors saved to data_sets\\faiss\\faiss.index.\n",
      "88 hashes saved to data_sets\\faiss\\embedding_fingerprints.json.\n",
      "Metadata for 1 tracks saved to data_sets\\faiss\\faiss_metadata.json.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "manager.add_embeddings(track_ids, embedding_matrix) \n",
    "manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1d9f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Result:\n",
      "Best match found is track_id: 'TRAAAGR128F425B14B'\n",
      "L2 Distance: 98.55679321289062\n"
     ]
    }
   ],
   "source": [
    "# Example query (with made-up embedding data)\n",
    "query_vec = [0.1] * manager.dimension\n",
    "\n",
    "#     # Perform the searc\n",
    "\n",
    "found_track_id, distance = manager.search_best_match(query_vec)\n",
    "#\n",
    "if found_track_id:\n",
    "    print(f\"\\nSearch Result:\")\n",
    "    print(f\"Best match found is track_id: '{found_track_id}'\")\n",
    "    print(f\"L2 Distance: {distance}\")\n",
    "else:\n",
    "    print(\"\\nNo match found in the index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03607e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
