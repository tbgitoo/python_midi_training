{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca81c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and loading the trained model\n",
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 4, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [2048, 2048, 2048]\n",
      "\n",
      "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  mu = tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  sigma = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "INFO:tensorflow:Restoring parameters from models/download.magenta.tensorflow.org/models/music_vae/checkpoints/mel_2bar_big.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Importing libraries and loading the trained model')\n",
    "import magenta.music as mm\n",
    "import note_seq\n",
    "from note_seq import sequences_lib\n",
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "from magenta.models.music_vae.trained_model import NoExtractedExamplesError\n",
    "from magenta.models.music_vae.trained_model import MultipleExtractedExamplesError\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss  # You'll need to install this: pip install faiss-cpu\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "# for setting up setup_faiss_db.py\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "\n",
    "BASE_DIR=\"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/mel_2bar_big.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc13c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration and function definitions  --\n",
    "DB_PATH = os.path.join('data_sets', 'midi_embeddings.db')\n",
    "FAISS_INDEX_PATH = os.path.join('data_sets', 'midi_embeddings.index')\n",
    "MELODY_DIR = os.path.join('data_sets', 'lmd_melodies') # Directory of extracted melodies\n",
    "EMBEDDING_DIM = 512 # The dimension of your MusicVAE embeddings\n",
    "\n",
    "def filter_pitch_range(ns, min_pitch=36, max_pitch=84):\n",
    "    \"\"\"Removes notes outside the specified MIDI pitch range.\"\"\"\n",
    "    valid_notes = [n for n in ns.notes if min_pitch <= n.pitch <= max_pitch]   \n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(valid_notes)   \n",
    "    return ns\n",
    "\n",
    "# --- Helper Functions (from previous iterations, still useful for cleaning) ---\n",
    "def make_monophonic(ns,steps_per_quarter=4):\n",
    "    \"\"\"Reduces a NoteSequence to be monophonic by picking the highest note at each step.\"\"\"\n",
    "    if not ns.notes:\n",
    "        return ns\n",
    "    quantized_ns = sequences_lib.quantize_note_sequence(ns, steps_per_quarter)    \n",
    "    notes_by_step = {}\n",
    "    for note in quantized_ns.notes:\n",
    "        # Use quantized_start_step for already quantized sequences\n",
    "        if note.quantized_start_step not in notes_by_step:\n",
    "            notes_by_step[note.quantized_start_step] = []\n",
    "        notes_by_step[note.quantized_start_step].append(note)\n",
    "    monophonic_notes = []\n",
    "    for step in sorted(notes_by_step.keys()):\n",
    "        notes_at_step = notes_by_step[step]\n",
    "        # If multiple notes at a step, pick the highest pitch\n",
    "        highest_note = max(notes_at_step, key=lambda n: n.pitch)\n",
    "        monophonic_notes.append(highest_note)\n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(monophonic_notes)\n",
    "    return ns\n",
    "\n",
    "def snap_chunk_notes_to_grid(unquantized_chunk, steps_per_quarter):\n",
    "    \"\"\"\n",
    "    Creates a new, unquantized NoteSequence with notes snapped to a grid.\n",
    "    This is the key function. It takes a time-based chunk, finds the ideal\n",
    "    quantized steps for its notes, and then creates a *new* unquantized\n",
    "    sequence where the note start/end times correspond perfectly to those steps.\n",
    "    Args:\n",
    "      unquantized_chunk: The unquantized NoteSequence chunk.\n",
    "      steps_per_quarter: The quantization resolution.\n",
    "    Returns:\n",
    "      A new, unquantized NoteSequence with grid-aligned note timings.\n",
    "    \"\"\"\n",
    "    # 1. Quantize the chunk to determine the ideal grid steps for each note.\n",
    "    try:\n",
    "        quantized_temp_chunk = note_seq.quantize_note_sequence(\n",
    "            unquantized_chunk, steps_per_quarter)\n",
    "    except note_seq.BadTimeSignatureError:\n",
    "        return None # Cannot process this chunk\n",
    "    qpm = unquantized_chunk.tempos[0].qpm if unquantized_chunk.tempos else 120.0\n",
    "    seconds_per_quarter = 60.0 / qpm\n",
    "    # 2. Create a new, empty, unquantized sequence to be the output.\n",
    "    grid_aligned_ns = music_pb2.NoteSequence()\n",
    "    grid_aligned_ns.tempos.add().qpm = qpm\n",
    "    grid_aligned_ns.ticks_per_quarter = unquantized_chunk.ticks_per_quarter\n",
    "    # 3. For each note in the quantized version, create a new note in our\n",
    "    #    output sequence with timings calculated from the quantized steps.\n",
    "    for q_note in quantized_temp_chunk.notes:\n",
    "        new_note = grid_aligned_ns.notes.add()\n",
    "        new_note.pitch = q_note.pitch\n",
    "        new_note.velocity = q_note.velocity\n",
    "        new_note.instrument = q_note.instrument\n",
    "        new_note.program = q_note.program\n",
    "        # Convert quantized steps back into precise seconds\n",
    "        start_quarters = q_note.quantized_start_step / steps_per_quarter\n",
    "        end_quarters = q_note.quantized_end_step / steps_per_quarter\n",
    "        new_note.start_time = start_quarters * seconds_per_quarter\n",
    "        new_note.end_time = end_quarters * seconds_per_quarter\n",
    "    # Set the total time of the new sequence.\n",
    "    total_quarters = quantized_temp_chunk.total_quantized_steps / steps_per_quarter\n",
    "    grid_aligned_ns.total_time = total_quarters * seconds_per_quarter\n",
    "    return grid_aligned_ns\n",
    "\n",
    "def set_program_for_all_notes(note_sequence, program_number=0):\n",
    "    \"\"\"\n",
    "    Resets the instrument program for every note in a NoteSequence.\n",
    "    Args:\n",
    "      note_sequence: The note_seq.NoteSequence object to modify.\n",
    "      program_number: The integer program number to set for all notes.\n",
    "                      Defaults to 0 (Acoustic Grand Piano).\n",
    "    Returns:\n",
    "      The modified NoteSequence.\n",
    "    \"\"\"\n",
    "    for note in note_sequence.notes:\n",
    "        note.program = program_number\n",
    "    return note_sequence\n",
    "\n",
    "def estimate_tempo_from_notes(\n",
    "    note_sequence: music_pb2.NoteSequence,\n",
    "    min_bpm: float = 60.0,\n",
    "    max_bpm: float = 240.0,\n",
    "    prior_bpm: float = 120.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimates the tempo of an unquantized NoteSequence by analyzing note onsets.\n",
    "\n",
    "    Args:\n",
    "        note_sequence: An unquantized NoteSequence object.\n",
    "        min_bpm: The minimum plausible tempo to consider.\n",
    "        max_bpm: The maximum plausible tempo to consider.\n",
    "        prior_bpm: The tempo to prefer (e.g., 120 BPM). The algorithm will favor\n",
    "                   candidates closer to this value.\n",
    "\n",
    "    Returns:\n",
    "        The estimated tempo in beats per minute (BPM). Returns prior_bpm if\n",
    "        not enough notes are present to make a guess.\n",
    "    \"\"\"\n",
    "    # 1. Extract unique, sorted note onset times\n",
    "    onsets = sorted(list(set(note.start_time for note in note_sequence.notes)))\n",
    "\n",
    "    if len(onsets) < 3:  # Need a reasonable number of notes for a good guess\n",
    "        print(\"Warning: Too few notes to reliably estimate tempo. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 2. Calculate Inter-Onset Intervals (IOIs)\n",
    "    iois = np.diff(onsets)\n",
    "    if len(iois) == 0:\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 3. Build a histogram of IOIs to find the most common intervals\n",
    "    # We use a small bin size to capture fine timing details\n",
    "    hist, bin_edges = np.histogram(iois, bins=np.arange(0, 5, 0.01), density=False)\n",
    "    \n",
    "    # Find peaks in the histogram. These are our primary rhythmic intervals.\n",
    "    # A simple way is to get the top N bins.\n",
    "    peak_indices = np.argsort(hist)[-10:] # Get indices of 10 strongest peaks\n",
    "    \n",
    "    tempo_candidates = defaultdict(float)\n",
    "\n",
    "    # 4. Generate and score tempo candidates from histogram peaks\n",
    "    for i in peak_indices:\n",
    "        if hist[i] < 2: # Ignore insignificant peaks\n",
    "            continue\n",
    "            \n",
    "        # The time (in seconds) corresponding to this peak\n",
    "        interval = bin_edges[i]\n",
    "        \n",
    "        # This interval could be a quarter note, eighth note, etc.\n",
    "        # Generate hypotheses based on this interval.\n",
    "        for multiple in [0.25, 0.33, 0.5, 1, 2, 3, 4]:\n",
    "            potential_beat_duration = interval * multiple\n",
    "            if potential_beat_duration == 0:\n",
    "                continue\n",
    "            \n",
    "            tempo = 60.0 / potential_beat_duration\n",
    "            \n",
    "            if min_bpm <= tempo <= max_bpm:\n",
    "                # 5. Score the candidate\n",
    "                # Score part 1: Rhythmic Strength (how strong was the peak?)\n",
    "                strength_score = hist[i]\n",
    "                \n",
    "                # Score part 2: Proximity to prior_bpm (Gaussian score)\n",
    "                # This gives a high score if tempo is near prior_bpm\n",
    "                proximity_score = np.exp(-0.5 * ((tempo - prior_bpm) / 20.0)**2)\n",
    "                \n",
    "                # Combine scores and add to any existing score for this tempo\n",
    "                combined_score = strength_score * proximity_score\n",
    "                tempo_candidates[tempo] += combined_score\n",
    "\n",
    "    if not tempo_candidates:\n",
    "        print(\"Warning: Could not find any valid tempo candidates. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 6. Return the tempo with the highest score\n",
    "    best_tempo = max(tempo_candidates, key=tempo_candidates.get)\n",
    "    return best_tempo\n",
    "\n",
    "def get_embeddings_for_song(track_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Finds all MIDI files for a given track_id, generates embeddings for each,\n",
    "    and returns them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        track_id (str): The ID of the track, e.g., \"TRAAAGR128F425B14B\".\n",
    "        root_path (str): The root path of the repository.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are MIDI filenames and values are lists of\n",
    "        numpy array embeddings generated from that MIDI file.\n",
    "        Returns an empty dictionary if the folder is not found or contains no MIDI files.\n",
    "    \"\"\"\n",
    "    # 1. Construct the folder path from the track_id\n",
    "    # e.g., 'data_sets/lmd_melodies/A/A/A/TRAAAGR128F425B14B'\n",
    "    if len(track_id) < 5:\n",
    "        print(f\"Error: track_id '{track_id}' is too short to build a path.\")\n",
    "        return {}\n",
    "        \n",
    "    song_folder_path = os.path.join(\n",
    "        'data_sets',\n",
    "        'lmd_melodies',\n",
    "        track_id[2],\n",
    "        track_id[3],\n",
    "        track_id[4],\n",
    "        track_id\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(song_folder_path):\n",
    "        print(f\"Warning: Directory not found at {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Find all MIDI files in the directory\n",
    "    midi_filepaths = glob.glob(os.path.join(song_folder_path, '*.mid'))\n",
    "    midi_filepaths.extend(glob.glob(os.path.join(song_folder_path, '*.midi')))\n",
    "\n",
    "    if not midi_filepaths:\n",
    "        print(f\"Warning: No MIDI files found in {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Process each MIDI file to generate embeddings\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    \n",
    "\n",
    "    for midi_path in midi_filepaths:\n",
    "        filename = os.path.basename(midi_path)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and quantize the MIDI file\n",
    "            midi_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "            print(f\"Processing {filename}...\")\n",
    "            cleaned_quantized_list = []\n",
    "            qpm = estimate_tempo_from_notes(midi_ns)\n",
    "            seconds_per_quarter = 60.0 / qpm\n",
    "            steps_per_quarter=mel_2bar_config.data_converter._steps_per_quarter \n",
    "            seconds_per_step = seconds_per_quarter / steps_per_quarter\n",
    "            num_steps_per_chunk = mel_2bar_config.hparams.max_seq_len\n",
    "            hop_size_in_seconds = num_steps_per_chunk * seconds_per_step # 32 / 4 = 8.0 seconds\n",
    "            cleaned_ms = make_monophonic(midi_ns)\n",
    "            \n",
    "            cleaned_ms = snap_chunk_notes_to_grid(cleaned_ms, steps_per_quarter)\n",
    "            cleaned_ms = set_program_for_all_notes(cleaned_ms, program_number=0)\n",
    "            \n",
    "            \n",
    "\n",
    "            if cleaned_ms.notes:\n",
    "                slices = sequences_lib.split_note_sequence(\n",
    "                    note_sequence=cleaned_ms,\n",
    "                    hop_size_seconds=hop_size_in_seconds  \n",
    "                )\n",
    "                for chunk in slices:\n",
    "                    cleaned_quantized_list.append(chunk)\n",
    "            embeddings=[]\n",
    "            \n",
    "            for chunk in cleaned_quantized_list:\n",
    "                try:\n",
    "                    embedding = mel_2bar.encode([chunk])\n",
    "                    embeddings.append(embedding)\n",
    "                except NoExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, insufficient note data\")\n",
    "                except MultipleExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, multiple examples extracted\")    \n",
    "                continue\n",
    "            if(len(embeddings)>0):\n",
    "                all_embeddings[filename] = embeddings\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Could not process file {filename}. Error: {e}\")\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "\n",
    "# --- NEW: Main Orchestrator Function ---\n",
    "def process_tracks_from_db(\n",
    "    start_line_to_process: int=0,\n",
    "    num_tracks_to_process: int = 1,\n",
    "    db_path: str='data_sets/track_metadata.db'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Reads track_ids from an SQLite database, generates embeddings for each,\n",
    "    and returns a nested dictionary of all embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_tracks_to_process (int, optional): The number of tracks to process.\n",
    "                                               If None, processes all tracks. Defaults to 1.\n",
    "        db_path (str): Path to the track_metadata.db SQLite file.\n",
    "       \n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are track_ids and values are the dictionaries\n",
    "        returned by get_embeddings_for_song.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database not found at {db_path}\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Connecting to database: {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 1. Fetch the track_ids from the database\n",
    "    query = \"SELECT track_id FROM songs\" # Assuming the table is named 'songs'\n",
    "    if num_tracks_to_process is not None and num_tracks_to_process > 0:\n",
    "        num_tracks_to_query=num_tracks_to_process\n",
    "        query += f\" LIMIT {start_line_to_process}, {num_tracks_to_query}\"\n",
    "    \n",
    "    print(\"Fetching track_ids...\")\n",
    "    cursor.execute(query)\n",
    "    # Fetch all rows and flatten the list of tuples [('id1',), ('id2',)] -> ['id1', 'id2']\n",
    "    track_ids = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    \n",
    "    if not track_ids:\n",
    "        print(\"No track_ids found in the database.\")\n",
    "        return {}\n",
    "\n",
    "    total_tracks = len(track_ids)\n",
    "    print(f\"Found {total_tracks} track_ids to process.\")\n",
    "\n",
    "    # 2. Iterate through track_ids and get embeddings for each\n",
    "    all_track_embeddings = {}\n",
    "    for i, track_id in enumerate(track_ids):\n",
    "        print(f\"\\n--- Processing track {i + 1}/{total_tracks}: {track_id} ---\")\n",
    "        \n",
    "        song_embeddings = get_embeddings_for_song(track_id)\n",
    "        \n",
    "        if song_embeddings:\n",
    "            all_track_embeddings[track_id] = song_embeddings\n",
    "            print(f\"-> Success: Found and processed {len(song_embeddings)} MIDI file(s) for this track.\")\n",
    "            \n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    return all_track_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b44388cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAISS database setup complete. Ready to add embeddings.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DB_FOLDER = os.path.join(\"data_sets\",\"faiss\")\n",
    "INDEX_FILE = os.path.join(DB_FOLDER,\"my_faiss_index.index\")\n",
    "METADATA_FILE =os.path.join(DB_FOLDER,\"faiss_metadata.json\")\n",
    "FINGERPRINT_FILE = os.path.join(DB_FOLDER, \"embedding_fingerprints.json\")\n",
    "\n",
    "\n",
    "# -------------------\n",
    "\n",
    "def setup_faiss_database(index_file, metadata_file,EMBEDDING_DIM=512):\n",
    "    \"\"\"\n",
    "    Initializes an empty FAISS index and an empty ID mapping,\n",
    "    saving them to disk.\n",
    "    \"\"\"\n",
    "    # Create a flat L2 index (Euclidean distance)\n",
    "    # IndexFlatL2 is a simple, brute-force index. For very large datasets,\n",
    "    # you might consider more advanced indices like IndexIVFFlat.\n",
    "    base_index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "    \n",
    "    # Wrap it with IndexIDMap to allow custom (integer) IDs\n",
    "    index_with_ids = faiss.IndexIDMap(base_index)\n",
    "    \n",
    "    # Save the empty index\n",
    "    faiss.write_index(index_with_ids, index_file)\n",
    "    print(f\"Empty FAISS index (dimension {EMBEDDING_DIM}) created and saved to: {index_file}\")\n",
    "    \n",
    "    # Initialize an empty ID mapping and save it\n",
    "    # This map will store string_id -> integer_id\n",
    "    string_to_int_map = {}\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(string_to_int_map, f)\n",
    "    print(f\"Empty ID metadata file created and saved to: {metadata_file}\")\n",
    "\n",
    "\n",
    "    # --- Create Empty Fingerprint Set ---\n",
    "    # We save it as an empty list in JSON\n",
    "    with open(FINGERPRINT_FILE, 'w') as f:\n",
    "        json.dump([], f)\n",
    "    print(f\"Empty fingerprint file saved to: {FINGERPRINT_FILE}\")\n",
    "\n",
    "\n",
    "\n",
    "def add_embeddings_with_deduplication(new_embedding_matrix, new_track_ids):\n",
    "    \"\"\"\n",
    "    Loads an existing FAISS index and adds new embeddings, skipping any\n",
    "    (track_id, embedding_vector) pairs that already exist.\n",
    "    \"\"\"\n",
    "    if not all(os.path.exists(f) for f in [INDEX_FILE, METADATA_FILE, FINGERPRINT_FILE]):\n",
    "        print(\"Error: Database files not found. Please run setup_faiss_db.py first.\")\n",
    "        return\n",
    "\n",
    "    # 1. LOAD EXISTING DATA\n",
    "    index = faiss.read_index(INDEX_FILE)\n",
    "    with open(METADATA_FILE, 'r') as f:\n",
    "        string_to_int_map = json.load(f)\n",
    "    with open(FINGERPRINT_FILE, 'r') as f:\n",
    "        # Load fingerprints and convert to a set of tuples for fast lookups\n",
    "        # A numpy array is not hashable, so we convert it to a tuple\n",
    "        fingerprint_list = json.load(f)\n",
    "        fingerprint_set = set(tuple(fp) for fp in fingerprint_list)\n",
    "\n",
    "    print(f\"Loaded index with {index.ntotal} embeddings and {len(fingerprint_set)} fingerprints.\")\n",
    "\n",
    "    # 2. FILTER OUT DUPLICATES\n",
    "    truly_new_embeddings = []\n",
    "    truly_new_track_ids = []\n",
    "    \n",
    "    for i, embedding in enumerate(new_embedding_matrix):\n",
    "        track_id = new_track_ids[i]\n",
    "        # Create a hashable fingerprint: (track_id, tuple(embedding_values))\n",
    "        # We convert the embedding to a list then a tuple to make it hashable\n",
    "        fingerprint = (track_id, tuple(embedding.tolist()))\n",
    "        \n",
    "        if fingerprint not in fingerprint_set:\n",
    "            truly_new_embeddings.append(embedding)\n",
    "            truly_new_track_ids.append(track_id)\n",
    "            # Add the new fingerprint to our set to handle duplicates within the new batch itself\n",
    "            fingerprint_set.add(fingerprint)\n",
    "\n",
    "    if not truly_new_embeddings:\n",
    "        print(\"No new unique embeddings to add. Database is already up-to-date.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(truly_new_embeddings)} new unique embeddings to add.\")\n",
    "\n",
    "    # 3. PREPARE AND ADD THE NEW DATA\n",
    "    new_embedding_array = np.array(truly_new_embeddings, dtype='float32')\n",
    "    \n",
    "    # Assign integer IDs, creating new ones if necessary\n",
    "    next_int_id = max(string_to_int_map.values()) + 1 if string_to_int_map else 0\n",
    "    integer_ids_for_faiss = []\n",
    "    for tid in truly_new_track_ids:\n",
    "        if tid not in string_to_int_map:\n",
    "            string_to_int_map[tid] = next_int_id\n",
    "            next_int_id += 1\n",
    "        integer_ids_for_faiss.append(string_to_int_map[tid])\n",
    "    \n",
    "    integer_ids_for_faiss = np.array(integer_ids_for_faiss, dtype='int64')\n",
    "\n",
    "    # Add to the FAISS index\n",
    "    index.add_with_ids(new_embedding_array, integer_ids_for_faiss)\n",
    "\n",
    "    # 4. SAVE EVERYTHING BACK TO DISK\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "    with open(METADATA_FILE, 'w') as f:\n",
    "        json.dump(string_to_int_map, f)\n",
    "    \n",
    "    # Convert the set of fingerprints back to a list of lists for JSON serialization\n",
    "    updated_fingerprint_list = [list(fp) for fp in fingerprint_set]\n",
    "    with open(FINGERPRINT_FILE, 'w') as f:\n",
    "        json.dump(updated_fingerprint_list, f)\n",
    "        \n",
    "    print(f\"Successfully added {len(truly_new_embeddings)} embeddings. Total in index: {index.ntotal}.\")\n",
    "    print(\"Database files have been updated.\")\n",
    "\n",
    "\n",
    "def search_nearest_track(query_embedding, k=1):\n",
    "    \"\"\"\n",
    "    Searches the FAISS database for the nearest neighbor(s) to a given embedding.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray or list): A single embedding vector of shape (512,) or (1, 512).\n",
    "        k (int): The number of nearest neighbors to return. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains (track_id, distance).\n",
    "              Returns an empty list if the database is not found or is empty.\n",
    "    \"\"\"\n",
    "    # 1. VALIDATE DATABASE AND INPUTS\n",
    "    if not os.path.exists(INDEX_FILE) or not os.path.exists(METADATA_FILE):\n",
    "        print(\"Error: Database files not found. Please run setup_faiss_db.py first.\")\n",
    "        return []\n",
    "\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(INDEX_FILE)\n",
    "    \n",
    "    if index.ntotal == 0:\n",
    "        print(\"Database is empty. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    # Validate the query embedding\n",
    "    query_vector = np.array(query_embedding, dtype='float32')\n",
    "    if query_vector.ndim == 1:\n",
    "        # If the input is a 1D array, reshape it to 2D (1, 512) for FAISS\n",
    "        query_vector = query_vector.reshape(1, -1)\n",
    "    \n",
    "    if query_vector.shape[1] != index.d:\n",
    "        print(f\"Error: Query embedding dimension ({query_vector.shape[1]}) does not match index dimension ({index.d}).\")\n",
    "        return []\n",
    "\n",
    "    # 2. LOAD THE ID MAPPING\n",
    "    # We need to create the reverse mapping from integer_id -> string_id\n",
    "    with open(METADATA_FILE, 'r') as f:\n",
    "        string_to_int_map = json.load(f)\n",
    "        # JSON keys are strings, so we must cast the integer ID back to int\n",
    "        int_to_string_map = {int(v): k for k, v in string_to_int_map.items()}\n",
    "\n",
    "    # 3. PERFORM THE SEARCH\n",
    "    # The search function returns distances and indices (the integer IDs)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    # 4. PROCESS AND RETURN THE RESULTS\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        # Get the integer ID from the search result\n",
    "        integer_id = indices[0][i]\n",
    "        \n",
    "        # Check if the ID is valid (FAISS can return -1 for invalid results)\n",
    "        if integer_id == -1:\n",
    "            continue\n",
    "            \n",
    "        # Look up the string track_id from our map\n",
    "        string_id = int_to_string_map.get(integer_id, \"ID_Not_Found\")\n",
    "        \n",
    "        # Get the corresponding distance\n",
    "        distance = distances[0][i]\n",
    "        \n",
    "        results.append((string_id, distance))\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "first_time=False\n",
    "\n",
    "if(first_time):\n",
    "    # Ensure the files don't exist if you want to start fresh\n",
    "    try:\n",
    "        os.makedirs('data_sets/faiss', exist_ok=True)\n",
    "        print(f\"Folder 'data_sets/faiss' created successfully or already exists.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating folder 'data_sets/faiss': {e}\")\n",
    "\n",
    "\n",
    "    \n",
    "    if os.path.exists(INDEX_FILE):\n",
    "        os.remove(INDEX_FILE)\n",
    "        print(f\"Removed existing {INDEX_FILE}\")\n",
    "    if os.path.exists(METADATA_FILE):\n",
    "        os.remove(METADATA_FILE)\n",
    "        print(f\"Removed existing {METADATA_FILE}\")\n",
    "        \n",
    "    setup_faiss_database(INDEX_FILE, METADATA_FILE,EMBEDDING_DIM)\n",
    "print(\"\\nFAISS database setup complete. Ready to add embeddings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293abeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embeddings for a set of tracks from the database\n",
    "all_embeddings = process_tracks_from_db(\n",
    "    start_line_to_process=0,\n",
    "    num_tracks_to_process=30)\n",
    "\n",
    "\n",
    "\n",
    "total_embeddings = sum(len(array) for track_dict in all_embeddings.values() for array in track_dict.values())\n",
    "\n",
    "track_ids = []\n",
    "\n",
    "# Iterate through each track ID and its corresponding inner dictionary\n",
    "for track_id, inner_dict in all_embeddings.items():\n",
    "    # Calculate the total number of embeddings for the current track_id\n",
    "    # This uses the sum() and generator expression method from before\n",
    "    count = sum(len(array) for array in inner_dict.values())  \n",
    "    # Extend the main list by adding the track_id 'count' times\n",
    "    # The expression [track_id] * count creates a new list like ['document_1', 'document_1', ...]\n",
    "    track_ids.extend([track_id] * count)\n",
    "\n",
    "# Flatten all numbers into a single list\n",
    "embedding_matrix = []\n",
    "\n",
    "# Loop through the top-level dictionary\n",
    "for inner_dict in all_embeddings.values():\n",
    "    # Loop through the second-level dictionary\n",
    "    for list_of_tuples in inner_dict.values():\n",
    "        # Loop through the list of tuples\n",
    "        for tpl in list_of_tuples:\n",
    "            # 1. Access the first element of the tuple (the wrapper)\n",
    "            wrapper = tpl[0]        \n",
    "            # 2. Access the first (and only) element of the wrapper\n",
    "            embedding_vector = wrapper[0]          \n",
    "            # 3. Add the 512-element vector to our matrix\n",
    "            embedding_matrix.append(embedding_vector)\n",
    "\n",
    "\n",
    "\n",
    "add_embeddings_with_deduplication(embedding_matrix, track_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1d9f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Searching for the single nearest track ---\n",
      "The nearest track is: 'TRAAAGR128F425B14B'\n",
      "Distance: 585.3926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Example Usage ---\n",
    "# This demonstrates how to call the function.\n",
    "    \n",
    "    # 1. Create a random \"observed\" embedding to simulate a query\n",
    "    # In a real application, this would come from your MIDI processing\n",
    "observed_embedding = np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "    # 2. Search for the single nearest track ID (k=1)\n",
    "print(\"--- Searching for the single nearest track ---\")\n",
    "nearest_results = search_nearest_track(observed_embedding, k=1)\n",
    "\n",
    "if nearest_results:\n",
    "    # The function returns a list, so we get the first element\n",
    "    track_id, distance = nearest_results[0]\n",
    "    print(f\"The nearest track is: '{track_id}'\")\n",
    "    print(f\"Distance: {distance:.4f}\")\n",
    "else:\n",
    "    print(\"Search returned no results.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
