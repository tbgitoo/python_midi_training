{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca81c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries and loading the trained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 4, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [2048, 2048, 2048]\n",
      "\n",
      "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:463: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  mu = tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  sigma = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "INFO:tensorflow:Restoring parameters from models/download.magenta.tensorflow.org/models/music_vae/checkpoints/mel_2bar_big.ckpt\n"
     ]
    }
   ],
   "source": [
    "print('Importing libraries and loading the trained model')\n",
    "import magenta.music as mm\n",
    "import note_seq\n",
    "from note_seq import sequences_lib\n",
    "from note_seq.protobuf import music_pb2\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "from magenta.models.music_vae.trained_model import NoExtractedExamplesError\n",
    "from magenta.models.music_vae.trained_model import MultipleExtractedExamplesError\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "import random\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss  # You'll need to install this: pip install faiss-cpu\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "# for setting up setup_faiss_db.py\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "\n",
    "BASE_DIR=\"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=4, checkpoint_dir_or_path=BASE_DIR + '/checkpoints/mel_2bar_big.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc13c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration and function definitions  --\n",
    "DB_PATH = os.path.join('data_sets', 'midi_embeddings.db')\n",
    "FAISS_INDEX_PATH = os.path.join('data_sets', 'midi_embeddings.index')\n",
    "MELODY_DIR = os.path.join('data_sets', 'lmd_melodies') # Directory of extracted melodies\n",
    "EMBEDDING_DIM = 512 # The dimension of your MusicVAE embeddings\n",
    "\n",
    "def filter_pitch_range(ns, min_pitch=36, max_pitch=84):\n",
    "    \"\"\"Removes notes outside the specified MIDI pitch range.\"\"\"\n",
    "    valid_notes = [n for n in ns.notes if min_pitch <= n.pitch <= max_pitch]   \n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(valid_notes)   \n",
    "    return ns\n",
    "\n",
    "# --- Helper Functions (from previous iterations, still useful for cleaning) ---\n",
    "def make_monophonic(ns,steps_per_quarter=4):\n",
    "    \"\"\"Reduces a NoteSequence to be monophonic by picking the highest note at each step.\"\"\"\n",
    "    if not ns.notes:\n",
    "        return ns\n",
    "    quantized_ns = sequences_lib.quantize_note_sequence(ns, steps_per_quarter)    \n",
    "    notes_by_step = {}\n",
    "    for note in quantized_ns.notes:\n",
    "        # Use quantized_start_step for already quantized sequences\n",
    "        if note.quantized_start_step not in notes_by_step:\n",
    "            notes_by_step[note.quantized_start_step] = []\n",
    "        notes_by_step[note.quantized_start_step].append(note)\n",
    "    monophonic_notes = []\n",
    "    for step in sorted(notes_by_step.keys()):\n",
    "        notes_at_step = notes_by_step[step]\n",
    "        # If multiple notes at a step, pick the highest pitch\n",
    "        highest_note = max(notes_at_step, key=lambda n: n.pitch)\n",
    "        monophonic_notes.append(highest_note)\n",
    "    del ns.notes[:]\n",
    "    ns.notes.extend(monophonic_notes)\n",
    "    return ns\n",
    "\n",
    "def snap_chunk_notes_to_grid(unquantized_chunk, steps_per_quarter):\n",
    "    \"\"\"\n",
    "    Creates a new, unquantized NoteSequence with notes snapped to a grid.\n",
    "    This is the key function. It takes a time-based chunk, finds the ideal\n",
    "    quantized steps for its notes, and then creates a *new* unquantized\n",
    "    sequence where the note start/end times correspond perfectly to those steps.\n",
    "    Args:\n",
    "      unquantized_chunk: The unquantized NoteSequence chunk.\n",
    "      steps_per_quarter: The quantization resolution.\n",
    "    Returns:\n",
    "      A new, unquantized NoteSequence with grid-aligned note timings.\n",
    "    \"\"\"\n",
    "    # 1. Quantize the chunk to determine the ideal grid steps for each note.\n",
    "    try:\n",
    "        quantized_temp_chunk = note_seq.quantize_note_sequence(\n",
    "            unquantized_chunk, steps_per_quarter)\n",
    "    except note_seq.BadTimeSignatureError:\n",
    "        return None # Cannot process this chunk\n",
    "    qpm = unquantized_chunk.tempos[0].qpm if unquantized_chunk.tempos else 120.0\n",
    "    seconds_per_quarter = 60.0 / qpm\n",
    "    # 2. Create a new, empty, unquantized sequence to be the output.\n",
    "    grid_aligned_ns = music_pb2.NoteSequence()\n",
    "    grid_aligned_ns.tempos.add().qpm = qpm\n",
    "    grid_aligned_ns.ticks_per_quarter = unquantized_chunk.ticks_per_quarter\n",
    "    # 3. For each note in the quantized version, create a new note in our\n",
    "    #    output sequence with timings calculated from the quantized steps.\n",
    "    for q_note in quantized_temp_chunk.notes:\n",
    "        new_note = grid_aligned_ns.notes.add()\n",
    "        new_note.pitch = q_note.pitch\n",
    "        new_note.velocity = q_note.velocity\n",
    "        new_note.instrument = q_note.instrument\n",
    "        new_note.program = q_note.program\n",
    "        # Convert quantized steps back into precise seconds\n",
    "        start_quarters = q_note.quantized_start_step / steps_per_quarter\n",
    "        end_quarters = q_note.quantized_end_step / steps_per_quarter\n",
    "        new_note.start_time = start_quarters * seconds_per_quarter\n",
    "        new_note.end_time = end_quarters * seconds_per_quarter\n",
    "    # Set the total time of the new sequence.\n",
    "    total_quarters = quantized_temp_chunk.total_quantized_steps / steps_per_quarter\n",
    "    grid_aligned_ns.total_time = total_quarters * seconds_per_quarter\n",
    "    return grid_aligned_ns\n",
    "\n",
    "def set_program_for_all_notes(note_sequence, program_number=0):\n",
    "    \"\"\"\n",
    "    Resets the instrument program for every note in a NoteSequence.\n",
    "    Args:\n",
    "      note_sequence: The note_seq.NoteSequence object to modify.\n",
    "      program_number: The integer program number to set for all notes.\n",
    "                      Defaults to 0 (Acoustic Grand Piano).\n",
    "    Returns:\n",
    "      The modified NoteSequence.\n",
    "    \"\"\"\n",
    "    for note in note_sequence.notes:\n",
    "        note.program = program_number\n",
    "    return note_sequence\n",
    "\n",
    "def estimate_tempo_from_notes(\n",
    "    note_sequence: music_pb2.NoteSequence,\n",
    "    min_bpm: float = 60.0,\n",
    "    max_bpm: float = 240.0,\n",
    "    prior_bpm: float = 120.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimates the tempo of an unquantized NoteSequence by analyzing note onsets.\n",
    "\n",
    "    Args:\n",
    "        note_sequence: An unquantized NoteSequence object.\n",
    "        min_bpm: The minimum plausible tempo to consider.\n",
    "        max_bpm: The maximum plausible tempo to consider.\n",
    "        prior_bpm: The tempo to prefer (e.g., 120 BPM). The algorithm will favor\n",
    "                   candidates closer to this value.\n",
    "\n",
    "    Returns:\n",
    "        The estimated tempo in beats per minute (BPM). Returns prior_bpm if\n",
    "        not enough notes are present to make a guess.\n",
    "    \"\"\"\n",
    "    # 1. Extract unique, sorted note onset times\n",
    "    onsets = sorted(list(set(note.start_time for note in note_sequence.notes)))\n",
    "\n",
    "    if len(onsets) < 3:  # Need a reasonable number of notes for a good guess\n",
    "        print(\"Warning: Too few notes to reliably estimate tempo. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 2. Calculate Inter-Onset Intervals (IOIs)\n",
    "    iois = np.diff(onsets)\n",
    "    if len(iois) == 0:\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 3. Build a histogram of IOIs to find the most common intervals\n",
    "    # We use a small bin size to capture fine timing details\n",
    "    hist, bin_edges = np.histogram(iois, bins=np.arange(0, 5, 0.01), density=False)\n",
    "    \n",
    "    # Find peaks in the histogram. These are our primary rhythmic intervals.\n",
    "    # A simple way is to get the top N bins.\n",
    "    peak_indices = np.argsort(hist)[-10:] # Get indices of 10 strongest peaks\n",
    "    \n",
    "    tempo_candidates = defaultdict(float)\n",
    "\n",
    "    # 4. Generate and score tempo candidates from histogram peaks\n",
    "    for i in peak_indices:\n",
    "        if hist[i] < 2: # Ignore insignificant peaks\n",
    "            continue\n",
    "            \n",
    "        # The time (in seconds) corresponding to this peak\n",
    "        interval = bin_edges[i]\n",
    "        \n",
    "        # This interval could be a quarter note, eighth note, etc.\n",
    "        # Generate hypotheses based on this interval.\n",
    "        for multiple in [0.25, 0.33, 0.5, 1, 2, 3, 4]:\n",
    "            potential_beat_duration = interval * multiple\n",
    "            if potential_beat_duration == 0:\n",
    "                continue\n",
    "            \n",
    "            tempo = 60.0 / potential_beat_duration\n",
    "            \n",
    "            if min_bpm <= tempo <= max_bpm:\n",
    "                # 5. Score the candidate\n",
    "                # Score part 1: Rhythmic Strength (how strong was the peak?)\n",
    "                strength_score = hist[i]\n",
    "                \n",
    "                # Score part 2: Proximity to prior_bpm (Gaussian score)\n",
    "                # This gives a high score if tempo is near prior_bpm\n",
    "                proximity_score = np.exp(-0.5 * ((tempo - prior_bpm) / 20.0)**2)\n",
    "                \n",
    "                # Combine scores and add to any existing score for this tempo\n",
    "                combined_score = strength_score * proximity_score\n",
    "                tempo_candidates[tempo] += combined_score\n",
    "\n",
    "    if not tempo_candidates:\n",
    "        print(\"Warning: Could not find any valid tempo candidates. Returning prior.\")\n",
    "        bpm=prior_bpm\n",
    "        if note_sequence.tempos:\n",
    "            bpm = note_sequence.tempos[0].qpm\n",
    "        return bpm\n",
    "\n",
    "    # 6. Return the tempo with the highest score\n",
    "    best_tempo = max(tempo_candidates, key=tempo_candidates.get)\n",
    "    return best_tempo\n",
    "\n",
    "def get_embeddings_for_song(track_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Finds all MIDI files for a given track_id, generates embeddings for each,\n",
    "    and returns them in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        track_id (str): The ID of the track, e.g., \"TRAAAGR128F425B14B\".\n",
    "        root_path (str): The root path of the repository.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are MIDI filenames and values are lists of\n",
    "        numpy array embeddings generated from that MIDI file.\n",
    "        Returns an empty dictionary if the folder is not found or contains no MIDI files.\n",
    "    \"\"\"\n",
    "    # 1. Construct the folder path from the track_id\n",
    "    # e.g., 'data_sets/lmd_melodies/A/A/A/TRAAAGR128F425B14B'\n",
    "    if len(track_id) < 5:\n",
    "        print(f\"Error: track_id '{track_id}' is too short to build a path.\")\n",
    "        return {}\n",
    "        \n",
    "    song_folder_path = os.path.join(\n",
    "        'data_sets',\n",
    "        'lmd_melodies',\n",
    "        track_id[2],\n",
    "        track_id[3],\n",
    "        track_id[4],\n",
    "        track_id\n",
    "    )\n",
    "\n",
    "    if not os.path.isdir(song_folder_path):\n",
    "        print(f\"Warning: Directory not found at {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 2. Find all MIDI files in the directory\n",
    "    midi_filepaths = glob.glob(os.path.join(song_folder_path, '*.mid'))\n",
    "    midi_filepaths.extend(glob.glob(os.path.join(song_folder_path, '*.midi')))\n",
    "\n",
    "    if not midi_filepaths:\n",
    "        print(f\"Warning: No MIDI files found in {song_folder_path}\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Process each MIDI file to generate embeddings\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    \n",
    "\n",
    "    for midi_path in midi_filepaths:\n",
    "        filename = os.path.basename(midi_path)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load and quantize the MIDI file\n",
    "            midi_ns = note_seq.midi_file_to_note_sequence(midi_path)\n",
    "            print(f\"Processing {filename}...\")\n",
    "            cleaned_quantized_list = []\n",
    "            qpm = estimate_tempo_from_notes(midi_ns)\n",
    "            seconds_per_quarter = 60.0 / qpm\n",
    "            steps_per_quarter=mel_2bar_config.data_converter._steps_per_quarter \n",
    "            seconds_per_step = seconds_per_quarter / steps_per_quarter\n",
    "            num_steps_per_chunk = mel_2bar_config.hparams.max_seq_len\n",
    "            hop_size_in_seconds = num_steps_per_chunk * seconds_per_step # 32 / 4 = 8.0 seconds\n",
    "            cleaned_ms = make_monophonic(midi_ns)\n",
    "            \n",
    "            cleaned_ms = snap_chunk_notes_to_grid(cleaned_ms, steps_per_quarter)\n",
    "            cleaned_ms = set_program_for_all_notes(cleaned_ms, program_number=0)\n",
    "            \n",
    "            \n",
    "\n",
    "            if cleaned_ms.notes:\n",
    "                slices = sequences_lib.split_note_sequence(\n",
    "                    note_sequence=cleaned_ms,\n",
    "                    hop_size_seconds=hop_size_in_seconds  \n",
    "                )\n",
    "                for chunk in slices:\n",
    "                    cleaned_quantized_list.append(chunk)\n",
    "            embeddings=[]\n",
    "            \n",
    "            for chunk in cleaned_quantized_list:\n",
    "                try:\n",
    "                    embedding = mel_2bar.encode([chunk])\n",
    "                    embeddings.append(embedding)\n",
    "                except NoExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, insufficient note data\")\n",
    "                except MultipleExtractedExamplesError as e:\n",
    "                    print(f\"Skipping chunk, multiple examples extracted\")    \n",
    "                continue\n",
    "            if(len(embeddings)>0):\n",
    "                all_embeddings[filename] = embeddings\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Could not process file {filename}. Error: {e}\")\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "\n",
    "# --- NEW: Main Orchestrator Function ---\n",
    "def process_tracks_from_db(\n",
    "    start_line_to_process: int=0,\n",
    "    num_tracks_to_process: int = 1,\n",
    "    db_path: str='data_sets/track_metadata.db'\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Reads track_ids from an SQLite database, generates embeddings for each,\n",
    "    and returns a nested dictionary of all embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_tracks_to_process (int, optional): The number of tracks to process.\n",
    "                                               If None, processes all tracks. Defaults to 1.\n",
    "        db_path (str): Path to the track_metadata.db SQLite file.\n",
    "       \n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are track_ids and values are the dictionaries\n",
    "        returned by get_embeddings_for_song.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"Error: Database not found at {db_path}\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Connecting to database: {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # 1. Fetch the track_ids from the database\n",
    "    query = \"SELECT track_id FROM songs\" # Assuming the table is named 'songs'\n",
    "    if num_tracks_to_process is not None and num_tracks_to_process > 0:\n",
    "        num_tracks_to_query=num_tracks_to_process\n",
    "        query += f\" LIMIT {start_line_to_process}, {num_tracks_to_query}\"\n",
    "    \n",
    "    print(\"Fetching track_ids...\")\n",
    "    cursor.execute(query)\n",
    "    # Fetch all rows and flatten the list of tuples [('id1',), ('id2',)] -> ['id1', 'id2']\n",
    "    track_ids = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    \n",
    "    if not track_ids:\n",
    "        print(\"No track_ids found in the database.\")\n",
    "        return {}\n",
    "\n",
    "    total_tracks = len(track_ids)\n",
    "    print(f\"Found {total_tracks} track_ids to process.\")\n",
    "\n",
    "    # 2. Iterate through track_ids and get embeddings for each\n",
    "    all_track_embeddings = {}\n",
    "    for i, track_id in enumerate(track_ids):\n",
    "        print(f\"\\n--- Processing track {i + 1}/{total_tracks}: {track_id} ---\")\n",
    "        \n",
    "        song_embeddings = get_embeddings_for_song(track_id)\n",
    "        \n",
    "        if song_embeddings:\n",
    "            all_track_embeddings[track_id] = song_embeddings\n",
    "            print(f\"-> Success: Found and processed {len(song_embeddings)} MIDI file(s) for this track.\")\n",
    "            \n",
    "    print(\"\\n--- Processing Complete ---\")\n",
    "    return all_track_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44388cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index with dimension 512.\n",
      "Manager initialized with 0 vectors, 0 unique hashes, and 0 tracks in metadata.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DB_FOLDER = os.path.join(\"data_sets\",\"faiss\")\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "METADATA_FILE =\"faiss_metadata.json\"\n",
    "FINGERPRINT_FILE =  \"embedding_fingerprints.json\"\n",
    "\n",
    "\n",
    "# -------------------\n",
    "\n",
    "class FaissManager:\n",
    "    \"\"\"\n",
    "    Manages a FAISS index with a deduplication layer, designed to accept\n",
    "    simple 1D lists of numbers as embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 dimension: int = 512, \n",
    "                 index_file: str = 'faiss.index', \n",
    "                 hashes_file: str = 'embeddings_hashes.json', \n",
    "                 metadata_file: str = 'metadata.json',\n",
    "                 data_root: str = 'data_sets/faiss'):\n",
    "        \"\"\"\n",
    "        Initializes the manager.\n",
    "\n",
    "        Args:\n",
    "            dimension: The dimension of the embedding vectors (e.g., 512).\n",
    "            index_file: Path to save/load the FAISS index.\n",
    "            hashes_file: Path to the JSON file storing unique hashes.\n",
    "            data_root: Root directory for storing FAISS data files. Defaults to 'data_sets/faiss'.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.data_root = data_root\n",
    "        self.index_file = os.path.join(data_root,index_file)\n",
    "        self.hashes_file = os.path.join(data_root,hashes_file)\n",
    "        self.metadata_file = os.path.join(data_root,metadata_file)\n",
    "        \n",
    "        # Load or initialize the FAISS index\n",
    "\n",
    "        # Load or initialize the FAISS index\n",
    "        self.index = self._load_faiss_index()\n",
    "            \n",
    "        # Load existing hashes for deduplication\n",
    "        self.existing_hashes = self._load_json_to_set(self.hashes_file)\n",
    "        \n",
    "        # Load the metadata mapping track_ids to FAISS indices\n",
    "        # Use defaultdict to simplify adding new track_ids\n",
    "        self.metadata = defaultdict(list, self._load_json_to_dict(self.metadata_file))\n",
    "        \n",
    "        print(\n",
    "            f\"Manager initialized with {self.index.ntotal} vectors, \"\n",
    "            f\"{len(self.existing_hashes)} unique hashes, and \"\n",
    "            f\"{len(self.metadata)} tracks in metadata.\"\n",
    "        )\n",
    "\n",
    "    def _load_faiss_index(self) -> faiss.Index:\n",
    "        \"\"\"Loads or creates a FAISS index.\"\"\"\n",
    "        if os.path.exists(self.index_file):\n",
    "            print(f\"Loading existing FAISS index from {self.index_file}\")\n",
    "            index = faiss.read_index(self.index_file)\n",
    "            if index.d != self.dimension:\n",
    "                raise ValueError(f\"Index dimension mismatch: loaded index has {index.d}, manager expects {self.dimension}.\")\n",
    "            return index\n",
    "        else:\n",
    "            print(f\"Creating new FAISS index with dimension {self.dimension}.\")\n",
    "            return faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "    def _load_json_to_set(self, file_path: str) -> set:\n",
    "        \"\"\"Loads a JSON array from a file into a set.\"\"\"\n",
    "        if not os.path.exists(file_path): return set()\n",
    "        try:\n",
    "            with open(file_path, 'r') as f: return set(json.load(f))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            print(f\"Warning: {file_path} is corrupted. Starting fresh.\")\n",
    "            return set()\n",
    "\n",
    "    def _load_json_to_dict(self, file_path: str) -> dict:\n",
    "        \"\"\"Loads a JSON object from a file into a dict.\"\"\"\n",
    "        if not os.path.exists(file_path): return {}\n",
    "        try:\n",
    "            with open(file_path, 'r') as f: return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: {file_path} is corrupted. Starting fresh.\")\n",
    "            return {}\n",
    "\n",
    "    def _generate_hash(self, track_id: str, embedding: list[float]) -> str:\n",
    "        \"\"\"Generates a SHA-256 hash for a track_id and embedding.\"\"\"\n",
    "\n",
    "         # --- FIX ---\n",
    "    # Convert the embedding to a standard Python list if it's a NumPy array.\n",
    "    # The hasattr check is a safe way to detect NumPy-like objects.\n",
    "        if hasattr(embedding, 'tolist'):\n",
    "            embedding_list = embedding.tolist()\n",
    "        else:\n",
    "            embedding_list = embedding\n",
    "        # --- END FIX ---\n",
    "\n",
    "        data_to_hash = {\"track_id\": track_id, \"embedding\": embedding_list}\n",
    "        canonical_string = json.dumps(data_to_hash, sort_keys=True, separators=(',', ':'))\n",
    "        return hashlib.sha256(canonical_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def add_embedding(self, track_id: str, embedding: list[float]) -> bool:\n",
    "        \"\"\"\n",
    "        Adds an embedding, updating the FAISS index and metadata if not a duplicate.\n",
    "\n",
    "        Args:\n",
    "            track_id: The identifier for the track.\n",
    "            embedding: A 1D list of floats representing the embedding.\n",
    "\n",
    "        Returns:\n",
    "            True if the embedding was added, False if it was a duplicate.\n",
    "        \"\"\"\n",
    "        if len(embedding) != self.dimension:\n",
    "            print(f\"Error: Embedding length {len(embedding)} != index dimension {self.dimension}.\")\n",
    "            return False\n",
    "\n",
    "        new_hash = self._generate_hash(track_id, embedding)\n",
    "        if new_hash in self.existing_hashes:\n",
    "            print(f\"Duplicate found for (track_id, embedding) pair: '{track_id}'. Skipping.\")\n",
    "            return False\n",
    "            \n",
    "        # Get the numerical index for the new vector *before* adding it.\n",
    "        new_faiss_id = self.index.ntotal\n",
    "        \n",
    "        # Add to FAISS index\n",
    "        vector_batch = np.array([embedding], dtype='float32')\n",
    "        self.index.add(vector_batch)\n",
    "        \n",
    "        # Update hashes\n",
    "        self.existing_hashes.add(new_hash)\n",
    "        \n",
    "        # Update metadata\n",
    "        self.metadata[track_id].append(new_faiss_id)\n",
    "        \n",
    "        print(f\"Added new embedding for track_id '{track_id}' at FAISS index {new_faiss_id}.\")\n",
    "        return True\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    def add_embeddings(self, track_ids: list[str], embeddings: list[list[float]]) -> int:\n",
    "        \"\"\"\n",
    "        Adds a batch of embeddings, ensuring no duplicates. This is the primary \n",
    "        method for adding data.\n",
    "\n",
    "        Args:\n",
    "            track_ids: A list of track identifiers.\n",
    "            embeddings: A list of 1D embedding lists (e.g., of 512 numbers each).\n",
    "\n",
    "        Returns:\n",
    "            The number of new embeddings that were successfully added to the index.\n",
    "        \"\"\"\n",
    "        if len(track_ids) != len(embeddings):\n",
    "            raise ValueError(\"Input error: The number of track_ids must match the number of embeddings.\")\n",
    "    \n",
    "        if not track_ids:\n",
    "            print(\"Warning: Called add_embeddings with empty lists.\")\n",
    "            return 0\n",
    "\n",
    "    \n",
    "        for track_id, embedding in zip(track_ids, embeddings):\n",
    "            if len(embedding) != self.dimension:\n",
    "                print(f\"Warning: Skipping embedding for track '{track_id}'. Invalid dimension {len(embedding)}.\")\n",
    "                continue\n",
    "            self.add_embedding(track_id, embedding)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Saves the FAISS index, deduplication hashes, and metadata to disk.\"\"\"\n",
    "        print(\"\\n--- Saving all data ---\")\n",
    "\n",
    "\n",
    "        # --- FIX: Ensure destination directories exist before writing ---\n",
    "    # Create a set of unique directory paths to avoid redundant checks\n",
    "        dir_paths = {\n",
    "            os.path.dirname(self.index_file),\n",
    "            os.path.dirname(self.hashes_file),\n",
    "            os.path.dirname(self.metadata_file)\n",
    "        }\n",
    "\n",
    "        for path in dir_paths:\n",
    "        # An empty path means the file is in the current directory, no need to create.\n",
    "            if path:\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "                print(f\"Ensured directory exists: {path}\")\n",
    "    # --- END FIX ---\n",
    "        \n",
    "        # 1. Save FAISS index\n",
    "        faiss.write_index(self.index, self.index_file)\n",
    "        print(f\"FAISS index with {self.index.ntotal} vectors saved to {self.index_file}.\")\n",
    "        \n",
    "        # 2. Save hashes\n",
    "        with open(self.hashes_file, 'w') as f:\n",
    "            json.dump(list(self.existing_hashes), f)\n",
    "        print(f\"{len(self.existing_hashes)} hashes saved to {self.hashes_file}.\")\n",
    "        \n",
    "        # 3. Save metadata\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        print(f\"Metadata for {len(self.metadata)} tracks saved to {self.metadata_file}.\")\n",
    "        \n",
    "        print(\"Save complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# --\n",
    "manager = FaissManager(dimension=EMBEDDING_DIM,\n",
    "                           index_file=INDEX_FILE,\n",
    "                           hashes_file=FINGERPRINT_FILE,\n",
    "                           metadata_file=METADATA_FILE,\n",
    "                           data_root=DB_FOLDER)\n",
    "\n",
    "\n",
    "\n",
    "    # Add multiple embeddings for the same track\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293abeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database: data_sets/track_metadata.db\n",
      "Fetching track_ids...\n",
      "Found 30 track_ids to process.\n",
      "\n",
      "--- Processing track 1/30: TRAAAAK128F9318786 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAK128F9318786\n",
      "\n",
      "--- Processing track 2/30: TRAAAAV128F421A322 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAV128F421A322\n",
      "\n",
      "--- Processing track 3/30: TRAAAAW128F429D538 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAW128F429D538\n",
      "\n",
      "--- Processing track 4/30: TRAAAAY128F42A73F0 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAAY128F42A73F0\n",
      "\n",
      "--- Processing track 5/30: TRAAABD128F429CF47 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAABD128F429CF47\n",
      "\n",
      "--- Processing track 6/30: TRAAACN128F9355673 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAACN128F9355673\n",
      "\n",
      "--- Processing track 7/30: TRAAACV128F423E09E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAACV128F423E09E\n",
      "\n",
      "--- Processing track 8/30: TRAAADJ128F4287B47 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADJ128F4287B47\n",
      "\n",
      "--- Processing track 9/30: TRAAADT12903CCC339 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADT12903CCC339\n",
      "\n",
      "--- Processing track 10/30: TRAAADZ128F9348C2E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAADZ128F9348C2E\n",
      "\n",
      "--- Processing track 11/30: TRAAAEA128F935A30D ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEA128F935A30D\n",
      "\n",
      "--- Processing track 12/30: TRAAAED128E0783FAB ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAED128E0783FAB\n",
      "\n",
      "--- Processing track 13/30: TRAAAEF128F4273421 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEF128F4273421\n",
      "\n",
      "--- Processing track 14/30: TRAAAEM128F93347B9 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEM128F93347B9\n",
      "\n",
      "--- Processing track 15/30: TRAAAEW128F42930C0 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAEW128F42930C0\n",
      "\n",
      "--- Processing track 16/30: TRAAAFD128F92F423A ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFD128F92F423A\n",
      "\n",
      "--- Processing track 17/30: TRAAAFI12903CE4F0E ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFI12903CE4F0E\n",
      "\n",
      "--- Processing track 18/30: TRAAAFP128F931B4E3 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFP128F931B4E3\n",
      "\n",
      "--- Processing track 19/30: TRAAAFW128F42A4CFD ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAFW128F42A4CFD\n",
      "\n",
      "--- Processing track 20/30: TRAAAGF12903CEC202 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAGF12903CEC202\n",
      "\n",
      "--- Processing track 21/30: TRAAAGR128F425B14B ---\n",
      "Processing 1d9d16a9da90c090809c153754823c2b.mid...\n",
      "Processing 1d9d16a9da90c090809c153754823c2b.mid...\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Skipping chunk, multiple examples extracted\n",
      "Processing 5dd29e99ed7bd3cc0c5177a6e9de22ea.mid...\n",
      "Processing 5dd29e99ed7bd3cc0c5177a6e9de22ea.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Processing b97c529ab9ef783a849b896816001748.mid...\n",
      "Processing b97c529ab9ef783a849b896816001748.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Processing dac3cdd0db6341d8dc14641e44ed0d44.mid...\n",
      "Processing dac3cdd0db6341d8dc14641e44ed0d44.mid...\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "Skipping chunk, insufficient note data\n",
      "-> Success: Found and processed 4 MIDI file(s) for this track.\n",
      "\n",
      "--- Processing track 22/30: TRAAAGW12903CC1049 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAGW12903CC1049\n",
      "\n",
      "--- Processing track 23/30: TRAAAHD128F42635A5 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHD128F42635A5\n",
      "\n",
      "--- Processing track 24/30: TRAAAHE12903C9669C ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHE12903C9669C\n",
      "\n",
      "--- Processing track 25/30: TRAAAHJ128F931194C ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHJ128F931194C\n",
      "\n",
      "--- Processing track 26/30: TRAAAHO128F423BBE3 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHO128F423BBE3\n",
      "\n",
      "--- Processing track 27/30: TRAAAHZ128E0799171 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAHZ128E0799171\n",
      "\n",
      "--- Processing track 28/30: TRAAAIC128F14A5138 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAIC128F14A5138\n",
      "\n",
      "--- Processing track 29/30: TRAAAIR128F1480971 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAIR128F1480971\n",
      "\n",
      "--- Processing track 30/30: TRAAAJG128F9308A25 ---\n",
      "Warning: Directory not found at data_sets\\lmd_melodies\\A\\A\\A\\TRAAAJG128F9308A25\n",
      "\n",
      "--- Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings for a set of tracks from the database\n",
    "all_embeddings = process_tracks_from_db(\n",
    "    start_line_to_process=30,\n",
    "    num_tracks_to_process=100)\n",
    "\n",
    "\n",
    "\n",
    "total_embeddings = sum(len(array) for track_dict in all_embeddings.values() for array in track_dict.values())\n",
    "\n",
    "track_ids = []\n",
    "\n",
    "# Iterate through each track ID and its corresponding inner dictionary\n",
    "for track_id, inner_dict in all_embeddings.items():\n",
    "    # Calculate the total number of embeddings for the current track_id\n",
    "    # This uses the sum() and generator expression method from before\n",
    "    count = sum(len(array) for array in inner_dict.values())  \n",
    "    # Extend the main list by adding the track_id 'count' times\n",
    "    # The expression [track_id] * count creates a new list like ['document_1', 'document_1', ...]\n",
    "    track_ids.extend([track_id] * count)\n",
    "\n",
    "# Flatten all numbers into a single list\n",
    "embedding_matrix = []\n",
    "\n",
    "# Loop through the top-level dictionary\n",
    "for inner_dict in all_embeddings.values():\n",
    "    # Loop through the second-level dictionary\n",
    "    for list_of_tuples in inner_dict.values():\n",
    "        # Loop through the list of tuples\n",
    "        for tpl in list_of_tuples:\n",
    "            # 1. Access the first element of the tuple (the wrapper)\n",
    "            wrapper = tpl[0]        \n",
    "            # 2. Access the first (and only) element of the wrapper\n",
    "            embedding_vector = wrapper[0]          \n",
    "            # 3. Add the 512-element vector to our matrix\n",
    "            embedding_matrix.append(embedding_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0f1015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 0.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 1.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 2.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 3.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 4.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 5.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 6.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 7.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 8.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 9.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 10.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 11.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 12.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 13.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 14.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 15.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 16.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 17.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 18.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 19.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 20.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 21.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 22.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 23.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 24.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 25.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 26.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 27.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 28.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 29.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 30.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 31.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 32.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 33.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 34.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 35.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 36.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 37.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 38.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 39.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 40.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 41.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 42.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 43.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 44.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 45.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 46.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 47.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 48.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 49.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 50.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 51.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 52.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 53.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 54.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 55.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 56.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 57.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 58.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 59.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 60.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 61.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 62.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 63.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 64.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 65.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 66.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 67.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 68.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 69.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 70.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 71.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 72.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 73.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 74.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 75.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 76.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 77.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 78.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 79.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 80.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 81.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 82.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 83.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 84.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 85.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 86.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 87.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 88.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 89.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 90.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 91.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 92.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 93.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 94.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 95.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 96.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 97.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 98.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 99.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 100.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 101.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 102.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 103.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 104.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 105.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 106.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 107.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 108.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 109.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 110.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 111.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 112.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 113.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 114.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 115.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 116.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 117.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 118.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 119.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 120.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 121.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 122.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 123.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 124.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 125.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 126.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 127.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 128.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 129.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 130.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 131.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 132.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 133.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 134.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 135.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 136.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 137.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 138.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 139.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 140.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 141.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 142.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 143.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 144.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 145.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 146.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 147.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 148.\n",
      "Added new embedding for track_id 'TRAAAGR128F425B14B' at FAISS index 149.\n",
      "\n",
      "--- Saving all data ---\n",
      "Ensured directory exists: data_sets\\faiss\n",
      "FAISS index with 150 vectors saved to data_sets\\faiss\\faiss.index.\n",
      "150 hashes saved to data_sets\\faiss\\embedding_fingerprints.json.\n",
      "Metadata for 1 tracks saved to data_sets\\faiss\\faiss_metadata.json.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "manager.add_embeddings(track_ids, embedding_matrix) \n",
    "manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1d9f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Searching for the single nearest track ---\n",
      "The nearest track is: 'TRAAAGR128F425B14B'\n",
      "Distance: 585.3926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Example Usage ---\n",
    "# This demonstrates how to call the function.\n",
    "    \n",
    "    # 1. Create a random \"observed\" embedding to simulate a query\n",
    "    # In a real application, this would come from your MIDI processing\n",
    "observed_embedding = np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "    # 2. Search for the single nearest track ID (k=1)\n",
    "print(\"--- Searching for the single nearest track ---\")\n",
    "nearest_results = search_nearest_track(observed_embedding, k=1)\n",
    "\n",
    "if nearest_results:\n",
    "    # The function returns a list, so we get the first element\n",
    "    track_id, distance = nearest_results[0]\n",
    "    print(f\"The nearest track is: '{track_id}'\")\n",
    "    print(f\"Distance: {distance:.4f}\")\n",
    "else:\n",
    "    print(\"Search returned no results.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03607e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
