{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a083f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Keras model loaded successfully.\n",
      "\n",
      "Loading TFLite model from: models/music_vae_encoder_tf2.tflite...\n",
      "TFLite model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  1. SETUP: Configuration and Environment\n",
    "# ======================================================================\n",
    "\n",
    "# The original MusicVAE model requires a TF1 compatibility environment.\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "\n",
    "BASE_DIR=\"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "MUSICVAE_CHECKPOINT_DIR= BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Path to your saved Keras standalone encoder model\n",
    "KERAS_MODEL_PATH = 'models/music_vae_encoder_keras'\n",
    "# Path to your newly created TFLite model\n",
    "TFLITE_MODEL_PATH = 'models/music_vae_encoder_tf2.tflite'\n",
    "\n",
    "# --- Model-specific tensor names for the original MusicVAE ---\n",
    "# (These are based on our previous explorations)\n",
    "MUSICVAE_INPUT_TENSOR_NAME = \"Placeholder_2:0\"\n",
    "MUSICVAE_INPUT_LENGTH_NAME = \"Placeholder_1:0\"\n",
    "MUSICVAE_OUTPUT_TENSOR_NAME = \"encoder/mu/BiasAdd:0\"\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  2. LOAD MODELS: Load each of the three models\n",
    "# ======================================================================\n",
    "\n",
    "print(\"--- Loading Models ---\")\n",
    "\n",
    "# --- Model A: Original MusicVAE (from Checkpoint) ---\n",
    "print(\"Loading original MusicVAE model...\")\n",
    "graph_a = tf.Graph()\n",
    "\n",
    "with graph_a.as_default():\n",
    "    # Load the MusicVAE model\n",
    "    mel_2bar = TrainedModel(mel_2bar_config, batch_size=BATCH_SIZE, checkpoint_dir_or_path=MUSICVAE_CHECKPOINT_DIR)\n",
    "\n",
    "sess_a=mel_2bar._sess\n",
    "\n",
    "\n",
    "\n",
    "# --- Model B: Standalone Encoder (from Keras .h5 file) ---\n",
    "print(f\"\\nLoading Keras model from: {KERAS_MODEL_PATH}...\")\n",
    "# We load this into its own graph and session to keep it isolated.\n",
    "graph_b = tf.Graph()\n",
    "sess_b = tf.compat.v1.Session(graph=graph_b)\n",
    "with graph_b.as_default(), sess_b.as_default():\n",
    "    keras_encoder = tf.keras.models.load_model(KERAS_MODEL_PATH)\n",
    "print(\"Keras model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Model C: TFLite Encoder (from .tflite file) ---\n",
    "print(f\"\\nLoading TFLite model from: {TFLITE_MODEL_PATH}...\")\n",
    "# The TFLite interpreter is independent of TF sessions and graphs.\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output details for later use\n",
    "tflite_input_details = interpreter.get_input_details()\n",
    "tflite_output_details = interpreter.get_output_details()\n",
    "print(\"TFLite model loaded successfully.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358dad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Generating a random input tensor ---\n",
      "Generated random input with shape: (1, 32, 90)\n",
      "Generated empty controls with shape: (1, 32, 0)\n",
      "\n",
      "--- Running Inference ---\n",
      "Got embedding from Keras model.\n",
      "Got embedding from TFLite model.\n",
      "\n",
      "--- Comparing Embeddings ---\n",
      "MusicVAE Embedding (sample): [-0.93795854  1.1208605   1.7217035  -1.7476901  -1.1771013 ]\n",
      "Keras Embedding (sample):   [-0.8793464  1.0845115  1.660597  -1.6555295 -1.2786037]\n",
      "TFLite Embedding (sample):  [-0.8523565  1.1237763  1.5658027 -1.6850423 -1.304228 ]\n",
      "\n",
      "--- Embedding Distances (Euclidean) ---\n",
      "Distance (MusicVAE vs. Keras):   1.985370\n",
      "Distance (Keras vs. TFLite):     1.536174\n",
      "Distance (MusicVAE vs. TFLite):  2.508734\n",
      "\n",
      "--- Analysis ---\n",
      "⚠️ The Keras and TFLite models show a numerical difference. This can be due to quantization or optimizations during conversion.\n",
      "ℹ️ The original MusicVAE and Keras models have a notable difference (distance: 1.9854). This confirms the discrepancy we observed previously.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#  3. PREPARE INPUT: Create a single, common input sequence\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Step 3: Generating a random input tensor ---\")\n",
    "seq_len = mel_2bar_config.hparams.max_seq_len\n",
    "input_depth = mel_2bar_config.data_converter.input_depth\n",
    "control_depth = mel_2bar_config.data_converter.control_depth # This will be 0\n",
    "input_shape = (BATCH_SIZE, seq_len, input_depth)\n",
    "\n",
    "random_input = np.random.rand(*input_shape).astype(np.float32)\n",
    "print(f\"Generated random input with shape: {random_input.shape}\")\n",
    "\n",
    "# Create an empty array for the `_controls` placeholder\n",
    "empty_controls = np.zeros((BATCH_SIZE, seq_len, control_depth), dtype=np.float32)\n",
    "print(f\"Generated empty controls with shape: {empty_controls.shape}\")\n",
    "\n",
    "# Add the empty controls to the feed_dict\n",
    "feed_dict = {\n",
    "    mel_2bar._inputs: random_input,\n",
    "    mel_2bar._inputs_length: [seq_len] * BATCH_SIZE,\n",
    "    mel_2bar._controls: empty_controls # Add the required empty placeholder value\n",
    "}\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  4. RUN INFERENCE: Get embeddings from all three models\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "\n",
    "# --- Get Embedding A: MusicVAE ---\n",
    "# `_mu` is the tensor that holds the embedding\n",
    "musicvae_embedding = sess_a.run(mel_2bar._mu, feed_dict)\n",
    "\n",
    "\n",
    "# --- Get Embedding B: Keras Encoder ---\n",
    "with graph_b.as_default():\n",
    "    keras_embedding = sess_b.run(\n",
    "        keras_encoder.output,\n",
    "        feed_dict={\n",
    "            keras_encoder.input: random_input\n",
    "        }\n",
    "    )\n",
    "print(\"Got embedding from Keras model.\")\n",
    "\n",
    "# --- Get Embedding C: TFLite Encoder ---\n",
    "interpreter.set_tensor(tflite_input_details[0]['index'], random_input)\n",
    "interpreter.invoke()\n",
    "tflite_embedding = interpreter.get_tensor(tflite_output_details[0]['index'])\n",
    "print(\"Got embedding from TFLite model.\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  5. COMPARE RESULTS: Calculate and display the differences\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Comparing Embeddings ---\")\n",
    "\n",
    "# Print the first 5 values of each embedding for a visual check\n",
    "print(f\"MusicVAE Embedding (sample): {musicvae_embedding[0, :5]}\")\n",
    "print(f\"Keras Embedding (sample):   {keras_embedding[0, :5]}\")\n",
    "print(f\"TFLite Embedding (sample):  {tflite_embedding[0, :5]}\")\n",
    "\n",
    "# Calculate the Euclidean distance between the embeddings\n",
    "dist_musicvae_vs_keras = np.linalg.norm(musicvae_embedding - keras_embedding)\n",
    "dist_keras_vs_tflite = np.linalg.norm(keras_embedding - tflite_embedding)\n",
    "dist_musicvae_vs_tflite = np.linalg.norm(musicvae_embedding - tflite_embedding)\n",
    "\n",
    "print(\"\\n--- Embedding Distances (Euclidean) ---\")\n",
    "print(f\"Distance (MusicVAE vs. Keras):   {dist_musicvae_vs_keras:.6f}\")\n",
    "print(f\"Distance (Keras vs. TFLite):     {dist_keras_vs_tflite:.6f}\")\n",
    "print(f\"Distance (MusicVAE vs. TFLite):  {dist_musicvae_vs_tflite:.6f}\")\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "if dist_keras_vs_tflite < 1e-5:\n",
    "    print(\"✅ The Keras and TFLite models produce nearly identical embeddings. The conversion was successful.\")\n",
    "else:\n",
    "    print(\"⚠️ The Keras and TFLite models show a numerical difference. This can be due to quantization or optimizations during conversion.\")\n",
    "\n",
    "if dist_musicvae_vs_keras < 1e-5:\n",
    "     print(\"✅ The original MusicVAE and the Keras standalone encoder produce nearly identical embeddings.\")\n",
    "else:\n",
    "     print(f\"ℹ️ The original MusicVAE and Keras models have a notable difference (distance: {dist_musicvae_vs_keras:.4f}). This confirms the discrepancy we observed previously.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
