{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a083f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Models ---\n",
      "Loading original MusicVAE model...\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [2048, 2048, 2048]\n",
      "\n",
      "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\lstm_utils.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:749: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\contrib\\rnn.py:751: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self._bias = self.add_variable(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/download.magenta.tensorflow.org/models/music_vae/checkpoints/mel_2bar_big.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:195: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  mu = tf.layers.dense(\n",
      "d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\magenta\\models\\music_vae\\base_model.py:200: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  sigma = tf.layers.dense(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Keras model from: models/music_vae_encoder_keras...\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Keras model loaded successfully.\n",
      "\n",
      "Loading TFLite model from: models/music_vae_encoder_tf2.tflite...\n",
      "TFLite model loaded successfully.\n",
      "\n",
      "--- Step 3: Generating a random input tensor ---\n",
      "Generated random input with shape: (1, 32, 90)\n",
      "Generated empty controls with shape: (1, 32, 0)\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  1. SETUP: Configuration and Environment\n",
    "# ======================================================================\n",
    "\n",
    "# The original MusicVAE model requires a TF1 compatibility environment.\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "\n",
    "BASE_DIR=\"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "MUSICVAE_CHECKPOINT_DIR= BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Path to your saved Keras standalone encoder model\n",
    "KERAS_MODEL_PATH = 'models/music_vae_encoder_keras'\n",
    "# Path to your newly created TFLite model\n",
    "TFLITE_MODEL_PATH = 'models/music_vae_encoder_tf2.tflite'\n",
    "\n",
    "# --- Model-specific tensor names for the original MusicVAE ---\n",
    "# (These are based on our previous explorations)\n",
    "MUSICVAE_INPUT_TENSOR_NAME = \"Placeholder_2:0\"\n",
    "MUSICVAE_INPUT_LENGTH_NAME = \"Placeholder_1:0\"\n",
    "MUSICVAE_OUTPUT_TENSOR_NAME = \"encoder/mu/BiasAdd:0\"\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  2. LOAD MODELS: Load each of the three models\n",
    "# ======================================================================\n",
    "\n",
    "print(\"--- Loading Models ---\")\n",
    "\n",
    "# --- Model A: Original MusicVAE (from Checkpoint) ---\n",
    "print(\"Loading original MusicVAE model...\")\n",
    "graph_a = tf.Graph()\n",
    "sess_a = tf.compat.v1.Session(graph=graph_a)\n",
    "with graph_a.as_default(), sess_a.as_default():\n",
    "    # Load the MusicVAE model\n",
    "    mel_2bar = TrainedModel(mel_2bar_config, batch_size=BATCH_SIZE, checkpoint_dir_or_path=MUSICVAE_CHECKPOINT_DIR)\n",
    "\n",
    "\n",
    "\n",
    "# --- Model B: Standalone Encoder (from Keras .h5 file) ---\n",
    "print(f\"\\nLoading Keras model from: {KERAS_MODEL_PATH}...\")\n",
    "# We load this into its own graph and session to keep it isolated.\n",
    "graph_b = tf.Graph()\n",
    "sess_b = tf.compat.v1.Session(graph=graph_b)\n",
    "with graph_b.as_default(), sess_b.as_default():\n",
    "    keras_encoder = tf.keras.models.load_model(KERAS_MODEL_PATH)\n",
    "print(\"Keras model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- Model C: TFLite Encoder (from .tflite file) ---\n",
    "print(f\"\\nLoading TFLite model from: {TFLITE_MODEL_PATH}...\")\n",
    "# The TFLite interpreter is independent of TF sessions and graphs.\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "# Get input and output details for later use\n",
    "tflite_input_details = interpreter.get_input_details()\n",
    "tflite_output_details = interpreter.get_output_details()\n",
    "print(\"TFLite model loaded successfully.\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  3. PREPARE INPUT: Create a single, common input sequence\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Step 3: Generating a random input tensor ---\")\n",
    "seq_len = mel_2bar_config.hparams.max_seq_len\n",
    "input_depth = mel_2bar_config.data_converter.input_depth\n",
    "control_depth = mel_2bar_config.data_converter.control_depth # This will be 0\n",
    "input_shape = (BATCH_SIZE, seq_len, input_depth)\n",
    "\n",
    "random_input = np.random.rand(*input_shape).astype(np.float32)\n",
    "print(f\"Generated random input with shape: {random_input.shape}\")\n",
    "\n",
    "# Create an empty array for the `_controls` placeholder\n",
    "empty_controls = np.zeros((BATCH_SIZE, seq_len, control_depth), dtype=np.float32)\n",
    "print(f\"Generated empty controls with shape: {empty_controls.shape}\")\n",
    "\n",
    "# Add the empty controls to the feed_dict\n",
    "feed_dict = {\n",
    "    mel_2bar._inputs: random_input,\n",
    "    mel_2bar._inputs_length: [seq_len] * BATCH_SIZE,\n",
    "    mel_2bar._controls: empty_controls # Add the required empty placeholder value\n",
    "}\n",
    "# `_mu` is the tensor that holds the embedding\n",
    "original_embedding = sess_a.run(mel_2bar._mu, feed_dict)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  4. RUN INFERENCE: Get embeddings from all three models\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "\n",
    "# --- Get Embedding A: MusicVAE ---\n",
    "with graph_a.as_default():\n",
    "    musicvae_embedding = sess_a.run(\n",
    "        MUSICVAE_OUTPUT_TENSOR_NAME,\n",
    "        feed_dict={\n",
    "            MUSICVAE_INPUT_TENSOR_NAME: common_input_sequence,\n",
    "            MUSICVAE_INPUT_LENGTH_NAME: sequence_length\n",
    "        }\n",
    "    )\n",
    "print(\"Got embedding from MusicVAE.\")\n",
    "\n",
    "# --- Get Embedding B: Keras Encoder ---\n",
    "with graph_b.as_default():\n",
    "    keras_embedding = sess_b.run(\n",
    "        keras_encoder.output,\n",
    "        feed_dict={\n",
    "            keras_encoder.input: common_input_sequence\n",
    "        }\n",
    "    )\n",
    "print(\"Got embedding from Keras model.\")\n",
    "\n",
    "# --- Get Embedding C: TFLite Encoder ---\n",
    "interpreter.set_tensor(tflite_input_details[0]['index'], common_input_sequence)\n",
    "interpreter.invoke()\n",
    "tflite_embedding = interpreter.get_tensor(tflite_output_details[0]['index'])\n",
    "print(\"Got embedding from TFLite model.\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  5. COMPARE RESULTS: Calculate and display the differences\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Comparing Embeddings ---\")\n",
    "\n",
    "# Print the first 5 values of each embedding for a visual check\n",
    "print(f\"MusicVAE Embedding (sample): {musicvae_embedding[0, :5]}\")\n",
    "print(f\"Keras Embedding (sample):   {keras_embedding[0, :5]}\")\n",
    "print(f\"TFLite Embedding (sample):  {tflite_embedding[0, :5]}\")\n",
    "\n",
    "# Calculate the Euclidean distance between the embeddings\n",
    "dist_musicvae_vs_keras = np.linalg.norm(musicvae_embedding - keras_embedding)\n",
    "dist_keras_vs_tflite = np.linalg.norm(keras_embedding - tflite_embedding)\n",
    "dist_musicvae_vs_tflite = np.linalg.norm(musicvae_embedding - tflite_embedding)\n",
    "\n",
    "print(\"\\n--- Embedding Distances (Euclidean) ---\")\n",
    "print(f\"Distance (MusicVAE vs. Keras):   {dist_musicvae_vs_keras:.6f}\")\n",
    "print(f\"Distance (Keras vs. TFLite):     {dist_keras_vs_tflite:.6f}\")\n",
    "print(f\"Distance (MusicVAE vs. TFLite):  {dist_musicvae_vs_tflite:.6f}\")\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "if dist_keras_vs_tflite < 1e-5:\n",
    "    print(\"✅ The Keras and TFLite models produce nearly identical embeddings. The conversion was successful.\")\n",
    "else:\n",
    "    print(\"⚠️ The Keras and TFLite models show a numerical difference. This can be due to quantization or optimizations during conversion.\")\n",
    "\n",
    "if dist_musicvae_vs_keras < 1e-5:\n",
    "     print(\"✅ The original MusicVAE and the Keras standalone encoder produce nearly identical embeddings.\")\n",
    "else:\n",
    "     print(f\"ℹ️ The original MusicVAE and Keras models have a notable difference (distance: {dist_musicvae_vs_keras:.4f}). This confirms the discrepancy we observed previously.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embedding = sess_a.run(mel_2bar._mu, feed_dict)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  4. RUN INFERENCE: Get embeddings from all three models\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "\n",
    "# --- Get Embedding A: MusicVAE ---\n",
    "with graph_a.as_default():\n",
    "    musicvae_embedding = sess_a.run(\n",
    "        MUSICVAE_OUTPUT_TENSOR_NAME,\n",
    "        feed_dict={\n",
    "            MUSICVAE_INPUT_TENSOR_NAME: common_input_sequence,\n",
    "            MUSICVAE_INPUT_LENGTH_NAME: sequence_length\n",
    "        }\n",
    "    )\n",
    "print(\"Got embedding from MusicVAE.\")\n",
    "\n",
    "# --- Get Embedding B: Keras Encoder ---\n",
    "with graph_b.as_default():\n",
    "    keras_embedding = sess_b.run(\n",
    "        keras_encoder.output,\n",
    "        feed_dict={\n",
    "            keras_encoder.input: common_input_sequence\n",
    "        }\n",
    "    )\n",
    "print(\"Got embedding from Keras model.\")\n",
    "\n",
    "# --- Get Embedding C: TFLite Encoder ---\n",
    "interpreter.set_tensor(tflite_input_details[0]['index'], common_input_sequence)\n",
    "interpreter.invoke()\n",
    "tflite_embedding = interpreter.get_tensor(tflite_output_details[0]['index'])\n",
    "print(\"Got embedding from TFLite model.\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  5. COMPARE RESULTS: Calculate and display the differences\n",
    "# ======================================================================\n",
    "\n",
    "print(\"\\n--- Comparing Embeddings ---\")\n",
    "\n",
    "# Print the first 5 values of each embedding for a visual check\n",
    "print(f\"MusicVAE Embedding (sample): {musicvae_embedding[0, :5]}\")\n",
    "print(f\"Keras Embedding (sample):   {keras_embedding[0, :5]}\")\n",
    "print(f\"TFLite Embedding (sample):  {tflite_embedding[0, :5]}\")\n",
    "\n",
    "# Calculate the Euclidean distance between the embeddings\n",
    "dist_musicvae_vs_keras = np.linalg.norm(musicvae_embedding - keras_embedding)\n",
    "dist_keras_vs_tflite = np.linalg.norm(keras_embedding - tflite_embedding)\n",
    "dist_musicvae_vs_tflite = np.linalg.norm(musicvae_embedding - tflite_embedding)\n",
    "\n",
    "print(\"\\n--- Embedding Distances (Euclidean) ---\")\n",
    "print(f\"Distance (MusicVAE vs. Keras):   {dist_musicvae_vs_keras:.6f}\")\n",
    "print(f\"Distance (Keras vs. TFLite):     {dist_keras_vs_tflite:.6f}\")\n",
    "print(f\"Distance (MusicVAE vs. TFLite):  {dist_musicvae_vs_tflite:.6f}\")\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "if dist_keras_vs_tflite < 1e-5:\n",
    "    print(\"✅ The Keras and TFLite models produce nearly identical embeddings. The conversion was successful.\")\n",
    "else:\n",
    "    print(\"⚠️ The Keras and TFLite models show a numerical difference. This can be due to quantization or optimizations during conversion.\")\n",
    "\n",
    "if dist_musicvae_vs_keras < 1e-5:\n",
    "     print(\"✅ The original MusicVAE and the Keras standalone encoder produce nearly identical embeddings.\")\n",
    "else:\n",
    "     print(f\"ℹ️ The original MusicVAE and Keras models have a notable difference (distance: {dist_musicvae_vs_keras:.4f}). This confirms the discrepancy we observed previously.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
