{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643329ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "\n",
    "import numpy as np\n",
    "from magenta.models.music_vae.trained_model import TrainedModel # We need this class\n",
    "\n",
    "def build_standalone_encoder(hparams, input_shape):\n",
    "    \"\"\"\n",
    "    Builds a standard Keras model that replicates the custom BidirectionalLstmEncoder.\n",
    "    \n",
    "    Args:\n",
    "        hparams: The same hparams object used by the original model.\n",
    "        input_shape: The shape of a single input sequence (e.g., (96, 64)).\n",
    "    Returns:\n",
    "        A tf.keras.Model instance that replicates MusicVAE's encoder architecture in TF2 style\n",
    "        and as a second return value, the graph containing the model.\n",
    "    \"\"\"\n",
    "\n",
    "    encoder_graph = tf.Graph()\n",
    "    with encoder_graph.as_default():\n",
    "    \n",
    "        # 1. Define the input layer\n",
    "        input_tensor = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "        # 2. Re-create the stacked LSTM cells as Keras layers\n",
    "        # The original code uses a shared cell for fw and bw. The modern equivalent\n",
    "        # is to create one stack of LSTM layers and wrap it in Bidirectional.\n",
    "    \n",
    "        # Start with the input tensor\n",
    "        x = input_tensor\n",
    "    \n",
    "        # Create the stack of LSTM layers\n",
    "        # The `return_sequences=True` is crucial for all but the last layer in a stack.\n",
    "        num_lstm_layers = len(hparams.enc_rnn_size)\n",
    "        for i, layer_size in enumerate(hparams.enc_rnn_size):\n",
    "            lstm_layer = tf.keras.layers.LSTM(\n",
    "                units=layer_size, \n",
    "                return_sequences=False, # The last (and only) LSTM returns a single vector\n",
    "                name='multi_rnn_cell/cell_0/lstm_cell', # CRITICAL: Match TF1 variable scope\n",
    "                recurrent_dropout=1.0 - hparams.dropout_keep_prob # To match the legacy DropoutWrapper\n",
    "            )\n",
    "        \n",
    "            bi_lstm = tf.keras.layers.Bidirectional(\n",
    "                lstm_layer,\n",
    "                name='bidirectional_rnn' # CRITICAL: Match TF1 variable scope\n",
    "            )\n",
    "\n",
    "            # To create the 'cell_0' scope, we wrap the Bidirectional layer in a tiny sub-model\n",
    "            # This is the key to matching the checkpoint's variable names.\n",
    "            bi_lstm_model = tf.keras.Model(inputs=x, outputs=bi_lstm(x), name=f'cell_{i}')\n",
    "            x = bi_lstm_model(x)\n",
    "\n",
    "        # 3. Replicate the flatten operation\n",
    "        # The original code flattens, but since the last LSTM returns a single vector per direction,\n",
    "        # the output of Bidirectional is already \"flat\" in the time dimension.\n",
    "    \n",
    "        # 4. Re-create the final Dense layers\n",
    "        # This layer needs to be inside an 'encoder' scope, which we will achieve by wrapping the whole thing.\n",
    "        mu_layer = tf.keras.layers.Dense(hparams.z_size, name='mu')\n",
    "    \n",
    "        # The final output of our model is just the embedding (mu)\n",
    "        mu_output = mu_layer(x)\n",
    "    \n",
    "        # 5. Create and return the final Keras model\n",
    "        # We name the final model 'encoder' to add the last required scope.\n",
    "        encoder_model = tf.keras.Model(inputs=input_tensor, outputs=mu_output, name='encoder')\n",
    "    \n",
    "        print(\"--- Standalone Keras Encoder Built with FINAL Corrected Naming ---\")\n",
    "        encoder_model.summary()\n",
    "    \n",
    "    return encoder_model, encoder_graph\n",
    "\n",
    "\n",
    "def standalone_encoder_with_weight_copy(music_vae_trained_model,music_vae_model_config):\n",
    "    \"\"\"\n",
    "    Instantiates a standalone TF2-style encore that \n",
    "    contains the approximative function and weights of \n",
    "    the MusicVAE encoder, which is in TF1 format\n",
    "    \n",
    "    Args:\n",
    "        music_vae_trained_model: a TrainedModel instance.\n",
    "        music_vae_model_config: The configuration associated with the TrainedModel\n",
    "    \"\"\"\n",
    "    sess = music_vae_trained_model._sess\n",
    "    \n",
    "    print(\"Step 1: Successfully obtained session from original model\")\n",
    "\n",
    "    print(\"\\n--- Step 2: Building the new standalone TF2 Keras encoder ---\")\n",
    "    encoder_input_shape = (music_vae_model_config.hparams.max_seq_len, music_vae_model_config.data_converter.input_depth)\n",
    "    standalone_encoder, standalone_encoder_graph = build_standalone_encoder(music_vae_model_config.hparams, input_shape=encoder_input_shape)\n",
    "\n",
    "    with standalone_encoder_graph.as_default():\n",
    "\n",
    "        print(\"\\n--- Step 3: Starting weight transfer from TF1 session to TF2 model ---\")\n",
    "        tf1_variables = sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        tf1_variable_names = [v.name for v in tf1_variables]\n",
    "        tf1_variable_values = sess.run(tf1_variables)\n",
    "        tf1_weights_map = dict(zip(tf1_variable_names, tf1_variable_values))\n",
    "\n",
    "        # Iterate through the layers of our new Keras model to set their weights\n",
    "        for layer in standalone_encoder.layers:\n",
    "            if layer.name == 'cell_0': # This is our Bidirectional sub-model\n",
    "                for sub_layer in layer.layers:\n",
    "                    if isinstance(sub_layer, tf.keras.layers.Bidirectional):\n",
    "                        print(f\"\\nProcessing layer: {layer.name}/{sub_layer.name}\")\n",
    "                \n",
    "                        fw_kernel_name = 'encoder/cell_0/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0'\n",
    "                        fw_bias_name = 'encoder/cell_0/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0'\n",
    "                        bw_kernel_name = 'encoder/cell_0/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0'\n",
    "                        bw_bias_name = 'encoder/cell_0/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0'\n",
    "                \n",
    "                        try:\n",
    "                            # --- WEIGHT RE-ORDERING LOGIC ---\n",
    "                            def reorder_lstm_weights(kernel, bias, num_units):\n",
    "                                # TF1 format: [i, c, f, o] (input, cell, forget, output)\n",
    "                                # TF2 format: [i, f, c, o] (input, forget, cell, output)\n",
    "                        \n",
    "                                # Split into the 4 gate weights\n",
    "                                k_i, k_c, k_f, k_o = np.split(kernel, 4, axis=-1)\n",
    "                                b_i, b_c, b_f, b_o = np.split(bias, 4, axis=-1)\n",
    "\n",
    "                                # The legacy LSTMCell adds a `forget_bias` of 1.0 by default.\n",
    "                                # We must manually add it to the forget gate's bias.\n",
    "                                b_f = b_f + 1.0\n",
    "                        \n",
    "                                # Re-assemble in TF2 order (swap c and f)\n",
    "                                reordered_kernel = np.concatenate([k_i, k_f, k_c, k_o], axis=-1)\n",
    "                                reordered_bias = np.concatenate([b_i, b_f, b_c, b_o], axis=-1)\n",
    "                        \n",
    "                                return reordered_kernel, reordered_bias\n",
    "\n",
    "                            # Get weights from checkpoint\n",
    "                            fw_kernel_tf1, fw_bias_tf1 = tf1_weights_map[fw_kernel_name], tf1_weights_map[fw_bias_name]\n",
    "                            bw_kernel_tf1, bw_bias_tf1 = tf1_weights_map[bw_kernel_name], tf1_weights_map[bw_bias_name]\n",
    "\n",
    "                            # Re-order weights to match Keras's expected format\n",
    "                            num_units = music_vae_model_config.hparams.enc_rnn_size[0]\n",
    "                            fw_kernel_tf2, fw_bias_tf2 = reorder_lstm_weights(fw_kernel_tf1, fw_bias_tf1, num_units)\n",
    "                            bw_kernel_tf2, bw_bias_tf2 = reorder_lstm_weights(bw_kernel_tf1, bw_bias_tf1, num_units)\n",
    "\n",
    "                            # Split the re-ordered kernels for Keras\n",
    "                            input_depth = music_vae_model_config.data_converter.input_depth\n",
    "                            fw_input_kernel, fw_recurrent_kernel = np.split(fw_kernel_tf2, [input_depth], axis=0)\n",
    "                            bw_input_kernel, bw_recurrent_kernel = np.split(bw_kernel_tf2, [input_depth], axis=0)\n",
    "                    \n",
    "                            # Assemble the final list of 6 weights\n",
    "                            keras_weights = [\n",
    "                                fw_input_kernel, fw_recurrent_kernel, fw_bias_tf2,\n",
    "                                bw_input_kernel, bw_recurrent_kernel, bw_bias_tf2\n",
    "                            ]\n",
    "                    \n",
    "                            sub_layer.set_weights(keras_weights)\n",
    "                            print(f\"  - Successfully re-ordered, split, and transferred 6 weights for Bidirectional LSTM.\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"  - FATAL ERROR: An exception occurred during weight transfer: {e}\")\n",
    "\n",
    "            if layer.name == 'mu' and isinstance(layer, tf.keras.layers.Dense):\n",
    "                # This part remains the same\n",
    "                print(f\"\\nProcessing layer: {layer.name}\")\n",
    "                kernel_name = f'encoder/mu/kernel:0'\n",
    "                bias_name = f'encoder/mu/bias:0'\n",
    "                try:\n",
    "                    layer.set_weights([tf1_weights_map[kernel_name], tf1_weights_map[bias_name]])\n",
    "                    print(f\"  - Successfully transferred weights for Dense layer 'mu'.\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"  - FATAL ERROR: Could not find weight {e} for 'mu' layer!\")\n",
    "\n",
    "    return standalone_encoder, standalone_encoder_graph\n",
    "\n",
    "def load_standalone_encoder(SAVED_ENCODER_PATH):\n",
    "    \"\"\"\n",
    "    Loads a standalone Keras encoder model from disk into its own graph and session.\n",
    "    \n",
    "    Args:\n",
    "        SAVED_ENCODER_PATH: Path to the saved Keras model (HDF5 or SavedModel format).\n",
    "    \"\"\"    \n",
    "\n",
    "\n",
    "    # ======================================================================\n",
    "    #  Load the Standalone Encoder into its own Graph and Session\n",
    "    # ======================================================================\n",
    "\n",
    "    # 1. Create a new, empty graph to serve as the blueprint for our loaded model.\n",
    "    encoder_graph = tf.Graph()\n",
    "\n",
    "    # 2. Create a new session that will be dedicated to this new graph.\n",
    "    # We create it now so we can use it to initialize variables after loading.\n",
    "    encoder_sess = tf.compat.v1.Session(graph=encoder_graph)\n",
    "\n",
    "    # 3. Use the graph's context to load the model. This tells Keras where to\n",
    "    #    rebuild the model's architecture.\n",
    "    with encoder_graph.as_default():\n",
    "        # Set the session context for Keras to use during loading.\n",
    "        with encoder_sess.as_default():\n",
    "            print(f\"Loading standalone encoder from: {SAVED_ENCODER_PATH}\")\n",
    "        \n",
    "            # Load the model. Keras will rebuild the layers on `encoder_graph`.\n",
    "            loaded_standalone_encoder = tf.keras.models.load_model(SAVED_ENCODER_PATH)\n",
    "        \n",
    "            # IMPORTANT: Initialize all variables in the new session.\n",
    "            # This finalizes the model loading process for the TF1 session.\n",
    "            #encoder_sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    return loaded_standalone_encoder, encoder_graph, encoder_sess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over the weights and save as TF2-style model\n",
    "\n",
    "print(\"--- Step 1: Loading original TF1-style MusicVAE model ---\")\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "BASE_DIR = \"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "checkpoint_path = BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=4, checkpoint_dir_or_path=checkpoint_path)\n",
    "print(\"Original model loaded successfully.\")\n",
    "\n",
    "standalone_encoder, standalone_encoder_graph=standalone_encoder_with_weight_copy(mel_2bar,mel_2bar_config)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Weight transfer complete. Saving the new Keras model. ---\")\n",
    "output_keras_model_path = 'models/music_vae_encoder_keras'\n",
    "tf.keras.models.save_model(standalone_encoder, output_keras_model_path)\n",
    "print(f\"\\nSuccessfully saved the new Keras model to: '{output_keras_model_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tensorflow.compat.v1 and disable V2 behavior for the original model\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP & MODEL LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- Step 1: Loading original TF1-style MusicVAE model ---\")\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "BASE_DIR = \"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "checkpoint_path = BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "# Use a batch size of 1 for easier comparison\n",
    "BATCH_SIZE = 1\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=BATCH_SIZE, checkpoint_dir_or_path=checkpoint_path)\n",
    "sess = mel_2bar._sess\n",
    "print(\"Original model loaded.\")\n",
    "\n",
    "print(\"\\n--- Step 2: Loading the new standalone Keras encoder ---\")\n",
    "keras_model_path = 'models/music_vae_encoder_keras'\n",
    "# We can load the Keras model using the modern API\n",
    "standalone_encoder, encoder_graph, encoder_sess =load_standalone_encoder(keras_model_path)\n",
    "#standalone_encoder = tf.keras.models.load_model(keras_model_path)\n",
    "\n",
    "print(\"New Keras model loaded.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. INFERENCE & COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Step 3: Generating a random input tensor ---\")\n",
    "seq_len = mel_2bar_config.hparams.max_seq_len\n",
    "input_depth = mel_2bar_config.data_converter.input_depth\n",
    "control_depth = mel_2bar_config.data_converter.control_depth # This will be 0\n",
    "input_shape = (BATCH_SIZE, seq_len, input_depth)\n",
    "\n",
    "random_input = np.random.rand(*input_shape).astype(np.float32)\n",
    "print(f\"Generated random input with shape: {random_input.shape}\")\n",
    "\n",
    "print(\"\\n--- Step 4: Running inference on both models ---\")\n",
    "\n",
    "# --- Get embedding from the ORIGINAL TF1 model ---\n",
    "\n",
    "# ** THE FIX IS HERE **\n",
    "# Create an empty array for the `_controls` placeholder\n",
    "empty_controls = np.zeros((BATCH_SIZE, seq_len, control_depth), dtype=np.float32)\n",
    "print(f\"Generated empty controls with shape: {empty_controls.shape}\")\n",
    "\n",
    "# Add the empty controls to the feed_dict\n",
    "feed_dict = {\n",
    "    mel_2bar._inputs: random_input,\n",
    "    mel_2bar._inputs_length: [seq_len] * BATCH_SIZE,\n",
    "    mel_2bar._controls: empty_controls # Add the required empty placeholder value\n",
    "}\n",
    "# `_mu` is the tensor that holds the embedding\n",
    "original_embedding = sess.run(mel_2bar._mu, feed_dict)\n",
    "\n",
    "\n",
    "# --- Get embedding from the NEW Keras model ---\n",
    "\n",
    "with encoder_graph.as_default():\n",
    "    with encoder_sess.as_default():\n",
    "        keras_embedding = standalone_encoder.predict(random_input)\n",
    "\n",
    "#keras_embedding = standalone_encoder.predict(random_input)\n",
    "# ==============================================================================\n",
    "# 3. VERIFICATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- Step 5: Comparing the outputs ---\")\n",
    "print(f\"Original model's embedding (first 5 values): {original_embedding[0, :5]}\")\n",
    "print(f\"New Keras model's embedding (first 5 values):  {keras_embedding[0, :5]}\")\n",
    "\n",
    "are_close = np.allclose(original_embedding, keras_embedding, atol=1e-6)\n",
    "\n",
    "print(\"\\n--- VERIFICATION RESULT ---\")\n",
    "if are_close:\n",
    "    print(\"✅ SUCCESS: The embeddings from both models are identical!\")\n",
    "    abs_diff = np.mean(np.abs(original_embedding - keras_embedding))\n",
    "    print(f\"   (Mean absolute difference: {abs_diff:.10f})\")\n",
    "else:\n",
    "    print(\"❌ FAILURE: The embeddings do not match.\")\n",
    "    abs_diff = np.mean(np.abs(original_embedding - keras_embedding))\n",
    "    print(f\"   (Mean absolute difference: {abs_diff})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Ensure we are in a TF1-compatible environment\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# --- Configuration ---\n",
    "SAVED_ENCODER_PATH = 'models/music_vae_encoder_keras'\n",
    "# Assume `mel_2bar` is your already-loaded original model object\n",
    "\n",
    "# ======================================================================\n",
    "#  1. SETUP: Prepare both models in their isolated sessions\n",
    "# ======================================================================\n",
    "\n",
    "# --- Model A: Original TF1 Model ---\n",
    "sess_a = mel_2bar._sess\n",
    "graph_a = sess_a.graph\n",
    "print(f\"Model A (Original) is ready in Session: {sess_a}\")\n",
    "\n",
    "# --- Model B: Loaded Keras Standalone Encoder ---\n",
    "# Should already be loaded from above\n",
    "#standalone_encoder, encoder_graph, encoder_sess = load_standalone_encoder(SAVED_ENCODER_PATH)\n",
    "#print(f\"Model B (Keras) is ready in Session: {encoder_sess}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#  2. DEFINE TARGETS: Identify the tensors to compare\n",
    "# ======================================================================\n",
    "\n",
    "# --- Tensor names from Model A (TF1) ---\n",
    "# (These are the names we found in previous steps)\n",
    "TF1_INPUT_SEQ = \"Placeholder_2:0\"\n",
    "TF1_INPUT_LEN = \"Placeholder_1:0\"\n",
    "TF1_LSTM_OUTPUT = \"encoder/cell_0/concat:0\"\n",
    "TF1_FINAL_STATE = \"concat_2:0\"\n",
    "TF1_MU_OUTPUT = \"encoder/mu/BiasAdd:0\"\n",
    "\n",
    "# --- Corresponding layers from Model B (Keras) ---\n",
    "# (Use the names you defined in your Keras model. Use `standalone_encoder.summary()` to find them)\n",
    "KERAS_LSTM_LAYER_NAME = 'cell_0' # The Bidirectional wrapper layer\n",
    "KERAS_CONCAT_LAYER_NAME = 'final_state_concat' # The layer that concatenates the final states\n",
    "KERAS_MU_LAYER_NAME = 'mu' # The final dense layer for mu\n",
    "\n",
    "# ======================================================================\n",
    "#  3. EXECUTE AND COMPARE: Run the layer-by-layer check\n",
    "# ======================================================================\n",
    "\n",
    "\n",
    "\n",
    "# --- Check 1: Bidirectional LSTM Sequence Output ---\n",
    "print(\"\\n--- Comparing Layer 1: Bi-LSTM Sequence Output ---\")\n",
    "\n",
    "# Fetch from Model A\n",
    "with graph_a.as_default():\n",
    "    tf1_lstm_out_val = sess_a.run(\n",
    "        graph_a.get_tensor_by_name(TF1_LSTM_OUTPUT),\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "\n",
    "# Fetch from Model B\n",
    "with encoder_graph.as_default():\n",
    "    # The Bi-LSTM layer returns multiple tensors; the sequence output is the first one ([0])\n",
    "    keras_lstm_output_tensor = standalone_encoder.get_layer(KERAS_LSTM_LAYER_NAME).output[0]\n",
    "    keras_lstm_out_val = encoder_sess.run(\n",
    "        keras_lstm_output_tensor,\n",
    "        feed_dict={\n",
    "            input_tensor: random_input\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Compare\n",
    "try:\n",
    "    np.testing.assert_allclose(tf1_lstm_out_val, keras_lstm_out_val, rtol=1e-5, atol=1e-5)\n",
    "    print(\"✅ SUCCESS: Bi-LSTM outputs match.\")\n",
    "except AssertionError:\n",
    "    print(\"❌ FAILURE: Bi-LSTM outputs DO NOT match. The problem is in your Bidirectional/LSTM layer configuration.\")\n",
    "    # If this fails, there's no need to check further layers.\n",
    "    # The issue could be: different number of units, activation functions, dropout settings, etc.\n",
    "\n",
    "\n",
    "# --- Check 2: Final Concatenated State ---\n",
    "# (Only run this if the previous check passed)\n",
    "print(\"\\n--- Comparing Layer 2: Final Concatenated State ---\")\n",
    "\n",
    "# Fetch from Model A\n",
    "with graph_a.as_default():\n",
    "    tf1_state_val = sess_a.run(\n",
    "        graph_a.get_tensor_by_name(TF1_FINAL_STATE),\n",
    "        feed_dict={\n",
    "            TF1_INPUT_SEQ: input_sequence,\n",
    "            TF1_INPUT_LEN: sequence_length\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Fetch from Model B\n",
    "with encoder_graph.as_default():\n",
    "    keras_state_tensor = standalone_encoder.get_layer(KERAS_CONCAT_LAYER_NAME).output\n",
    "    keras_state_val = encoder_sess.run(\n",
    "        keras_state_tensor,\n",
    "        feed_dict={\n",
    "            standalone_encoder.input: input_sequence\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Compare\n",
    "try:\n",
    "    np.testing.assert_allclose(tf1_state_val, keras_state_val, rtol=1e-5, atol=1e-5)\n",
    "    print(\"✅ SUCCESS: Final states match.\")\n",
    "except AssertionError:\n",
    "    print(\"❌ FAILURE: Final states DO NOT match. The problem is how you extract and concatenate the LSTM states.\")\n",
    "\n",
    "\n",
    "# --- Check 3: Final `mu` Embedding ---\n",
    "# (Only run this if the previous checks passed)\n",
    "print(\"\\n--- Comparing Layer 3: Final `mu` Embedding ---\")\n",
    "\n",
    "# Fetch from Model A\n",
    "with graph_a.as_default():\n",
    "    tf1_mu_val = sess_a.run(\n",
    "        graph_a.get_tensor_by_name(TF1_MU_OUTPUT),\n",
    "        feed_dict={\n",
    "            TF1_INPUT_SEQ: input_sequence,\n",
    "            TF1_INPUT_LEN: sequence_length\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Fetch from Model B\n",
    "with encoder_graph.as_default():\n",
    "    keras_mu_tensor = standalone_encoder.get_layer(KERAS_MU_LAYER_NAME).output\n",
    "    keras_mu_val = encoder_sess.run(\n",
    "        keras_mu_tensor,\n",
    "        feed_dict={\n",
    "            standalone_encoder.input: input_sequence\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Compare\n",
    "try:\n",
    "    np.testing.assert_allclose(tf1_mu_val, keras_mu_val, rtol=1e-5, atol=1e-5)\n",
    "    print(\"✅ SUCCESS: The final `mu` embeddings match!\")\n",
    "except AssertionError:\n",
    "    print(\"❌ FAILURE: The final `mu` embeddings DO NOT match. The problem is in the final Dense layer.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
