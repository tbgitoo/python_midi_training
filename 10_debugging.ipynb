{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538ee65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Keras model from: models/music_vae_decoder_keras\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Keras model loaded successfully.\n",
      "Located 'generate' signature.\n",
      "Signature inputs: ['z']\n",
      "Signature outputs: ['output_0']\n",
      "\n",
      "--- Debugging First LSTM Cell (Manual vs. Keras) ---\n",
      "Distance on hidden state (h_1): 0.0000000000\n",
      "Distance on cell state (c_1):   0.0000000000\n",
      "\n",
      "SUCCESS: The first LSTM cell's forward pass matches the manual calculation.\n",
      "This means the weights for cell_0 are loaded correctly and the problem is likely in a subsequent cell (cell_1) or the final output_projection.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# --- IMPORTANT: Set this to your actual checkpoint path ---\n",
    "BASE_DIR = \"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "CHECKPOINT_PATH = BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def debug_first_lstm_cell(decoder_model, z, first_step_input):\n",
    "    \"\"\"\n",
    "    Performs a manual forward pass for the first LSTM cell at the first timestep\n",
    "    and compares its output to the Keras layer's output.\n",
    "\n",
    "    Args:\n",
    "        decoder_model: Your loaded Keras decoder model.\n",
    "        z: The latent vector, shape [1, latent_dim].\n",
    "        first_step_input: The first element of the teacher sequence, shape [1, 1, output_depth].\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Debugging First LSTM Cell (Manual vs. Keras) ---\")\n",
    "\n",
    "    # 1. Get the initial state for the first cell from the Keras model\n",
    "    # This tests your _get_initial_state logic implicitly.\n",
    "    initial_state_list = decoder_model._get_initial_state(z)\n",
    "    h_0, c_0 = initial_state_list[0]  # State for the first layer\n",
    "\n",
    "    # 2. Prepare the single-step input for the first cell\n",
    "    # This is `concat([input_step_0, z])`\n",
    "    z_repeated_step = tf.tile(tf.expand_dims(z, 1), [1, 1, 1])\n",
    "    rnn_input_step = tf.concat([first_step_input, z_repeated_step], axis=-1)\n",
    "    rnn_input_step = tf.squeeze(rnn_input_step, axis=1)  # Shape: [batch, features]\n",
    "\n",
    "    # 3. Get the output from the Keras LSTMCell layer directly\n",
    "    first_cell = decoder_model.lstm_cells[0]\n",
    "    keras_output, (keras_h_1, keras_c_1) = first_cell(rnn_input_step, states=[h_0, c_0])\n",
    "\n",
    "    # 4. Perform the same calculation MANUALLY using the cell's loaded weights\n",
    "    weights = first_cell.get_weights()\n",
    "    if len(weights) != 3:\n",
    "        print(\"ERROR: First LSTM cell does not have 3 weights (kernel, recurrent_kernel, bias).\")\n",
    "        return\n",
    "\n",
    "    kernel, recurrent_kernel, bias = weights\n",
    "\n",
    "    # Manual LSTM math: gates = (x @ W) + (h @ U) + b\n",
    "    gate_inputs = tf.matmul(rnn_input_step, kernel)\n",
    "    gate_recurrent = tf.matmul(h_0, recurrent_kernel)\n",
    "    gates = gate_inputs + gate_recurrent + bias\n",
    "\n",
    "    # Split into i, f, c, o gates (assuming Keras [i, f, c, o] order)\n",
    "    i, f, c_tilde, o = tf.split(gates, 4, axis=-1)\n",
    "\n",
    "    # Apply activations\n",
    "    i = tf.sigmoid(i)\n",
    "    f = tf.sigmoid(f)\n",
    "    c_tilde = tf.tanh(c_tilde)\n",
    "    o = tf.sigmoid(o)\n",
    "\n",
    "    # Calculate new cell state and hidden state\n",
    "    manual_c_1 = f * c_0 + i * c_tilde\n",
    "    manual_h_1 = o * tf.tanh(manual_c_1)\n",
    "\n",
    "    # 5. Compare the results\n",
    "    h_dist = tf.reduce_mean(tf.square(keras_h_1 - manual_h_1))\n",
    "    c_dist = tf.reduce_mean(tf.square(keras_c_1 - manual_c_1))\n",
    "\n",
    "    print(f\"Distance on hidden state (h_1): {h_dist.numpy():.10f}\")\n",
    "    print(f\"Distance on cell state (c_1):   {c_dist.numpy():.10f}\")\n",
    "\n",
    "    if h_dist < 1e-9 and c_dist < 1e-9:\n",
    "        print(\"\\nSUCCESS: The first LSTM cell's forward pass matches the manual calculation.\")\n",
    "        print(\"This means the weights for cell_0 are loaded correctly and the problem is likely in a subsequent cell (cell_1) or the final output_projection.\")\n",
    "    else:\n",
    "        print(\"\\nFAILURE: Mismatch in the first LSTM cell's calculation.\")\n",
    "        print(\"This strongly suggests the weight loading logic for the LSTM (gate order, forget bias, or kernel split) is still incorrect.\")\n",
    "\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model_def.decoder import MusicVAEDecoder, AutoregressiveStep\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_DIR = \"models/music_vae_decoder_keras\"\n",
    "LATENT_DIM = 512\n",
    "SEQUENCE_LENGTH = 32 # The sequence length you used when saving the model\n",
    "\n",
    "# --- 1. Load the Keras SavedModel ---\n",
    "print(f\"Loading Keras model from: {MODEL_DIR}\")\n",
    "# Create the dictionary of custom objects\n",
    "custom_objects = {\n",
    "    \"MusicVAEDecoder\": MusicVAEDecoder,\n",
    "    \"AutoregressiveStep\": AutoregressiveStep\n",
    "}\n",
    "\n",
    "\n",
    "# This loads the entire model, including the architecture and the traced signatures.\n",
    "loaded_keras_model = tf.keras.models.load_model(MODEL_DIR,custom_objects=custom_objects)\n",
    "print(\"Keras model loaded successfully.\")\n",
    "\n",
    "# --- 2. Access the 'generate' signature ---\n",
    "# The signatures dictionary holds the pre-traced functions we saved.\n",
    "generate_signature = loaded_keras_model.signatures['generate']\n",
    "print(\"Located 'generate' signature.\")\n",
    "print(\"Signature inputs:\", list(generate_signature.structured_input_signature[1].keys()))\n",
    "print(\"Signature outputs:\", list(generate_signature.structured_outputs.keys()))\n",
    "\n",
    "\n",
    "# Assuming `my_decoder` is your model instance after calling load_magenta_weights\n",
    "\n",
    "# Create a deterministic, non-zero input for testing\n",
    "z_sample = tf.random.normal([1, loaded_keras_model.latent_dim], seed=42)\n",
    "teacher_input_sample = tf.random.uniform([1, loaded_keras_model.sequence_length, loaded_keras_model.output_depth], seed=42)\n",
    "\n",
    "# Extract the very first step of the teacher input\n",
    "first_step = teacher_input_sample[:, :1, :] \n",
    "\n",
    "# Run the debugging function\n",
    "debug_first_lstm_cell(loaded_keras_model, z_sample, first_step)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87628fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging Second LSTM Cell (Manual vs. Keras) ---\n",
      "Distance on hidden state (h_1) for cell_1: 0.0000000000\n",
      "Distance on cell state (c_1) for cell_1:   0.0000000000\n",
      "\n",
      "SUCCESS: The second LSTM cell's forward pass matches the manual calculation.\n",
      "This means weights for cell_1 are also correct. The problem is likely in the final cell or the output_projection layer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def debug_second_lstm_cell(decoder_model, z, first_step_input):\n",
    "    \"\"\"\n",
    "    Performs a manual forward pass for the SECOND LSTM cell at the first timestep\n",
    "    and compares its output to the Keras layer's output.\n",
    "    This function assumes the first cell has already been verified.\n",
    "\n",
    "    Args:\n",
    "        decoder_model: Your loaded Keras decoder model.\n",
    "        z: The latent vector, shape [1, latent_dim].\n",
    "        first_step_input: The first element of the teacher sequence, shape [1, 1, output_depth].\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Debugging Second LSTM Cell (Manual vs. Keras) ---\")\n",
    "\n",
    "    # 1. Get initial states for the first two cells.\n",
    "    initial_state_list = decoder_model._get_initial_state(z)\n",
    "    if len(initial_state_list) < 2:\n",
    "        print(\"ERROR: Model does not have at least two LSTM layers.\")\n",
    "        return\n",
    "    h0_cell0, c0_cell0 = initial_state_list[0]\n",
    "    h0_cell1, c0_cell1 = initial_state_list[1]\n",
    "\n",
    "    # 2. Calculate the output of the first cell (which is the input to the second).\n",
    "    # We trust this calculation now, so we just use the Keras layer.\n",
    "    first_cell = decoder_model.lstm_cells[0]\n",
    "    z_repeated_step = tf.tile(tf.expand_dims(z, 1), [1, 1, 1])\n",
    "    rnn_input_step_cell0 = tf.concat([first_step_input, z_repeated_step], axis=-1)\n",
    "    rnn_input_step_cell0 = tf.squeeze(rnn_input_step_cell0, axis=1)\n",
    "    \n",
    "    # This is the input for the second cell.\n",
    "    input_for_cell1, _ = first_cell(rnn_input_step_cell0, states=[h0_cell0, c0_cell0])\n",
    "\n",
    "    # 3. Get the output from the Keras LSTMCell layer for cell_1.\n",
    "    second_cell = decoder_model.lstm_cells[1]\n",
    "    keras_output_cell1, (keras_h1_cell1, keras_c1_cell1) = second_cell(input_for_cell1, states=[h0_cell1, c0_cell1])\n",
    "\n",
    "    # 4. Perform the same calculation MANUALLY for cell_1.\n",
    "    weights = second_cell.get_weights()\n",
    "    if len(weights) != 3:\n",
    "        print(\"ERROR: Second LSTM cell does not have 3 weights (kernel, recurrent_kernel, bias).\")\n",
    "        return\n",
    "\n",
    "    kernel, recurrent_kernel, bias = weights\n",
    "\n",
    "    # Manual LSTM math: gates = (x @ W) + (h @ U) + b\n",
    "    gate_inputs = tf.matmul(input_for_cell1, kernel)\n",
    "    gate_recurrent = tf.matmul(h0_cell1, recurrent_kernel)\n",
    "    gates = gate_inputs + gate_recurrent + bias\n",
    "\n",
    "    # Split into i, f, c, o gates (assuming Keras [i, f, c, o] order)\n",
    "    i, f, c_tilde, o = tf.split(gates, 4, axis=-1)\n",
    "\n",
    "    # Apply activations\n",
    "    i, f, o = tf.sigmoid(i), tf.sigmoid(f), tf.sigmoid(o)\n",
    "    c_tilde = tf.tanh(c_tilde)\n",
    "\n",
    "    # Calculate new cell state and hidden state\n",
    "    manual_c1_cell1 = f * c0_cell1 + i * c_tilde\n",
    "    manual_h1_cell1 = o * tf.tanh(manual_c1_cell1)\n",
    "\n",
    "    # 5. Compare the results\n",
    "    h_dist = tf.reduce_mean(tf.square(keras_h1_cell1 - manual_h1_cell1))\n",
    "    c_dist = tf.reduce_mean(tf.square(keras_c1_cell1 - manual_c1_cell1))\n",
    "\n",
    "    print(f\"Distance on hidden state (h_1) for cell_1: {h_dist.numpy():.10f}\")\n",
    "    print(f\"Distance on cell state (c_1) for cell_1:   {c_dist.numpy():.10f}\")\n",
    "\n",
    "    if h_dist.numpy() < 1e-9 and c_dist.numpy() < 1e-9:\n",
    "        print(\"\\nSUCCESS: The second LSTM cell's forward pass matches the manual calculation.\")\n",
    "        print(\"This means weights for cell_1 are also correct. The problem is likely in the final cell or the output_projection layer.\")\n",
    "    else:\n",
    "        print(\"\\nFAILURE: Mismatch in the second LSTM cell's calculation.\")\n",
    "        print(\"This suggests the weight loading logic for cell_1 is incorrect, specifically the `input_dim` used for the kernel split.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- In your main execution block ---\n",
    "\n",
    "# ... (instantiate model, load weights, create sample inputs) ...\n",
    "\n",
    "# Run the first debug function (which we know succeeds)\n",
    "# debug_first_lstm_cell(my_decoder, z_sample, first_step)\n",
    "\n",
    "# Now, run the new debug function for the second cell\n",
    "debug_second_lstm_cell(loaded_keras_model, z_sample, first_step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
