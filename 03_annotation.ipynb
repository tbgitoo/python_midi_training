{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9847eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of 'data_sets\\lmd_matched_h5\\A\\A\\A\\TRAAAGR128F425B14B.h5':\n",
      "- Group: analysis\n",
      "  - Dataset: bars_confidence (Shape: (123,), Dtype: float64)\n",
      "  - Dataset: bars_start (Shape: (123,), Dtype: float64)\n",
      "  - Dataset: beats_confidence (Shape: (497,), Dtype: float64)\n",
      "  - Dataset: beats_start (Shape: (497,), Dtype: float64)\n",
      "  - Dataset: sections_confidence (Shape: (8,), Dtype: float64)\n",
      "  - Dataset: sections_start (Shape: (8,), Dtype: float64)\n",
      "  - Dataset: segments_confidence (Shape: (940,), Dtype: float64)\n",
      "  - Dataset: segments_loudness_max (Shape: (940,), Dtype: float64)\n",
      "  - Dataset: segments_loudness_max_time (Shape: (940,), Dtype: float64)\n",
      "  - Dataset: segments_loudness_start (Shape: (940,), Dtype: float64)\n",
      "  - Dataset: segments_pitches (Shape: (940, 12), Dtype: float64)\n",
      "  - Dataset: segments_start (Shape: (940,), Dtype: float64)\n",
      "  - Dataset: segments_timbre (Shape: (940, 12), Dtype: float64)\n",
      "  - Dataset: songs (Shape: (1,), Dtype: [('analysis_sample_rate', '<i4'), ('audio_md5', 'S32'), ('danceability', '<f8'), ('duration', '<f8'), ('end_of_fade_in', '<f8'), ('energy', '<f8'), ('idx_bars_confidence', '<i4'), ('idx_bars_start', '<i4'), ('idx_beats_confidence', '<i4'), ('idx_beats_start', '<i4'), ('idx_sections_confidence', '<i4'), ('idx_sections_start', '<i4'), ('idx_segments_confidence', '<i4'), ('idx_segments_loudness_max', '<i4'), ('idx_segments_loudness_max_time', '<i4'), ('idx_segments_loudness_start', '<i4'), ('idx_segments_pitches', '<i4'), ('idx_segments_start', '<i4'), ('idx_segments_timbre', '<i4'), ('idx_tatums_confidence', '<i4'), ('idx_tatums_start', '<i4'), ('key', '<i4'), ('key_confidence', '<f8'), ('loudness', '<f8'), ('mode', '<i4'), ('mode_confidence', '<f8'), ('start_of_fade_out', '<f8'), ('tempo', '<f8'), ('time_signature', '<i4'), ('time_signature_confidence', '<f8'), ('track_id', 'S32')])\n",
      "  - Dataset: tatums_confidence (Shape: (994,), Dtype: float64)\n",
      "  - Dataset: tatums_start (Shape: (994,), Dtype: float64)\n",
      "- Group: metadata\n",
      "  - Dataset: artist_terms (Shape: (19,), Dtype: |S256)\n",
      "  - Dataset: artist_terms_freq (Shape: (19,), Dtype: float64)\n",
      "  - Dataset: artist_terms_weight (Shape: (19,), Dtype: float64)\n",
      "  - Dataset: similar_artists (Shape: (100,), Dtype: |S20)\n",
      "  - Dataset: songs (Shape: (1,), Dtype: [('analyzer_version', 'S32'), ('artist_7digitalid', '<i4'), ('artist_familiarity', '<f8'), ('artist_hotttnesss', '<f8'), ('artist_id', 'S32'), ('artist_latitude', '<f8'), ('artist_location', 'S1024'), ('artist_longitude', '<f8'), ('artist_mbid', 'S40'), ('artist_name', 'S1024'), ('artist_playmeid', '<i4'), ('genre', 'S1024'), ('idx_artist_terms', '<i4'), ('idx_similar_artists', '<i4'), ('release', 'S1024'), ('release_7digitalid', '<i4'), ('song_hotttnesss', '<f8'), ('song_id', 'S32'), ('title', 'S1024'), ('track_7digitalid', '<i4')])\n",
      "- Group: musicbrainz\n",
      "  - Dataset: artist_mbtags (Shape: (1,), Dtype: |S256)\n",
      "    Content: ['classic pop and rock']\n",
      "  - Dataset: artist_mbtags_count (Shape: (1,), Dtype: int32)\n",
      "  - Dataset: songs (Shape: (1,), Dtype: [('idx_artist_mbtags', '<i4'), ('year', '<i4')])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def list_h5_contents(h5_file_path):\n",
    "    \"\"\"\n",
    "    Generically lists the contents of an HDF5 file, including groups, datasets,\n",
    "    and the content of small, text-based datasets.\n",
    "    Args:\n",
    "        h5_file_path (str): The path to the .h5 file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(h5_file_path):\n",
    "        print(f\"Error: File not found at '{h5_file_path}'\")\n",
    "        return\n",
    "    try:\n",
    "        with h5py.File(h5_file_path, 'r') as h5_file:\n",
    "            print(f\"Contents of '{h5_file_path}':\")\n",
    "            \n",
    "            def print_structure(name, obj):\n",
    "                \"\"\"Callback function to print the structure of the HDF5 file.\"\"\"\n",
    "                indent = '  ' * name.count('/')\n",
    "                if isinstance(obj, h5py.Dataset):\n",
    "                    print(f\"{indent}- Dataset: {os.path.basename(name)} (Shape: {obj.shape}, Dtype: {obj.dtype})\")\n",
    "                    \n",
    "                    # --- Check if the dataset contains short text ---\n",
    "                    # Heuristic: Check if dtype is string-like and if it's a scalar or small array.\n",
    "                    is_string_like = h5py.check_string_dtype(obj.dtype) is not None\n",
    "                    is_small = obj.size < 10 and obj.ndim <= 1\n",
    "\n",
    "                    if is_string_like and is_small:\n",
    "                        try:\n",
    "                            # Read the data from the dataset\n",
    "                            data = obj[()]\n",
    "                            # Decode if it's in bytes (common in HDF5)\n",
    "                            if isinstance(data, bytes):\n",
    "                                value = data.decode('utf-8', 'ignore')\n",
    "                            elif isinstance(data, np.ndarray) and data.size > 0:\n",
    "                                # Handle array of strings/bytes\n",
    "                                value = [d.decode('utf-8', 'ignore') if isinstance(d, bytes) else str(d) for d in data]\n",
    "                            else:\n",
    "                                value = str(data)\n",
    "                            print(f\"{indent}  Content: {value}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"{indent}  (Could not read content: {e})\")\n",
    "\n",
    "                elif isinstance(obj, h5py.Group):\n",
    "                    print(f\"{indent}- Group: {os.path.basename(name) or '/'}\")\n",
    "\n",
    "            h5_file.visititems(print_structure)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the H5 file: {e}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "# NOTE: You must first extract the 'lmd_matched_h5.tar.gz' archive.\n",
    "# The path should point to an actual .h5 file on your disk.\n",
    "\n",
    "# Create a placeholder for the path to the H5 dataset directory\n",
    "LMD_H5_DIR = os.path.join('data_sets', 'lmd_matched_h5')\n",
    "example_h5_file = os.path.join(LMD_H5_DIR, 'A', 'A', 'A', 'TRAAAGR128F425B14B.h5')\n",
    "\n",
    "# Check if the example file exists before trying to read it\n",
    "if os.path.exists(example_h5_file):\n",
    "    list_h5_contents(example_h5_file)\n",
    "else:\n",
    "    print(\"Example H5 file not found. Please ensure 'lmd_matched_h5.tar.gz' is extracted\")\n",
    "    print(f\"and the file exists at: {example_h5_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c849d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Million Song Dataset metadata database already exists at: data_sets\\track_metadata.db\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# --- Download the Million Song Dataset Metadata Database ---\n",
    "\n",
    "MSD_METADATA_URL = 'http://millionsongdataset.com/sites/default/files/AdditionalFiles/track_metadata.db'\n",
    "DB_DIR = 'data_sets'\n",
    "DB_PATH = os.path.join(DB_DIR, 'track_metadata.db')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(DB_DIR, exist_ok=True)\n",
    "\n",
    "# Check if the database file already exists\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(f\"Downloading Million Song Dataset metadata from: {MSD_METADATA_URL}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(MSD_METADATA_URL, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Write the content directly to the database file\n",
    "        with open(DB_PATH, 'wb') as f:\n",
    "            print(f\"Saving 'track_metadata.db' to '{DB_PATH}'...\")\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        if os.path.exists(DB_PATH):\n",
    "            print(\"Successfully downloaded 'track_metadata.db'.\")\n",
    "        else:\n",
    "            print(\"Error: Download failed, 'track_metadata.db' not found.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: Could not download the metadata database. An error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(f\"Million Song Dataset metadata database already exists at: {DB_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6052b91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new annotations database at 'data_sets\\midi_annotations.db'...\n",
      "Creating 'annotations' table with indexes...\n",
      "Extracting data from source database...\n",
      "Data extraction and insertion complete.\n",
      "Successfully created 'data_sets\\midi_annotations.db' with 1000000 records.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_annotations_database(source_db_path, new_db_path):\n",
    "    \"\"\"\n",
    "    Extracts relevant song data from the MSD database into a new, indexed database.\n",
    "\n",
    "    Args:\n",
    "        source_db_path (str): Path to the source 'track_metadata.db'.\n",
    "        new_db_path (str): Path for the new SQLite database to be created.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_db_path):\n",
    "        print(f\"Error: Source database not found at '{source_db_path}'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Creating new annotations database at '{new_db_path}'...\")\n",
    "\n",
    "    # Connect to both databases\n",
    "    source_conn = None\n",
    "    new_conn = None\n",
    "    try:\n",
    "        source_conn = sqlite3.connect(source_db_path)\n",
    "        new_conn = sqlite3.connect(new_db_path)\n",
    "        \n",
    "        source_cursor = source_conn.cursor()\n",
    "        new_cursor = new_conn.cursor()\n",
    "\n",
    "        # 1. Create the new table\n",
    "        print(\"Creating 'annotations' table with indexes...\")\n",
    "        new_cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS annotations (\n",
    "                track_id TEXT PRIMARY KEY,\n",
    "                title TEXT NOT NULL,\n",
    "                artist_name TEXT NOT NULL,\n",
    "                year INTEGER\n",
    "            )\n",
    "        ''')\n",
    "\n",
    "        # 2. Create indexes for efficient searching\n",
    "        new_cursor.execute('CREATE INDEX IF NOT EXISTS idx_title ON annotations (title)')\n",
    "        new_cursor.execute('CREATE INDEX IF NOT EXISTS idx_artist_name ON annotations (artist_name)')\n",
    "        new_cursor.execute('CREATE INDEX IF NOT EXISTS idx_artist_title ON annotations (artist_name, title)')\n",
    "        \n",
    "        # 3. Extract data from the source table\n",
    "        print(\"Extracting data from source database...\")\n",
    "        source_cursor.execute('SELECT track_id, title, artist_name, year FROM songs')\n",
    "\n",
    "        # 4. Insert data into the new table in batches for efficiency\n",
    "        batch_size = 10000\n",
    "        while True:\n",
    "            rows = source_cursor.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            new_cursor.executemany('''\n",
    "                INSERT OR IGNORE INTO annotations (track_id, title, artist_name, year)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', rows)\n",
    "            new_conn.commit()\n",
    "        \n",
    "        print(\"Data extraction and insertion complete.\")\n",
    "\n",
    "        # Verify the number of rows in the new table\n",
    "        count = new_cursor.execute('SELECT COUNT(*) FROM annotations').fetchone()[0]\n",
    "        print(f\"Successfully created '{new_db_path}' with {count} records.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if source_conn:\n",
    "            source_conn.close()\n",
    "        if new_conn:\n",
    "            new_conn.close()\n",
    "\n",
    "# --- Execution ---\n",
    "SOURCE_DB = os.path.join('data_sets', 'track_metadata.db')\n",
    "NEW_DB = os.path.join('data_sets', 'midi_annotations.db')\n",
    "\n",
    "# Run the function to create and populate the new database\n",
    "create_annotations_database(SOURCE_DB, NEW_DB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
