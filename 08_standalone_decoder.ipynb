{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTMCell, RNN, Dense\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# The MusicVAEDecoder class remains the same\n",
    "class MusicVAEDecoder(tf.keras.Model):\n",
    "    \"\"\"The decoder portion of the MusicVAE model.\"\"\"\n",
    "    def __init__(self, output_depth, lstm_units=2048, num_layers=2, name=\"decoder\"):\n",
    "        super(MusicVAEDecoder, self).__init__(name=name)\n",
    "        self.z_to_initial_state = Dense(lstm_units * num_layers * 2, name=\"z_to_initial_state\")\n",
    "        self.lstm_cells = [LSTMCell(lstm_units, name=f\"lstm_cell_{i}\") for i in range(num_layers)]\n",
    "        self.rnn = RNN(self.lstm_cells, return_sequences=True, return_state=True, name=\"decoder_rnn\")\n",
    "        self.output_projection = Dense(output_depth, name=\"output_projection\")\n",
    "\n",
    "    # In the MusicVAEDecoder class:\n",
    "\n",
    "    def call(self, z, sequence_length, inputs=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the decoder.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        num_layers = len(self.lstm_cells)\n",
    "        lstm_units = self.lstm_cells[0].units\n",
    "\n",
    "        # Project the latent vector 'z' to get the initial state for the LSTM.\n",
    "        # Shape: (batch_size, num_layers * 2 * lstm_units)\n",
    "        initial_state_flat = self.z_to_initial_state(z)\n",
    "\n",
    "        # Reshape to separate layers and the h/c states.\n",
    "        # Shape: (batch_size, num_layers, 2, lstm_units)\n",
    "        initial_state_reshaped = tf.reshape(\n",
    "            initial_state_flat, [batch_size, num_layers, 2, lstm_units]\n",
    "        )\n",
    "\n",
    "        # Transpose to group h/c states by layer.\n",
    "        # Shape: (num_layers, 2, batch_size, lstm_units)\n",
    "        initial_state_transposed = tf.transpose(initial_state_reshaped, [1, 2, 0, 3])\n",
    "\n",
    "        # Unstack to create the final list of states for each layer.\n",
    "        # This creates a list of `num_layers` elements.\n",
    "        # Each element is a tensor of shape (2, batch_size, lstm_units).\n",
    "        initial_state_list = tf.unstack(initial_state_transposed)\n",
    "\n",
    "        # Further unstack each layer's state into (h, c) tuples.\n",
    "        # The final structure is: [ (h0, c0), (h1, c1), ... ]\n",
    "        # which is what the Keras RNN layer expects.\n",
    "        initial_state = [tf.unstack(s) for s in initial_state_list]\n",
    "\n",
    "                # If no input sequence is provided, create the dummy one for generation.\n",
    "        # IMPORTANT: The dummy input must have the correct dimension.\n",
    "        if inputs is None:\n",
    "            # The input to the first LSTM is z + previous_output\n",
    "            # which has a dimension of 512 + 90 = 602\n",
    "            input_depth = 602 \n",
    "            inputs = tf.zeros([batch_size, sequence_length, input_depth])\n",
    "\n",
    "        # Run the RNN.\n",
    "        rnn_output, *_ = self.rnn(inputs, initial_state=initial_state)\n",
    "\n",
    "        # Project the RNN output to the final output space.\n",
    "        output = self.output_projection(rnn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_magenta_weights(decoder_model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads weights from a TF1 Magenta checkpoint into a TF2 Keras decoder model.\n",
    "    (Final corrected version)\n",
    "    \"\"\"\n",
    "    reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "\n",
    "    # --- 1. Load z_to_initial_state weights ---\n",
    "    z_kernel = reader.get_tensor(\"decoder/z_to_initial_state/kernel\")\n",
    "    z_bias = reader.get_tensor(\"decoder/z_to_initial_state/bias\")\n",
    "    decoder_model.z_to_initial_state.set_weights([z_kernel, z_bias])\n",
    "    print(\"Loaded weights for 'z_to_initial_state' layer.\")\n",
    "\n",
    "    # --- 2. Load LSTM cell weights ---\n",
    "    for i, cell in enumerate(decoder_model.lstm_cells):\n",
    "        tf1_kernel_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/kernel\"\n",
    "        tf1_bias_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/bias\"\n",
    "        \n",
    "        tf1_kernel = reader.get_tensor(tf1_kernel_name)\n",
    "        tf1_bias = reader.get_tensor(tf1_bias_name)\n",
    "\n",
    "        # THE FIX: Use the correct input dimension for splitting the kernel,\n",
    "        # based on the layer index.\n",
    "        if i == 0:\n",
    "            # The original model's first layer has a complex input of dim 602.\n",
    "            input_dim = 602\n",
    "        else:\n",
    "            # Subsequent layers take the output of the previous LSTM layer.\n",
    "            input_dim = cell.units # which is 2048\n",
    "\n",
    "        # Perform the split at the correct index.\n",
    "        keras_kernel = tf1_kernel[:input_dim, :]\n",
    "        keras_recurrent_kernel = tf1_kernel[input_dim:, :]\n",
    "        \n",
    "        # Now the shapes will match perfectly.\n",
    "        cell.set_weights([keras_kernel, keras_recurrent_kernel, tf1_bias])\n",
    "        print(f\"Loaded weights for LSTM cell {i} from '{tf1_kernel_name}'.\")\n",
    "\n",
    "    # --- 3. Load output_projection weights ---\n",
    "    out_kernel = reader.get_tensor(\"decoder/output_projection/kernel\")\n",
    "    out_bias = reader.get_tensor(\"decoder/output_projection/bias\")\n",
    "    decoder_model.output_projection.set_weights([out_kernel, out_bias])\n",
    "    print(\"Loaded weights for 'output_projection' layer.\")\n",
    "\n",
    "    print(\"\\nSuccessfully loaded all decoder weights from Magenta checkpoint!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "# --- Correct parameters for 'mel_2bar_big' ---\n",
    "LATENT_DIM = 512\n",
    "OUTPUT_DEPTH = 90\n",
    "SEQUENCE_LENGTH = 32 # The correct, fixed sequence length for this model\n",
    "LSTM_UNITS = 2048\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# --- Main execution ---\n",
    "    \n",
    "# 2. Instantiate our TF2 Keras decoder model with the correct number of layers.\n",
    "decoder = MusicVAEDecoder(\n",
    "    output_depth=OUTPUT_DEPTH,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    num_layers=NUM_LAYERS\n",
    ")\n",
    "\n",
    "# 3. Build the model by calling it once.\n",
    "print(\"\\nBuilding Keras model to initialize variables...\")\n",
    "dummy_z = tf.zeros([1, LATENT_DIM])\n",
    "# We must use the correct sequence length here to build the graph properly.\n",
    "decoder(dummy_z, sequence_length=SEQUENCE_LENGTH)\n",
    "print(\"Model built.\")\n",
    "\n",
    "# 4. Load the weights from the downloaded checkpoint.\n",
    "load_magenta_weights(decoder, CHECKPOINT_PATH)\n",
    "\n",
    "# 5. Verify the model can run inference with the correct dimensions.\n",
    "print(\"\\nRunning a test inference with correct parameters...\")\n",
    "batch_size = 1\n",
    "z = tf.random.normal([batch_size, LATENT_DIM])\n",
    "    \n",
    "# Generate a sequence of the correct length\n",
    "generated_sequence = decoder(z, sequence_length=SEQUENCE_LENGTH)\n",
    "    \n",
    "print(\"Shape of the generated sequence:\", generated_sequence.shape)\n",
    "print(\"Inference successful!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5739ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_checkpoint(checkpoint_path):\n",
    "    \"\"\"\n",
    "    A helper function to print all variable names and their shapes in a checkpoint.\n",
    "    This is extremely useful for debugging name-related errors.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Inspecting variables in checkpoint: {checkpoint_path} ---\")\n",
    "    try:\n",
    "        reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "        shape_map = reader.get_variable_to_shape_map()\n",
    "        for key in sorted(shape_map.keys()):\n",
    "            print(f\"Tensor name: {key}, shape: {shape_map[key]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read checkpoint: {e}\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "inspect_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
