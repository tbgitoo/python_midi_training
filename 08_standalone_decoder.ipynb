{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c973dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTMCell, RNN, Dense\n",
    "import os\n",
    "import requests\n",
    "\n",
    "LATENT_DIM = 512\n",
    "OUTPUT_DEPTH = 90\n",
    "SEQUENCE_LENGTH = 32 # The correct, fixed sequence length for this model\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Helper class for the specific step logic in autoregressive generation\n",
    "class AutoregressiveStep(tf.keras.layers.Layer):\n",
    "    def __init__(self, lstm_cells, output_projection_layer, **kwargs):\n",
    "        super(AutoregressiveStep, self).__init__(**kwargs)\n",
    "        self.lstm_cells = lstm_cells\n",
    "        self.output_projection = output_projection_layer\n",
    "        # The state size is the sum of the state sizes of all LSTM cells\n",
    "        self.state_size = [cell.state_size for cell in self.lstm_cells]\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # `inputs` is a tuple: (the input for this step, the constant z vector)\n",
    "        step_input, z = inputs\n",
    "        \n",
    "        # Concatenate the current step's input with z\n",
    "        step_input_with_z = tf.concat([step_input, z], axis=-1)\n",
    "\n",
    "        # Manually run the stack of LSTM cells for a single step\n",
    "        cell_input = step_input_with_z\n",
    "        new_states = []\n",
    "        for i, cell in enumerate(self.lstm_cells):\n",
    "            cell_output, (new_h, new_c) = cell(cell_input, states=states[i])\n",
    "            new_states.append([new_h, new_c])\n",
    "            cell_input = cell_output\n",
    "        \n",
    "        final_cell_output = cell_output\n",
    "        \n",
    "        # Project to get the logits for this single step\n",
    "        step_logits = self.output_projection(final_cell_output)\n",
    "        \n",
    "        # The output of this layer is the logits for the next step\n",
    "        # The new state is passed internally by the RNN wrapper\n",
    "        return step_logits, new_states\n",
    "\n",
    "\n",
    "\n",
    "# The MusicVAEDecoder class remains the same\n",
    "class MusicVAEDecoder(tf.keras.Model):\n",
    "    \"\"\"The decoder portion of the MusicVAE model.\"\"\"\n",
    "    def __init__(self, output_depth, lstm_units=2048, num_layers=2, name=\"decoder\",sequence_length=32):\n",
    "        super(MusicVAEDecoder, self).__init__(name=name)\n",
    "        self.z_to_initial_state = Dense(lstm_units * num_layers * 2, name=\"z_to_initial_state\")\n",
    "        self.lstm_cells = [LSTMCell(lstm_units, name=f\"lstm_cell_{i}\") for i in range(num_layers)]\n",
    "        self.rnn = RNN(self.lstm_cells, return_sequences=True, return_state=True, name=\"decoder_rnn\")\n",
    "        self.output_projection = Dense(output_depth, name=\"output_projection\")\n",
    "        self.vocab_size = output_depth\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "        # --- THE FIX: Create the autoregressive step layer ---\n",
    "        autoregressive_step = AutoregressiveStep(self.lstm_cells, self.output_projection)\n",
    "        # --- The RNN wrapper for generation ---\n",
    "        self.generation_rnn = tf.keras.layers.RNN(autoregressive_step, return_sequences=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # --- 2. The \"Teaching\" Endpoint: Decorated for Training/Reconstruction ---\n",
    "    # This will be one of the functions available in your saved model.\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, LATENT_DIM], name=\"z\"),\n",
    "        tf.TensorSpec(shape=[BATCH_SIZE, None, OUTPUT_DEPTH], name=\"inputs\")\n",
    "    ])\n",
    "    def reconstruct(self, z, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the decoder.\n",
    "        \"\"\"\n",
    "        # 1. Get initial state from z\n",
    "        initial_state=self.get_initial_state(z)\n",
    "\n",
    "    \n",
    "        # 2. Prepare the latent vector for concatenation at each time step.\n",
    "        # We need to repeat `z` so it can be attached to every element of the sequence.\n",
    "        # Tile z from shape [batch, latent_dim] to [batch, sequence_length, latent_dim]\n",
    "        z_repeated = tf.tile(tf.expand_dims(z, 1), [1, self.sequence_length, 1])\n",
    "        \n",
    "        \n",
    "        # Concatenate z with the inputs along the feature dimension.\n",
    "        # `inputs` has shape [batch, sequence_length, 90]\n",
    "            # `z_repeated` has shape [batch, sequence_length, 512]\n",
    "            # The result will have shape [batch, sequence_length, 602]\n",
    "        rnn_inputs = tf.concat([inputs, z_repeated], axis=-1)\n",
    "\n",
    "         # Run the RNN.\n",
    "        rnn_output, *_ = self.rnn(rnn_inputs, initial_state=initial_state)\n",
    "\n",
    "        # Project the RNN output to the final output space.\n",
    "        output = self.output_projection(rnn_output)\n",
    "        return output\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, LATENT_DIM], name=\"z\")\n",
    "    ])\n",
    "    def generate(self, z):\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        \n",
    "        # 1. Get initial state from z\n",
    "        initial_state = self.get_initial_state(z) # Same initial state processing as for reconstruction\n",
    "        \n",
    "        # 2. Create the inputs for the RNN layer.\n",
    "        # The RNN layer needs a sequence to iterate over. We give it a dummy sequence.\n",
    "        # The actual input for each step will be constructed *inside* the AutoregressiveStep layer.\n",
    "        dummy_sequence = tf.zeros([batch_size, self.sequence_length, self.vocab_size])\n",
    "        \n",
    "        # We also need to pass `z` as a constant to each step.\n",
    "        z_repeated = tf.tile(tf.expand_dims(z, 1), [1, self.sequence_length, 1])\n",
    "        \n",
    "        # The RNN layer will unpack this tuple at each time step\n",
    "        rnn_inputs = (dummy_sequence, z_repeated)\n",
    "        \n",
    "        # 3. Call the generation RNN\n",
    "        # This is now a single, clean, graph-native call.\n",
    "        logits_sequence = self.generation_rnn(rnn_inputs, initial_state=initial_state)\n",
    "        \n",
    "        return logits_sequence\n",
    "    \n",
    "    def get_initial_state(self, z):\n",
    "\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        num_layers = len(self.lstm_cells)\n",
    "        lstm_units = self.lstm_cells[0].units\n",
    "\n",
    "        # Project the latent vector 'z' to get the initial state for the LSTM.\n",
    "        # Shape: (batch_size, num_layers * 2 * lstm_units)\n",
    "        initial_state_flat = self.z_to_initial_state(z)\n",
    "\n",
    "        # Reshape to separate layers and the h/c states.\n",
    "        # Shape: (batch_size, num_layers, 2, lstm_units)\n",
    "        initial_state_reshaped = tf.reshape(\n",
    "            initial_state_flat, [batch_size, num_layers, 2, lstm_units]\n",
    "        )\n",
    "\n",
    "        # Transpose to group h/c states by layer.\n",
    "        # Shape: (num_layers, 2, batch_size, lstm_units)\n",
    "        initial_state_transposed = tf.transpose(initial_state_reshaped, [1, 2, 0, 3])\n",
    "\n",
    "        # Unstack to create the final list of states for each layer.\n",
    "        # This creates a list of `num_layers` elements.\n",
    "        # Each element is a tensor of shape (2, batch_size, lstm_units).\n",
    "        initial_state_list = tf.unstack(initial_state_transposed)\n",
    "\n",
    "        # Further unstack each layer's state into (h, c) tuples.\n",
    "        # The final structure is: [ (h0, c0), (h1, c1), ... ]\n",
    "        # which is what the Keras RNN layer expects.\n",
    "        initial_state = [tf.unstack(s) for s in initial_state_list]\n",
    "\n",
    "        return(initial_state)\n",
    "    \n",
    "\n",
    "    # --- THE FIX 1: Make `generate` logic the primary `call` method ---\n",
    "    # This method is UNDECORATED.\n",
    "    def call(self, z):\n",
    "        \"\"\"\n",
    "        This is the primary forward pass, implementing autoregressive generation.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        initial_state = self.get_initial_state(z)\n",
    "        \n",
    "        # The RNN layer needs a dummy sequence to know how many steps to run.\n",
    "        dummy_sequence = tf.zeros([batch_size, self.sequence_length, self.vocab_size])\n",
    "        \n",
    "        # We also need to pass `z` as a constant to each step.\n",
    "        z_repeated = tf.tile(tf.expand_dims(z, 1), [1, self.sequence_length, 1])\n",
    "        \n",
    "        rnn_inputs = (dummy_sequence, z_repeated)\n",
    "        \n",
    "        logits_sequence = self.generation_rnn(rnn_inputs, initial_state=initial_state)\n",
    "        \n",
    "        return logits_sequence\n",
    "   \n",
    "def load_magenta_weights(decoder_model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads weights from a TF1 Magenta checkpoint into a TF2 Keras decoder model.\n",
    "    (Final corrected version)\n",
    "    \"\"\"\n",
    "    reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "\n",
    "    # --- 1. Load z_to_initial_state weights ---\n",
    "    z_kernel = reader.get_tensor(\"decoder/z_to_initial_state/kernel\")\n",
    "    z_bias = reader.get_tensor(\"decoder/z_to_initial_state/bias\")\n",
    "    decoder_model.z_to_initial_state.set_weights([z_kernel, z_bias])\n",
    "    print(\"Loaded weights for 'z_to_initial_state' layer.\")\n",
    "\n",
    "    # --- 2. Load LSTM cell weights ---\n",
    "    for i, cell in enumerate(decoder_model.lstm_cells):\n",
    "        tf1_kernel_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/kernel\"\n",
    "        tf1_bias_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/bias\"\n",
    "        \n",
    "        tf1_kernel = reader.get_tensor(tf1_kernel_name)\n",
    "        tf1_bias = reader.get_tensor(tf1_bias_name)\n",
    "\n",
    "        # THE FIX: Use the correct input dimension for splitting the kernel,\n",
    "        # based on the layer index.\n",
    "        if i == 0:\n",
    "            # The original model's first layer has a complex input of dim 602.\n",
    "            input_dim = 602\n",
    "        else:\n",
    "            # Subsequent layers take the output of the previous LSTM layer.\n",
    "            input_dim = cell.units # which is 2048\n",
    "\n",
    "        # Perform the split at the correct index.\n",
    "        keras_kernel = tf1_kernel[:input_dim, :]\n",
    "        keras_recurrent_kernel = tf1_kernel[input_dim:, :]\n",
    "        \n",
    "        # Now the shapes will match perfectly.\n",
    "        cell.set_weights([keras_kernel, keras_recurrent_kernel, tf1_bias])\n",
    "        print(f\"Loaded weights for LSTM cell {i} from '{tf1_kernel_name}'.\")\n",
    "\n",
    "    # --- 3. Load output_projection weights ---\n",
    "    out_kernel = reader.get_tensor(\"decoder/output_projection/kernel\")\n",
    "    out_bias = reader.get_tensor(\"decoder/output_projection/bias\")\n",
    "    decoder_model.output_projection.set_weights([out_kernel, out_bias])\n",
    "    print(\"Loaded weights for 'output_projection' layer.\")\n",
    "\n",
    "    print(\"\\nSuccessfully loaded all decoder weights from Magenta checkpoint!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26e3665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instance created.\n",
      "\n",
      "Manually building model layers...\n",
      "Built 'z_to_initial_state' layer.\n",
      "Built LSTM cell 0 with input dimension 602.\n",
      "Built LSTM cell 1 with input dimension 2048.\n",
      "Built LSTM cell 2 with input dimension 2048.\n",
      "Built 'output_projection' layer.\n",
      "\n",
      "All layers built successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Define constants ---\n",
    "LATENT_DIM = 512\n",
    "OUTPUT_DEPTH = 90 # This is your OUTPUT_DEPTH\n",
    "LSTM_UNITS = 2048\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# 2. Instantiate our TF2 Keras decoder model with the correct number of layers.\n",
    "decoder = MusicVAEDecoder(\n",
    "    output_depth=OUTPUT_DEPTH,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "print(\"Model instance created.\")\n",
    "\n",
    "# --- MANUALLY BUILD EACH LAYER WITH THE CORRECT INPUT SHAPE ---\n",
    "print(\"\\nManually building model layers...\")\n",
    "\n",
    "# 1. Build the initial dense layer. It takes `z` as input.\n",
    "decoder.z_to_initial_state.build(input_shape=(None, LATENT_DIM))\n",
    "print(f\"Built 'z_to_initial_state' layer.\")\n",
    "\n",
    "# 2. Build the LSTM cells. This is the most critical part.\n",
    "# The input to the *first* LSTM cell is the concatenation of the previous step's output (90) and z (512).\n",
    "first_lstm_input_dim = OUTPUT_DEPTH + LATENT_DIM # 90 + 512 = 602\n",
    "decoder.lstm_cells[0].build(input_shape=(None, first_lstm_input_dim))\n",
    "print(f\"Built LSTM cell 0 with input dimension {first_lstm_input_dim}.\")\n",
    "\n",
    "# The input to subsequent LSTM cells is the output of the previous cell.\n",
    "for i in range(1, len(decoder.lstm_cells)):\n",
    "    prev_cell_output_dim = decoder.lstm_cells[i-1].units\n",
    "    decoder.lstm_cells[i].build(input_shape=(None, prev_cell_output_dim))\n",
    "    print(f\"Built LSTM cell {i} with input dimension {prev_cell_output_dim}.\")\n",
    "\n",
    "# 3. Build the final output projection layer. It takes the output of the last LSTM cell.\n",
    "last_lstm_output_dim = decoder.lstm_cells[-1].units\n",
    "decoder.output_projection.build(input_shape=(None, last_lstm_output_dim))\n",
    "print(f\"Built 'output_projection' layer.\")\n",
    "\n",
    "print(\"\\nAll layers built successfully.\")\n",
    "# 4. Finally, indicate that the model is built.\n",
    "_ = decoder(z=tf.zeros((1, LATENT_DIM)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d4b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Magenta weights into the built model...\n",
      "Loaded weights for 'z_to_initial_state' layer.\n",
      "Loaded weights for LSTM cell 0 from 'decoder/multi_rnn_cell/cell_0/lstm_cell/kernel'.\n",
      "Loaded weights for LSTM cell 1 from 'decoder/multi_rnn_cell/cell_1/lstm_cell/kernel'.\n",
      "Loaded weights for LSTM cell 2 from 'decoder/multi_rnn_cell/cell_2/lstm_cell/kernel'.\n",
      "Loaded weights for 'output_projection' layer.\n",
      "\n",
      "Successfully loaded all decoder weights from Magenta checkpoint!\n",
      "Weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Now, loading the weights will work ---\n",
    "print(\"\\nLoading Magenta weights into the built model...\")\n",
    "\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "\n",
    "load_magenta_weights(decoder, CHECKPOINT_PATH) # This should now succeed\n",
    "print(\"Weights loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5135fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete functions created successfully.\n",
      "INFO:tensorflow:Assets written to: models/music_vae_decoder_keras\\assets\n",
      "\n",
      "Model saved successfully to 'models/music_vae_decoder_keras'.\n"
     ]
    }
   ],
   "source": [
    "# This forces the creation of the `FuncGraph(name=reconstruct)`\n",
    "concrete_reconstruct = decoder.reconstruct.get_concrete_function()\n",
    "\n",
    "# This forces the creation of the `FuncGraph(name=generate)`\n",
    "concrete_generate = decoder.generate.get_concrete_function()\n",
    "\n",
    "print(\"Concrete functions created successfully.\")\n",
    "\n",
    "# --- Now, you can save the full model with signatures ---\n",
    "# Define the path for the saved model directory\n",
    "model_save_path = \"models/music_vae_decoder_keras\" \n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "decoder.save(model_save_path, signatures={\n",
    "    'reconstruct': concrete_reconstruct,\n",
    "    'generate': concrete_generate\n",
    "},save_format=\"tf\")\n",
    "print(f\"\\nModel saved successfully to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5739ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading original TF1-style MusicVAE model ---\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [2048, 2048, 2048]\n",
      "\n",
      "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from models/download.magenta.tensorflow.org/models/music_vae/checkpoints/mel_2bar_big.ckpt\n",
      "Original model loaded.\n",
      "\n",
      "Logits for the very first step (first 5 values):\n",
      "[  3.6458497    0.19452271  -8.071625    -5.0703244  -10.072413  ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "LATENT_DIM = 512\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "# --- Correct parameters for 'mel_2bar_big' ---\n",
    "\n",
    "# IMPORTANT: We must use TF1 compatibility mode to load and run the original Magenta model.\n",
    "# This needs to be at the very top of your script.\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel # We need this class\n",
    "\n",
    "# Use tensorflow.compat.v1 and disable V2 behavior for the original model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP & MODEL LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- Step 1: Loading original TF1-style MusicVAE model ---\")\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "BASE_DIR = \"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "checkpoint_path = BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "# Use a batch size of 1 for easier comparison\n",
    "LATENT_DIM = mel_2bar_config.hparams.z_size\n",
    "SEQUENCE_LENGTH = 32 # Define the desired generation length\n",
    "BATCH_SIZE = 1\n",
    "VOCAB_SIZE = 90\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=BATCH_SIZE, checkpoint_dir_or_path=checkpoint_path)\n",
    "print(\"Original model loaded.\")\n",
    "\n",
    "graph = mel_2bar._sess.graph\n",
    "\n",
    "# --- Use the exact names discovered from your debugging ---\n",
    "# The z placeholder with shape (1, 512)\n",
    "Z_PLACEHOLDER_NAME = 'Placeholder_1:0'\n",
    "\n",
    "# The output logits tensor from the sampling graph\n",
    "LOGITS_TENSOR_NAME = 'sample/decoder/rnn_output:0'\n",
    "\n",
    "\n",
    "\n",
    "model_blueprint = mel_2bar_config.model\n",
    "decoder_blueprint = model_blueprint.decoder\n",
    "\n",
    "    # Retrive the relevant elements of the graph\n",
    "temperature_placeholder = graph.get_tensor_by_name('Placeholder:0')\n",
    "z_placeholder = graph.get_tensor_by_name('Placeholder_1:0')\n",
    "inputs_placeholder = graph.get_tensor_by_name('Placeholder_2:0')\n",
    "controls_placeholder = graph.get_tensor_by_name('Placeholder_3:0')\n",
    "inputs_length_placeholder = graph.get_tensor_by_name('Placeholder_4:0')\n",
    "output_length_placeholder = graph.get_tensor_by_name('Placeholder_5:0') # The final placeholder\n",
    "logits_tensor = graph.get_tensor_by_name('decoder/TensorArrayStack_1/TensorArrayGatherV3:0')\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "z_np = np.random.randn(BATCH_SIZE, LATENT_DIM).astype(np.float32)\n",
    "\n",
    "# Dummy inputs to satisfy the graph's requirements, based on your debugging\n",
    "dummy_inputs = np.zeros((BATCH_SIZE, SEQUENCE_LENGTH, VOCAB_SIZE), dtype=np.float32)\n",
    "dummy_inputs_length = np.array([SEQUENCE_LENGTH] * BATCH_SIZE, dtype=np.int32)\n",
    "dummy_controls = np.zeros((BATCH_SIZE, SEQUENCE_LENGTH, 0), dtype=np.float32)\n",
    "\n",
    "# Construct the full, correct feed dictionary\n",
    "feed_dict = {\n",
    "    temperature_placeholder: 0, # We don't need the temperature here other than as a dummy\n",
    "    z_placeholder: z_np,\n",
    "    inputs_placeholder: dummy_inputs,\n",
    "    inputs_length_placeholder: dummy_inputs_length,\n",
    "    controls_placeholder: dummy_controls,\n",
    "    output_length_placeholder: SEQUENCE_LENGTH # The final missing piece\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "logits_tf1 = mel_2bar._sess.run(\n",
    "    logits_tensor,\n",
    "    feed_dict=feed_dict\n",
    ")\n",
    "\n",
    "print(\"\\nLogits for the very first step (first 5 values):\")\n",
    "print(logits_tf1[2, 0, :5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Comparison Logic ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06da8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = decoder(z_np, sequence_length=SEQUENCE_LENGTH)\n",
    "print(generated_sequence[0, 0, :5][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_checkpoint(checkpoint_path):\n",
    "    \"\"\"\n",
    "    A helper function to print all variable names and their shapes in a checkpoint.\n",
    "    This is extremely useful for debugging name-related errors.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Inspecting variables in checkpoint: {checkpoint_path} ---\")\n",
    "    try:\n",
    "        reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "        shape_map = reader.get_variable_to_shape_map()\n",
    "        for key in sorted(shape_map.keys()):\n",
    "            print(f\"Tensor name: {key}, shape: {shape_map[key]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read checkpoint: {e}\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "inspect_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "def get_tensor_names_from_graph(graph):\n",
    "    \"\"\"\n",
    "    A helper function to print all tensor names in a TensorFlow graph.\n",
    "    This helps identify the correct names to use when accessing tensors.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Inspecting tensors in the graph ---\")\n",
    "    for index in range(len(graph.get_operations())):\n",
    "        op = graph.get_operations()[index]\n",
    "        print(f\"Operation name: {op.name}\"+\"\\n\")\n",
    "        for tensor in op.values():\n",
    "            print(f\"Tensor name: {tensor.name}, shape: {tensor.shape}\"+\"\\n\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
