{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c973dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTMCell, RNN, Dense\n",
    "import os\n",
    "import requests\n",
    "\n",
    "LATENT_DIM = 512\n",
    "OUTPUT_DEPTH = 90\n",
    "SEQUENCE_LENGTH = 32 # The correct, fixed sequence length for this model\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# The MusicVAEDecoder class remains the same\n",
    "class MusicVAEDecoder(tf.keras.Model):\n",
    "    \"\"\"The decoder portion of the MusicVAE model.\"\"\"\n",
    "    def __init__(self, output_depth, lstm_units=2048, num_layers=2, name=\"decoder\"):\n",
    "        super(MusicVAEDecoder, self).__init__(name=name)\n",
    "        self.z_to_initial_state = Dense(lstm_units * num_layers * 2, name=\"z_to_initial_state\")\n",
    "        self.lstm_cells = [LSTMCell(lstm_units, name=f\"lstm_cell_{i}\") for i in range(num_layers)]\n",
    "        self.rnn = RNN(self.lstm_cells, return_sequences=True, return_state=True, name=\"decoder_rnn\")\n",
    "        self.output_projection = Dense(output_depth, name=\"output_projection\")\n",
    "        self.vocab_size = output_depth\n",
    "\n",
    "    def call(self, z, sequence_length, inputs=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the decoder.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(z)[0]\n",
    "        num_layers = len(self.lstm_cells)\n",
    "        lstm_units = self.lstm_cells[0].units\n",
    "\n",
    "        # Project the latent vector 'z' to get the initial state for the LSTM.\n",
    "        # Shape: (batch_size, num_layers * 2 * lstm_units)\n",
    "        initial_state_flat = self.z_to_initial_state(z)\n",
    "\n",
    "        # Reshape to separate layers and the h/c states.\n",
    "        # Shape: (batch_size, num_layers, 2, lstm_units)\n",
    "        initial_state_reshaped = tf.reshape(\n",
    "            initial_state_flat, [batch_size, num_layers, 2, lstm_units]\n",
    "        )\n",
    "\n",
    "        # Transpose to group h/c states by layer.\n",
    "        # Shape: (num_layers, 2, batch_size, lstm_units)\n",
    "        initial_state_transposed = tf.transpose(initial_state_reshaped, [1, 2, 0, 3])\n",
    "\n",
    "        # Unstack to create the final list of states for each layer.\n",
    "        # This creates a list of `num_layers` elements.\n",
    "        # Each element is a tensor of shape (2, batch_size, lstm_units).\n",
    "        initial_state_list = tf.unstack(initial_state_transposed)\n",
    "\n",
    "        # Further unstack each layer's state into (h, c) tuples.\n",
    "        # The final structure is: [ (h0, c0), (h1, c1), ... ]\n",
    "        # which is what the Keras RNN layer expects.\n",
    "        initial_state = [tf.unstack(s) for s in initial_state_list]\n",
    "\n",
    "\n",
    "         # --- THE CRUCIAL FIX ---\n",
    "        # 2. Prepare the latent vector for concatenation at each time step.\n",
    "        # We need to repeat `z` so it can be attached to every element of the sequence.\n",
    "        # Tile z from shape [batch, latent_dim] to [batch, sequence_length, latent_dim]\n",
    "        z_repeated = tf.tile(tf.expand_dims(z, 1), [1, sequence_length, 1])\n",
    "\n",
    "        \n",
    "\n",
    "        # This distinction here is very subtle and important. If inputs is provided,\n",
    "        # we run the RNN in \"training\" mode (teacher forcing). If inputs is None,\n",
    "        # we run in \"inference\" mode (autoregressive generation).\n",
    "        if inputs is not None:\n",
    "            # The input to the first LSTM is z \n",
    "            # which has a dimension of 512 \n",
    "            input_depth = 90 \n",
    "            inputs = tf.zeros([batch_size, sequence_length, input_depth])\n",
    "        \n",
    "        # Concatenate z with the inputs along the feature dimension.\n",
    "        # `inputs` has shape [batch, sequence_length, 90]\n",
    "            # `z_repeated` has shape [batch, sequence_length, 512]\n",
    "            # The result will have shape [batch, sequence_length, 602]\n",
    "            rnn_inputs = tf.concat([inputs, z_repeated], axis=-1)\n",
    "\n",
    "             # Run the RNN.\n",
    "            rnn_output, *_ = self.rnn(rnn_inputs, initial_state=initial_state)\n",
    "\n",
    "            # Project the RNN output to the final output space.\n",
    "            output = self.output_projection(rnn_output)\n",
    "            return output\n",
    "        else:\n",
    "            # Start with a \"start token\", which is a zero vector.\n",
    "            # Shape: [batch_size, vocab_size]\n",
    "            step_input = tf.zeros([batch_size, self.vocab_size])\n",
    "            \n",
    "            # The state will be updated at each step of the loop\n",
    "            current_state = initial_state\n",
    "            \n",
    "            # List to collect the output logits at each step\n",
    "            all_logits = []\n",
    "\n",
    "            for _ in range(sequence_length):\n",
    "                # A. Concatenate the current step's input with z\n",
    "                # Shape: [batch_size, 90 + 512] -> [batch_size, 602]\n",
    "                step_input_with_z = tf.concat([step_input, z], axis=-1)\n",
    "\n",
    "                # B. Run the stacked LSTM cells for a single step\n",
    "                # We must manually call each cell in the stack.\n",
    "                # The input to the first cell is our concatenated vector.\n",
    "                # The input to subsequent cells is the output of the previous cell.\n",
    "                cell_input = step_input_with_z\n",
    "                new_states = []\n",
    "                for i, cell in enumerate(self.lstm_cells):\n",
    "                    # `cell` returns (output, [new_h, new_c])\n",
    "                    # `current_state[i]` is the (h, c) tuple for this cell\n",
    "                    cell_output, (new_h, new_c) = cell(cell_input, states=current_state[i])\n",
    "                    new_states.append([new_h, new_c])\n",
    "                    # The output of this cell becomes the input for the next\n",
    "                    cell_input = cell_output\n",
    "                \n",
    "                # The final output of the stack is the last cell's output\n",
    "                final_cell_output = cell_output\n",
    "                current_state = new_states # Update the state for the next iteration\n",
    "\n",
    "                # C. Project the output to get the logits for this single step\n",
    "                # Shape: [batch_size, 90]\n",
    "                step_logits = self.output_projection(final_cell_output)\n",
    "                all_logits.append(step_logits)\n",
    "\n",
    "                # D. Prepare the input for the *next* step (autoregression)\n",
    "                # Get the most likely note index from the logits\n",
    "                next_token_indices = tf.argmax(step_logits, axis=-1)\n",
    "                # Convert the index back to a one-hot vector\n",
    "                step_input = tf.one_hot(next_token_indices, depth=self.vocab_size)\n",
    "\n",
    "            # Stack all the single-step logits into a final tensor\n",
    "            # Shape: [batch_size, sequence_length, 90]\n",
    "            final_logits = tf.stack(all_logits, axis=1)\n",
    "            return final_logits\n",
    "\n",
    "\n",
    "    # --- 2. The \"Teaching\" Endpoint: Decorated for Training/Reconstruction ---\n",
    "    # This will be one of the functions available in your saved model.\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, LATENT_DIM], name=\"z\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32, name=\"sequence_length\"),\n",
    "        tf.TensorSpec(shape=[None, None, VOCAB_SIZE], name=\"inputs\")\n",
    "    ])\n",
    "    def reconstruct(self, z, sequence_length, inputs):\n",
    "        \"\"\"Runs the model in teacher-forcing mode.\"\"\"\n",
    "        return self.call(z, sequence_length, inputs=inputs)\n",
    "\n",
    "    # --- 3. The \"Improvising\" Endpoint: Decorated for Generation ---\n",
    "    # This will be another function available in your saved model.\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None, LATENT_DIM], name=\"z\"),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32, name=\"sequence_length\")\n",
    "    ])\n",
    "    def generate(self, z, sequence_length):\n",
    "        \"\"\"Runs the model in autoregressive generation mode.\"\"\"\n",
    "        return self.call(z, sequence_length, inputs=None)\n",
    "   \n",
    "def load_magenta_weights(decoder_model, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Loads weights from a TF1 Magenta checkpoint into a TF2 Keras decoder model.\n",
    "    (Final corrected version)\n",
    "    \"\"\"\n",
    "    reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "\n",
    "    # --- 1. Load z_to_initial_state weights ---\n",
    "    z_kernel = reader.get_tensor(\"decoder/z_to_initial_state/kernel\")\n",
    "    z_bias = reader.get_tensor(\"decoder/z_to_initial_state/bias\")\n",
    "    decoder_model.z_to_initial_state.set_weights([z_kernel, z_bias])\n",
    "    print(\"Loaded weights for 'z_to_initial_state' layer.\")\n",
    "\n",
    "    # --- 2. Load LSTM cell weights ---\n",
    "    for i, cell in enumerate(decoder_model.lstm_cells):\n",
    "        tf1_kernel_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/kernel\"\n",
    "        tf1_bias_name = f\"decoder/multi_rnn_cell/cell_{i}/lstm_cell/bias\"\n",
    "        \n",
    "        tf1_kernel = reader.get_tensor(tf1_kernel_name)\n",
    "        tf1_bias = reader.get_tensor(tf1_bias_name)\n",
    "\n",
    "        # THE FIX: Use the correct input dimension for splitting the kernel,\n",
    "        # based on the layer index.\n",
    "        if i == 0:\n",
    "            # The original model's first layer has a complex input of dim 602.\n",
    "            input_dim = 602\n",
    "        else:\n",
    "            # Subsequent layers take the output of the previous LSTM layer.\n",
    "            input_dim = cell.units # which is 2048\n",
    "\n",
    "        # Perform the split at the correct index.\n",
    "        keras_kernel = tf1_kernel[:input_dim, :]\n",
    "        keras_recurrent_kernel = tf1_kernel[input_dim:, :]\n",
    "        \n",
    "        # Now the shapes will match perfectly.\n",
    "        cell.set_weights([keras_kernel, keras_recurrent_kernel, tf1_bias])\n",
    "        print(f\"Loaded weights for LSTM cell {i} from '{tf1_kernel_name}'.\")\n",
    "\n",
    "    # --- 3. Load output_projection weights ---\n",
    "    out_kernel = reader.get_tensor(\"decoder/output_projection/kernel\")\n",
    "    out_bias = reader.get_tensor(\"decoder/output_projection/bias\")\n",
    "    decoder_model.output_projection.set_weights([out_kernel, out_bias])\n",
    "    print(\"Loaded weights for 'output_projection' layer.\")\n",
    "\n",
    "    print(\"\\nSuccessfully loaded all decoder weights from Magenta checkpoint!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "# --- Correct parameters for 'mel_2bar_big' ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e3665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instance created.\n",
      "\n",
      "Manually building model layers...\n",
      "Built 'z_to_initial_state' layer.\n",
      "Built LSTM cell 0 with input dimension 602.\n",
      "Built LSTM cell 1 with input dimension 2048.\n",
      "Built LSTM cell 2 with input dimension 2048.\n",
      "Built 'output_projection' layer.\n",
      "\n",
      "All layers built successfully.\n",
      "\n",
      "Loading Magenta weights into the built model...\n",
      "Loaded weights for 'z_to_initial_state' layer.\n",
      "Loaded weights for LSTM cell 0 from 'decoder/multi_rnn_cell/cell_0/lstm_cell/kernel'.\n",
      "Loaded weights for LSTM cell 1 from 'decoder/multi_rnn_cell/cell_1/lstm_cell/kernel'.\n",
      "Loaded weights for LSTM cell 2 from 'decoder/multi_rnn_cell/cell_2/lstm_cell/kernel'.\n",
      "Loaded weights for 'output_projection' layer.\n",
      "\n",
      "Successfully loaded all decoder weights from Magenta checkpoint!\n",
      "Weights loaded successfully.\n"
     ]
    },
    {
     "ename": "InaccessibleTensorError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1994342386.py\", line 154, in generate  *\n        return self.call(z, sequence_length, inputs=None)\n    File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 131, in call  *\n        final_logits = tf.stack(all_logits, axis=1)\n\n    InaccessibleTensorError: <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> was defined here:\n        File \"D:\\Program Files\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"D:\\Program Files\\Python310\\lib\\runpy.py\", line 86, in _run_code\n          exec(code, run_globals)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n          app.launch_new_instance()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n          app.start()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n          self.io_loop.start()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n          self.asyncio_loop.run_forever()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n          self._run_once()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n          handle._run()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n          return await f(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n          await self.dispatch_shell(msg, subshell_id=subshell_id)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n          await result\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n          await super().execute_request(stream, ident, parent)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n          reply_content = await reply_content\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n          res = shell.run_cell(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n          return super().run_cell(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n          result = self._run_cell(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n          result = runner(coro)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n          coro.send(None)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n          if await self.run_code(code, result, async_=asy):\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1239954717.py\", line 54, in <module>\n          decoder.save(model_save_path, signatures={\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n          return fn(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2826, in save\n          saving_api.save_model(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\saving_api.py\", line 145, in save_model\n          return legacy_sm_saving_lib.save_model(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n          return fn(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\legacy\\save.py\", line 168, in save_model\n          saved_model_save.save(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\legacy\\saved_model\\save.py\", line 98, in save\n          saved_nodes, node_paths = save_lib.save_and_return_nodes(\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1994342386.py\", line 154, in generate\n          return self.call(z, sequence_length, inputs=None)\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 66, in call\n          if inputs is not None:\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 95, in call\n          for _ in range(sequence_length):\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 120, in call\n          step_logits = self.output_projection(final_cell_output)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\engine\\base_layer_v1.py\", line 838, in __call__\n          outputs = call_fn(cast_inputs, *args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 252, in call\n          outputs = tf.nn.bias_add(outputs, self.bias)\n    \n    The tensor <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> cannot be accessed from FuncGraph(name=generate, id=1272522258656), because it was defined in FuncGraph(name=tmp, id=1272522417984), which is out of scope.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/music_vae_decoder_keras\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     53\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_save_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 54\u001b[0m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreconstruct\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved successfully to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1200\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1994342386.py\", line 154, in generate  *\n        return self.call(z, sequence_length, inputs=None)\n    File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 131, in call  *\n        final_logits = tf.stack(all_logits, axis=1)\n\n    InaccessibleTensorError: <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> was defined here:\n        File \"D:\\Program Files\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n          return _run_code(code, main_globals, None,\n        File \"D:\\Program Files\\Python310\\lib\\runpy.py\", line 86, in _run_code\n          exec(code, run_globals)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n          app.launch_new_instance()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n          app.start()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n          self.io_loop.start()\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n          self.asyncio_loop.run_forever()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n          self._run_once()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n          handle._run()\n        File \"D:\\Program Files\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n          self._context.run(self._callback, *self._args)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n          return await f(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n          await self.dispatch_shell(msg, subshell_id=subshell_id)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n          await result\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n          await super().execute_request(stream, ident, parent)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n          reply_content = await reply_content\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n          res = shell.run_cell(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n          return super().run_cell(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n          result = self._run_cell(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n          result = runner(coro)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n          coro.send(None)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n          has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n          if await self.run_code(code, result, async_=asy):\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n          exec(code_obj, self.user_global_ns, self.user_ns)\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1239954717.py\", line 54, in <module>\n          decoder.save(model_save_path, signatures={\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n          return fn(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2826, in save\n          saving_api.save_model(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\saving_api.py\", line 145, in save_model\n          return legacy_sm_saving_lib.save_model(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n          return fn(*args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\legacy\\save.py\", line 168, in save_model\n          saved_model_save.save(\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\saving\\legacy\\saved_model\\save.py\", line 98, in save\n          saved_nodes, node_paths = save_lib.save_and_return_nodes(\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\1994342386.py\", line 154, in generate\n          return self.call(z, sequence_length, inputs=None)\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 66, in call\n          if inputs is not None:\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 95, in call\n          for _ in range(sequence_length):\n        File \"C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_3540\\3144523106.py\", line 120, in call\n          step_logits = self.output_projection(final_cell_output)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\engine\\base_layer_v1.py\", line 838, in __call__\n          outputs = call_fn(cast_inputs, *args, **kwargs)\n        File \"d:\\Users\\thoma\\Documents\\git\\python_midi_training\\.venv\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 252, in call\n          outputs = tf.nn.bias_add(outputs, self.bias)\n    \n    The tensor <tf.Tensor 'output_projection/BiasAdd:0' shape=(?, 90) dtype=float32> cannot be accessed from FuncGraph(name=generate, id=1272522258656), because it was defined in FuncGraph(name=tmp, id=1272522417984), which is out of scope.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Define constants ---\n",
    "LATENT_DIM = 512\n",
    "OUTPUT_DEPTH = 90 # This is your OUTPUT_DEPTH\n",
    "LSTM_UNITS = 2048\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# 2. Instantiate our TF2 Keras decoder model with the correct number of layers.\n",
    "decoder = MusicVAEDecoder(\n",
    "    output_depth=OUTPUT_DEPTH,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    num_layers=NUM_LAYERS\n",
    ")\n",
    "print(\"Model instance created.\")\n",
    "\n",
    "# --- MANUALLY BUILD EACH LAYER WITH THE CORRECT INPUT SHAPE ---\n",
    "print(\"\\nManually building model layers...\")\n",
    "\n",
    "# 1. Build the initial dense layer. It takes `z` as input.\n",
    "decoder.z_to_initial_state.build(input_shape=(None, LATENT_DIM))\n",
    "print(f\"Built 'z_to_initial_state' layer.\")\n",
    "\n",
    "# 2. Build the LSTM cells. This is the most critical part.\n",
    "# The input to the *first* LSTM cell is the concatenation of the previous step's output (90) and z (512).\n",
    "first_lstm_input_dim = OUTPUT_DEPTH + LATENT_DIM # 90 + 512 = 602\n",
    "decoder.lstm_cells[0].build(input_shape=(None, first_lstm_input_dim))\n",
    "print(f\"Built LSTM cell 0 with input dimension {first_lstm_input_dim}.\")\n",
    "\n",
    "# The input to subsequent LSTM cells is the output of the previous cell.\n",
    "for i in range(1, len(decoder.lstm_cells)):\n",
    "    prev_cell_output_dim = decoder.lstm_cells[i-1].units\n",
    "    decoder.lstm_cells[i].build(input_shape=(None, prev_cell_output_dim))\n",
    "    print(f\"Built LSTM cell {i} with input dimension {prev_cell_output_dim}.\")\n",
    "\n",
    "# 3. Build the final output projection layer. It takes the output of the last LSTM cell.\n",
    "last_lstm_output_dim = decoder.lstm_cells[-1].units\n",
    "decoder.output_projection.build(input_shape=(None, last_lstm_output_dim))\n",
    "print(f\"Built 'output_projection' layer.\")\n",
    "\n",
    "print(\"\\nAll layers built successfully.\")\n",
    "# 4. Finally, indicate that the model is built.\n",
    "decoder.built=True\n",
    "\n",
    "# --- Now, loading the weights will work ---\n",
    "print(\"\\nLoading Magenta weights into the built model...\")\n",
    "load_magenta_weights(decoder, CHECKPOINT_PATH) # This should now succeed\n",
    "print(\"Weights loaded successfully.\")\n",
    "\n",
    "# This forces the creation of the `FuncGraph(name=reconstruct)`\n",
    "concrete_reconstruct = decoder.reconstruct.get_concrete_function()\n",
    "\n",
    "# This forces the creation of the `FuncGraph(name=generate)`\n",
    "concrete_generate = decoder.generate.get_concrete_function()\n",
    "\n",
    "print(\"Concrete functions created successfully.\")\n",
    "\n",
    "# --- Now, you can save the full model with signatures ---\n",
    "# Define the path for the saved model directory\n",
    "model_save_path = \"models/music_vae_decoder_keras\" \n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "decoder.save(model_save_path, signatures={\n",
    "    'reconstruct': concrete_reconstruct,\n",
    "    'generate': concrete_generate\n",
    "},save_format=\"tf\")\n",
    "print(f\"\\nModel saved successfully to '{model_save_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5739ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading original TF1-style MusicVAE model ---\n",
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 512, 'free_bits': 0, 'max_beta': 0.5, 'beta_rate': 0.99999, 'batch_size': 1, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [2048, 2048, 2048], 'enc_rnn_size': [2048], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False, 'control_preprocessing_rnn_size': [256]}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [2048]\n",
      "\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [2048, 2048, 2048]\n",
      "\n",
      "WARNING:tensorflow:Setting non-training sampling schedule from inverse_sigmoid:1000.000000 to constant:1.0.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from models/download.magenta.tensorflow.org/models/music_vae/checkpoints/mel_2bar_big.ckpt\n",
      "Original model loaded.\n",
      "\n",
      "Logits for the very first step (first 5 values):\n",
      "[  3.6458497    0.19452271  -8.071625    -5.0703244  -10.072413  ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "\n",
    "LATENT_DIM = 512\n",
    "\n",
    "# IMPORTANT: We must use TF1 compatibility mode to load and run the original Magenta model.\n",
    "# This needs to be at the very top of your script.\n",
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.disable_v2_behavior()\n",
    "\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel # We need this class\n",
    "\n",
    "# Use tensorflow.compat.v1 and disable V2 behavior for the original model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP & MODEL LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- Step 1: Loading original TF1-style MusicVAE model ---\")\n",
    "mel_2bar_config = configs.CONFIG_MAP['cat-mel_2bar_big']\n",
    "BASE_DIR = \"models/download.magenta.tensorflow.org/models/music_vae\"\n",
    "checkpoint_path = BASE_DIR + '/checkpoints/mel_2bar_big.ckpt'\n",
    "\n",
    "# Use a batch size of 1 for easier comparison\n",
    "LATENT_DIM = mel_2bar_config.hparams.z_size\n",
    "SEQUENCE_LENGTH = 32 # Define the desired generation length\n",
    "BATCH_SIZE = 1\n",
    "VOCAB_SIZE = 90\n",
    "mel_2bar = TrainedModel(mel_2bar_config, batch_size=BATCH_SIZE, checkpoint_dir_or_path=checkpoint_path)\n",
    "print(\"Original model loaded.\")\n",
    "\n",
    "graph = mel_2bar._sess.graph\n",
    "\n",
    "# --- Use the exact names discovered from your debugging ---\n",
    "# The z placeholder with shape (1, 512)\n",
    "Z_PLACEHOLDER_NAME = 'Placeholder_1:0'\n",
    "\n",
    "# The output logits tensor from the sampling graph\n",
    "LOGITS_TENSOR_NAME = 'sample/decoder/rnn_output:0'\n",
    "\n",
    "\n",
    "\n",
    "model_blueprint = mel_2bar_config.model\n",
    "decoder_blueprint = model_blueprint.decoder\n",
    "\n",
    "    # Retrive the relevant elements of the graph\n",
    "temperature_placeholder = graph.get_tensor_by_name('Placeholder:0')\n",
    "z_placeholder = graph.get_tensor_by_name('Placeholder_1:0')\n",
    "inputs_placeholder = graph.get_tensor_by_name('Placeholder_2:0')\n",
    "controls_placeholder = graph.get_tensor_by_name('Placeholder_3:0')\n",
    "inputs_length_placeholder = graph.get_tensor_by_name('Placeholder_4:0')\n",
    "output_length_placeholder = graph.get_tensor_by_name('Placeholder_5:0') # The final placeholder\n",
    "logits_tensor = graph.get_tensor_by_name('decoder/TensorArrayStack_1/TensorArrayGatherV3:0')\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "z_np = np.random.randn(BATCH_SIZE, LATENT_DIM).astype(np.float32)\n",
    "\n",
    "# Dummy inputs to satisfy the graph's requirements, based on your debugging\n",
    "dummy_inputs = np.zeros((BATCH_SIZE, SEQUENCE_LENGTH, VOCAB_SIZE), dtype=np.float32)\n",
    "dummy_inputs_length = np.array([SEQUENCE_LENGTH] * BATCH_SIZE, dtype=np.int32)\n",
    "dummy_controls = np.zeros((BATCH_SIZE, SEQUENCE_LENGTH, 0), dtype=np.float32)\n",
    "\n",
    "# Construct the full, correct feed dictionary\n",
    "feed_dict = {\n",
    "    temperature_placeholder: 0, # We don't need the temperature here other than as a dummy\n",
    "    z_placeholder: z_np,\n",
    "    inputs_placeholder: dummy_inputs,\n",
    "    inputs_length_placeholder: dummy_inputs_length,\n",
    "    controls_placeholder: dummy_controls,\n",
    "    output_length_placeholder: SEQUENCE_LENGTH # The final missing piece\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "logits_tf1 = mel_2bar._sess.run(\n",
    "    logits_tensor,\n",
    "    feed_dict=feed_dict\n",
    ")\n",
    "\n",
    "print(\"\\nLogits for the very first step (first 5 values):\")\n",
    "print(logits_tf1[2, 0, :5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Main Comparison Logic ---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06da8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_2:0\", shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "generated_sequence = decoder(z_np, sequence_length=SEQUENCE_LENGTH)\n",
    "print(generated_sequence[0, 0, :5][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_checkpoint(checkpoint_path):\n",
    "    \"\"\"\n",
    "    A helper function to print all variable names and their shapes in a checkpoint.\n",
    "    This is extremely useful for debugging name-related errors.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Inspecting variables in checkpoint: {checkpoint_path} ---\")\n",
    "    try:\n",
    "        reader = tf.train.load_checkpoint(checkpoint_path)\n",
    "        shape_map = reader.get_variable_to_shape_map()\n",
    "        for key in sorted(shape_map.keys()):\n",
    "            print(f\"Tensor name: {key}, shape: {shape_map[key]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not read checkpoint: {e}\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "MODEL_NAME = \"mel_2bar_big\"\n",
    "CHECKPOINT_DIR = \"models/download.magenta.tensorflow.org/models/music_vae/checkpoints\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, f\"{MODEL_NAME}.ckpt\")\n",
    "\n",
    "inspect_checkpoint(CHECKPOINT_PATH)\n",
    "\n",
    "\n",
    "def get_tensor_names_from_graph(graph):\n",
    "    \"\"\"\n",
    "    A helper function to print all tensor names in a TensorFlow graph.\n",
    "    This helps identify the correct names to use when accessing tensors.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Inspecting tensors in the graph ---\")\n",
    "    for index in range(len(graph.get_operations())):\n",
    "        op = graph.get_operations()[index]\n",
    "        print(f\"Operation name: {op.name}\"+\"\\n\")\n",
    "        for tensor in op.values():\n",
    "            print(f\"Tensor name: {tensor.name}, shape: {tensor.shape}\"+\"\\n\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
